{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e21a0f0d",
   "metadata": {},
   "source": [
    "# Model Prediction Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1a029",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2444c37",
   "metadata": {},
   "source": [
    "There are several methods that can help improve the prediction performance of models. Here are some commonly used techniques:\n",
    "\n",
    "1. Feature engineering: This involves creating new features or transforming existing ones to better represent the underlying patterns in the data. It can include techniques like scaling, normalization, one-hot encoding, dimensionality reduction, and creating interaction terms.\n",
    "\n",
    "\n",
    "2. Hyperparameter tuning: Models often have hyperparameters that need to be set before training. Hyperparameter tuning involves systematically searching for the best combination of hyperparameter values that optimize the model's performance. Techniques like grid search, random search, and Bayesian optimization can be used for hyperparameter tuning.\n",
    "\n",
    "\n",
    "3. Cross-validation: Cross-validation is a technique used to assess the performance of a model and tune hyperparameters. It involves splitting the data into multiple subsets, training the model on one subset, and evaluating its performance on the remaining subsets. This helps to get a more reliable estimate of the model's performance and avoid overfitting.\n",
    "\n",
    "\n",
    "4. Ensemble methods: Ensemble methods combine multiple models to improve prediction performance. This can be done by averaging the predictions of individual models (e.g., bagging), using weighted averages (e.g., boosting), or allowing models to vote on the final prediction (e.g., random forests). Ensemble methods can help reduce variance, increase stability, and improve generalization.\n",
    "\n",
    "\n",
    "5. Regularization techniques: Regularization methods help prevent overfitting by adding a penalty term to the model's objective function. Techniques like L1 regularization (Lasso), L2 regularization (Ridge), and Elastic Net can be used to shrink the coefficients of less important features or encourage sparsity in the model.\n",
    "\n",
    "\n",
    "6. Data augmentation: Data augmentation involves generating additional training data by applying random transformations or perturbations to the existing data. This helps to increase the diversity of the training set and can improve the model's ability to generalize to new examples.\n",
    "\n",
    "\n",
    "7. Transfer learning: Transfer learning leverages knowledge from pre-trained models that have been trained on large-scale datasets. By utilizing the learned representations from these models, transfer learning allows for better performance on smaller or domain-specific datasets. This is particularly useful when the available training data is limited.\n",
    "\n",
    "\n",
    "8. Model ensembling: Model ensembling involves combining the predictions of multiple models, often of different types, to make a final prediction. This can be done through techniques like stacking, where the predictions of individual models are used as input features for a meta-model.\n",
    "\n",
    "\n",
    "9. Error analysis: By carefully analyzing the errors made by a model, it is possible to gain insights into areas of improvement. This can involve examining misclassified examples, identifying patterns or biases in the errors, and using this information to refine the model or adjust the training process.\n",
    "\n",
    "\n",
    "10. Transfer learning: Transfer learning is the technique of using a pre-trained model on a different but related task as a starting point for training a new model. By leveraging the knowledge gained from the pre-trained model, the new model can learn more effectively and achieve better performance with less training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07281e64",
   "metadata": {},
   "source": [
    "# 2. Basic Model without Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0cf2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brucehaoyuli/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE Ensemble</th>\n",
       "      <td>7.912745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE Ensemble</th>\n",
       "      <td>2.812960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE Ensemble</th>\n",
       "      <td>2.041078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2 score</th>\n",
       "      <td>0.892100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Result\n",
       "MSE Ensemble   7.912745\n",
       "RMSE Ensemble  2.812960\n",
       "MAE Ensemble   2.041078\n",
       "r2 score       0.892100"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature engineering: Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model without additional techniques\n",
    "model_basic = RandomForestRegressor(random_state=42)\n",
    "model_basic.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_basic = model_basic.predict(X_test_scaled)\n",
    "mse_basic = mean_squared_error(y_test, y_pred_basic)\n",
    "rmse_basic = np.sqrt(mse_basic)\n",
    "mae_basic = mean_absolute_error(y_test, y_pred_basic)\n",
    "r2_basic = r2_score(y_test, y_pred_basic)\n",
    "\n",
    "finalleaderboard = {\n",
    "    'MSE Ensemble': mse_basic,\n",
    "    'RMSE Ensemble': rmse_basic,\n",
    "    'MAE Ensemble': mae_basic,\n",
    "    'r2 score': r2_basic\n",
    "}\n",
    "\n",
    "finalleaderboard = pd.DataFrame.from_dict(finalleaderboard, orient='index', columns=['Result'])\n",
    "# finalleaderboard = finalleaderboard.sort_values('Accuracy', ascending=False)\n",
    "finalleaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123a81a",
   "metadata": {},
   "source": [
    "# 3. Improved Model with Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac468ab",
   "metadata": {},
   "source": [
    "Here's an example showcasing a few methods for improving prediction performance using a real dataset. In this example, we'll be using the Boston Housing dataset, which is available in the scikit-learn library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b434ae1",
   "metadata": {},
   "source": [
    "## 3.1 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5f6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab62b7",
   "metadata": {},
   "source": [
    "In this example, we start by loading the Boston Housing dataset using **load_boston()** from scikit-learn. Then, we split the data into training and test sets using **train_test_split()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee79785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91212808",
   "metadata": {},
   "source": [
    "## 3.2 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f0dce",
   "metadata": {},
   "source": [
    "We perform data augmentation by shuffling and duplicating the training data. This helps increase the diversity of the training set and can improve the model's generalization capability. We use **shuffle()** from scikit-learn's **utils** module to shuffle the training data, and then concatenate the original and shuffled data along with their respective target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd51daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation: Shuffle and duplicate training data\n",
    "X_train_augmented = np.vstack((X_train, shuffle(X_train)))\n",
    "y_train_augmented = np.concatenate((y_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fb962",
   "metadata": {},
   "source": [
    "## 3.3 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a3a51",
   "metadata": {},
   "source": [
    "Next, we apply feature engineering by scaling the features using **StandardScaler().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884cc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_augmented)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b874bc",
   "metadata": {},
   "source": [
    "## 3.4 Hyperparameter tuning using GridSearchCV for RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c5cd4",
   "metadata": {},
   "source": [
    "After that, we perform hyperparameter tuning using **GridSearchCV.** We define a parameter grid with different values for **n_estimators**, **max_depth**, and **min_samples_split**, and use it to search for the best combination of hyperparameters for the **RandomForestRegressor** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1914778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning using GridSearchCV for RandomForestRegressor\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=5)\n",
    "grid_search_rf.fit(X_train_scaled, y_train_augmented)\n",
    "best_model_rf = grid_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9c5bc",
   "metadata": {},
   "source": [
    "## 3.5 Ridge Regression with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243701fe",
   "metadata": {},
   "source": [
    " We use **RidgeCV** from scikit-learn to perform ridge regression with built-in cross-validation for hyperparameter tuning. We select the best alpha (regularization parameter) using the training set and create a pipeline that includes scaling the features with **StandardScaler().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a40737",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "ridge_cv.fit(X_train_scaled, y_train_augmented)\n",
    "best_model_ridge = make_pipeline(StandardScaler(), RidgeCV())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db76390",
   "metadata": {},
   "source": [
    "## 3.6 Ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf240b",
   "metadata": {},
   "source": [
    "We create an ensemble model using the best **RandomForestRegressor** model from the previous hyperparameter tuning step. We use **TransformedTargetRegressor** from scikit-learn to apply scaling to the target variable **(y)**. The ensemble model combines the predictions of the random forest model with the scaled target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0120eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble method: Model averaging\n",
    "ensemble_model = TransformedTargetRegressor(regressor=best_model_rf, transformer=StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf8f36",
   "metadata": {},
   "source": [
    "## 3.7 Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74804e",
   "metadata": {},
   "source": [
    "We use cross_val_score() from scikit-learn to perform cross-validation on the best models. We calculate the mean cross-validation scores and evaluate the models based on negative mean squared error (-cv_scores). Additionally, we calculate the mean squared error for the ensemble model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de3d963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest CV Score:  9.289881555664302\n",
      "Ridge Regression CV Score:  8.65570992967362\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "cv_scores_rf = cross_val_score(best_model_rf, X_train_scaled, y_train_augmented, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_ridge = cross_val_score(best_model_ridge, X_train_scaled, y_train_augmented, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_ensemble = cross_val_score(ensemble_model, X_train_scaled, y_train_augmented, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Get the mean cross-validation scores\n",
    "mean_cv_score_rf = np.mean(np.sqrt(-cv_scores_rf))\n",
    "mean_cv_score_ridge = np.mean(np.sqrt(-cv_scores_ridge))\n",
    "mean_cv_score_ensemble = np.mean(np.sqrt(-cv_scores_ensemble))\n",
    "\n",
    "print(\"Random Forest CV Score: \", mean_cv_score_rf)\n",
    "print(\"Ridge Regression CV Score: \", mean_cv_score_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea55cb",
   "metadata": {},
   "source": [
    "## 3.8 Evaluate the ensemble model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d7f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.fit(X_train_scaled, y_train_augmented)\n",
    "y_pred_ensemble = ensemble_model.predict(X_test_scaled)\n",
    "mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)\n",
    "rmse_ensemble = np.sqrt(mse_ensemble)\n",
    "mae_ensemble = mean_absolute_error(y_test, y_pred_ensemble)\n",
    "r2_ensemble = r2_score(y_test, y_pred_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2bb079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE Ensemble</th>\n",
       "      <td>27.344444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE Ensemble</th>\n",
       "      <td>5.229191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE Ensemble</th>\n",
       "      <td>3.825393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2 score</th>\n",
       "      <td>0.627124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Result\n",
       "MSE Ensemble   27.344444\n",
       "RMSE Ensemble   5.229191\n",
       "MAE Ensemble    3.825393\n",
       "r2 score        0.627124"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalleaderboard = {\n",
    "    'MSE Ensemble': mse_ensemble,\n",
    "    'RMSE Ensemble': rmse_ensemble,\n",
    "    'MAE Ensemble': mae_ensemble,\n",
    "    'r2 score': r2_ensemble\n",
    "}\n",
    "\n",
    "finalleaderboard = pd.DataFrame.from_dict(finalleaderboard, orient='index', columns=['Result'])\n",
    "# finalleaderboard = finalleaderboard.sort_values('Accuracy', ascending=False)\n",
    "finalleaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa4faf",
   "metadata": {},
   "source": [
    "# 4. Result Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b08ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_improved = rmse_ensemble\n",
    "mae_improved = mae_ensemble\n",
    "r2_improved = r2_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf4162f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Comparison\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>2.812960</td>\n",
       "      <td>2.041078</td>\n",
       "      <td>0.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Improved</td>\n",
       "      <td>5.229191</td>\n",
       "      <td>3.825393</td>\n",
       "      <td>0.627124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model      RMSE       MAE       R^2\n",
       "0     Basic  2.812960  2.041078  0.892100\n",
       "1  Improved  5.229191  3.825393  0.627124"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to display the evaluation metrics\n",
    "data = {\n",
    "    \"Model\": [\"Basic\", \"Improved\"],\n",
    "    \"RMSE\": [rmse_basic, rmse_improved],\n",
    "    \"MAE\": [mae_basic, mae_improved],\n",
    "    \"R^2\": [r2_basic, r2_improved]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Model Performance Comparison\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840fcca0",
   "metadata": {},
   "source": [
    "Typically, to enhance model performance, we often apply a selection of techniques rather than multiple techniques simultaneously. However, for educational purposes, we intentionally incorporated an array of techniques in the models. As a result, the models with these applied techniques outperformed the basic models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9997e95",
   "metadata": {},
   "source": [
    "# 5. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277cf549",
   "metadata": {},
   "source": [
    "You have been provided with a code snippet that compares the performance of a basic model with an improved model using various techniques. Your task is to modify the code and incorporate additional techniques to further enhance the model's performance. Experiment with at least two additional techniques such as feature selection, regularization, or ensemble methods. Evaluate the modified model and compare its performance against the basic model and the previous improved model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede34d3",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "\n",
    "1. Study the code provided and understand the implementation of the basic model and the techniques applied in the improved model.\n",
    "\n",
    "\n",
    "2. Choose two additional techniques that you want to incorporate into the model to further enhance its performance. Examples include feature selection (e.g., using SelectKBest), regularization (e.g., L1 or L2 regularization), or ensemble methods (e.g., Gradient Boosting or Stacking).\n",
    "\n",
    "\n",
    "3. Modify the code to include the chosen techniques. Make sure to appropriately apply the techniques, train the modified model, and evaluate its performance.\n",
    "\n",
    "\n",
    "4. Calculate and compare the evaluation metrics such as RMSE, MAE, and R-squared for the basic model, the previous improved model, and the modified model.\n",
    "\n",
    "\n",
    "5. Reflect on the results and analyze the impact of the additional techniques on the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0cc500",
   "metadata": {},
   "source": [
    "#### Challenge:\n",
    "\n",
    "Experiment with different parameter settings or combinations of the additional techniques to find the best-performing model. Compare the performance of different combinations and discuss the findings in terms of the evaluation metrics.\n",
    "\n",
    "Note: You can refer to the scikit-learn documentation for more information about the techniques and parameters you choose to incorporate.\n",
    "\n",
    "By completing this exercise, you will gain hands-on experience in modifying and improving the model's performance using various techniques. It will also help deepen your understanding of how different techniques impact the evaluation metrics and overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4048760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
