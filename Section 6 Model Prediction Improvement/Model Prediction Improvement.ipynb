{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790589b8",
   "metadata": {},
   "source": [
    "# Model Prediction Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd865073",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba8ffcd",
   "metadata": {},
   "source": [
    "There are several methods that can help improve the prediction performance of models. Here are some commonly used techniques:\n",
    "   \n",
    "1. **Data Augmentation**: This refers to techniques that increase the amount of data by adding slightly modified copies of already existing data. For example, in image processing, these techniques could include rotation, scaling, flipping, etc. In text data, it can include methods like back translation or synonym replacement.\n",
    "\n",
    "\n",
    "2. **Data Cleaning**: This involves taking care of missing values (by either filling them in based on existing data, or removing the data points entirely), and handling outliers (which might distort the training of the model).\n",
    "\n",
    "\n",
    "3. **Feature engineering**: This is the process of creating new features from existing data that can help improve model performance. This can involve transformations of existing features, creating interaction features, or any other kind of data manipulation that creates new, useful input for the model.\n",
    "\n",
    "\n",
    "4. **Model Selection**: This involves choosing the right machine learning algorithm for your specific problem. This could be a linear regression model, a decision tree, a neural network, etc. The choice depends on the nature of your data and the problem you're trying to solve.\n",
    "\n",
    "\n",
    "5. **Hyperparameter tuning**: Hyperparameters are parameters that are not learned from the data but are set before the training process. Examples are learning rate, number of layers in a neural network, number of clusters in a K-means clustering, etc. Tuning these can often significantly improve performance. Techniques for hyperparameter tuning include grid search, random search, and more advanced methods like Bayesian optimization.\n",
    "\n",
    "\n",
    "6. **Cross-validation**: This is a resampling procedure used to evaluate the performance of a model on a limited data sample. The dataset is partitioned into 'k' equally sized folds, and the model is trained on 'k-1' folds, and the remaining fold is used for testing. This process is repeated 'k' times so that we obtain a model performance score for each fold. It helps in assessing how the results of a statistical analysis will generalize to an independent data set.\n",
    "\n",
    "\n",
    "7. **Regularization**: This is a technique used to prevent overfitting, which is when a model performs well on the training data but poorly on unseen data. Regularization works by adding a penalty term to the loss function that increases as the complexity of the model increases.\n",
    "\n",
    "\n",
    "8. **Ensemble your model**: This refers to combining different models to improve overall performance. Techniques include Bagging (e.g., Random Forest), Boosting (e.g., Gradient Boosting, XGBoost), and Stacking.\n",
    "\n",
    "\n",
    "Since we have already covered data cleaning, feature engineering in the previous sections, our attention in this section will shift to other topics, including data augmentation, model selection, ensemble model, regularization, cross-validation and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c5639",
   "metadata": {},
   "source": [
    "# 2. Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfff27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Create a dataframe\n",
    "# \"digits.data\" contains the features and \"digits.target\" contains the target\n",
    "df = pd.DataFrame(data= np.c_[digits['data'], digits['target']],\n",
    "                  columns= digits['feature_names'] + ['target'])\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X = df[digits['feature_names']]\n",
    "y = df['target']\n",
    "\n",
    "# Display the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ca008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de7f81",
   "metadata": {},
   "source": [
    "As we can see, The dataset does **NOT** contain any NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec576de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952ed06",
   "metadata": {},
   "source": [
    "# 3. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198d0fb",
   "metadata": {},
   "source": [
    "The `augment_data` function is defined to perform data augmentation. It takes the original images and labels as input and generates augmented versions of each image. The augmentation includes adding the original image, its horizontal flip, and a 90-degree rotation. The augmented images and labels are stored in `augmented_images` and `augmented_labels`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4133f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digit dataset\n",
    "digits = load_digits()\n",
    "images = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "# Data augmentation (optional)\n",
    "def augment_data(images, labels):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    for image, label in zip(images, labels):\n",
    "        augmented_images.append(image)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        augmented_images.append(np.fliplr(image))\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        augmented_images.append(np.rot90(image, k=1))\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_labels)\n",
    "\n",
    "augmented_images, augmented_labels = augment_data(images, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051281d2",
   "metadata": {},
   "source": [
    "This code combines the original images and their augmented versions into a single dataset, resulting in `all_images` and `all_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a798ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original and augmented data\n",
    "all_images = np.concatenate([images, augmented_images])\n",
    "all_labels = np.concatenate([labels, augmented_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b4e0b",
   "metadata": {},
   "source": [
    "The `plot_images` function is defined to visualize the original images and their augmented counterparts. It uses Matplotlib to create a grid of images, with the number of rows and columns specified by `rows` and `cols`. The function displays `num_samples` samples of original and augmented images side by side for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934e37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original images and their augmented counterparts\n",
    "def plot_images(images, labels, rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i], cmap='gray')\n",
    "        ax.set_title(f\"Label: {labels[i]}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "num_samples = 10# Number of samples to visualize for each category\n",
    "original_images_sample = images[:num_samples]\n",
    "augmented_images_sample = augmented_images[:num_samples]\n",
    "\n",
    "plot_images(original_images_sample, labels[:num_samples], 1, num_samples)\n",
    "plot_images(augmented_images_sample, labels[:num_samples], 1, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d425fa8",
   "metadata": {},
   "source": [
    "# 4. Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e937da",
   "metadata": {},
   "source": [
    "The digits dataset from sklearn is a clean dataset, meaning it `doesn't have missing values`, it `doesn't contain categorical features` that need to be encoded, and it `doesn't have obvious outliers`. Therefore, some pre-processing steps like handling missing values, encoding categorical variables, or outlier treatment are not applicable in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287258e",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfaea12",
   "metadata": {},
   "source": [
    "The digits dataset is a set of 8x8 pixel images, and each pixel in the image is a feature. There are a total of 64 features for each image. These features are already in a form that's suitable for machine learning models, so it's typically not necessary to do additional feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99afc07",
   "metadata": {},
   "source": [
    "# 6. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec500d8d",
   "metadata": {},
   "source": [
    "In this code, we load the digits dataset, split it into training and testing sets, and then iterate over four different models: K-Nearest Neighbors, Support Vector Machine, Random Forest, and Multi-Layer Perceptron. For each model, we train it on the training data, make predictions on the test data, calculate the accuracy, and print a classification report.\n",
    "\n",
    "The classification report provides more detailed metrics such as precision, recall, and F1-score for each class. It is useful for understanding how well the model performs for individual digits.\n",
    "\n",
    "After evaluating the models, the code also visualizes some example predictions for the first few test samples to give you an idea of how well the models are performing on specific digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e9f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a list of model names and corresponding classifier objects\n",
    "models = [\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('Support Vector Machine', SVC()),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Multi-Layer Perceptron', MLPClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Accuracy'])\n",
    "\n",
    "# Loop over the models\n",
    "for model_name, model in models:\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    results_df = results_df.append({'Model': model_name, 'Accuracy': accuracy}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b3d62",
   "metadata": {},
   "source": [
    "The `sort_values` function is used to sort the DataFrame in descending order based on the 'Accuracy' column. The `reset_index(drop=True)` function is used to reset the index of the DataFrame after sorting, so the index starts from 0 without any gaps.\n",
    "\n",
    "Now, the `results_df` DataFrame will be ranked based on the accuracy of each model, with the highest accuracy at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774acc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26990234",
   "metadata": {},
   "source": [
    "# 7. Hyperparameter Tuning using GridSearchCV with Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b238f5",
   "metadata": {},
   "source": [
    "## 7.1 Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d3d8e",
   "metadata": {},
   "source": [
    "## 7.2 Hyperparameter Tuning using GridSearchCV with Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, return_train_score=True, n_jobs=-1)\n",
    "grid_search.fit(X_train.reshape(len(X_train), -1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac68f99b",
   "metadata": {},
   "source": [
    "## 7.3 Visualize Hyperparameter Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = grid_search.cv_results_['mean_test_score'].reshape(len(param_grid['C']), len(param_grid['gamma']))\n",
    "sns.heatmap(scores, annot=True, fmt='.3f', xticklabels=param_grid['gamma'], yticklabels=param_grid['C'])\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('C')\n",
    "plt.title('Hyperparameter Tuning Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05fb723",
   "metadata": {},
   "source": [
    "## 7.4 Best Model from Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model from Hyperparameter Tuning\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ffe23",
   "metadata": {},
   "source": [
    "# 8. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbb05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores = cross_val_score(best_model, X_train.reshape(len(X_train), -1), y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b1884",
   "metadata": {},
   "source": [
    "# 9. Ensemble Techniques (Bagging and Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78410738",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_model = BaggingClassifier(base_estimator=best_model, n_estimators=10, random_state=42)\n",
    "voting_model = VotingClassifier([('svm', best_model), ('bagging', bagging_model)])\n",
    "\n",
    "# Train the Bagging and Voting models on the entire training data\n",
    "bagging_model.fit(X_train.reshape(len(X_train), -1), y_train)\n",
    "voting_model.fit(X_train.reshape(len(X_train), -1), y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cecf2a",
   "metadata": {},
   "source": [
    "# 10. Evaluate the Bagging and Voting models on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f070f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bagging = bagging_model.predict(X_test.reshape(len(X_test), -1))\n",
    "y_pred_voting = voting_model.predict(X_test.reshape(len(X_test), -1))\n",
    "\n",
    "test_accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "test_accuracy_voting = accuracy_score(y_test, y_pred_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d93b939",
   "metadata": {},
   "source": [
    "# 11. Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eeb9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Cross-Validation Scores:\", cross_val_scores)\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Test Accuracy (Best Model):\", best_model.score(X_test.reshape(len(X_test), -1), y_test))\n",
    "print(\"Test Accuracy (Bagging Model):\", test_accuracy_bagging)\n",
    "print(\"Test Accuracy (Voting Model):\", test_accuracy_voting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
