{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNYFmag2f3aYabzVMthAYe/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLcmore2023/MLcmore2023/blob/main/day10_pm_afternoon/Week2_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Week 2 Reinforcement Learning - Evaluation\n",
        "\n",
        "**Email your solutions in PDF (or Python Notebook) to `matin.moezzi@mail.utoronto.ca`**\n",
        "\n",
        "**Dynamic Programming in RL:**\n",
        "1. Which RL technique uses a complete model of the environment and finds the optimal policy in a deterministic way?\n",
        "\n",
        "   a) Monte Carlo Methods\n",
        "\n",
        "   b) Temporal Difference Learning\n",
        "\n",
        "   c) Dynamic Programming\n",
        "\n",
        "   d) Q-learning\n",
        "\n",
        "**Policy Iteration:**\n",
        "2. Which of the following best describes the steps of policy iteration?\n",
        "\n",
        "   a) Policy Evaluation, Policy Stabilization\n",
        "\n",
        "   b) Policy Stabilization, Policy Degradation\n",
        "\n",
        "   c) Policy Evaluation, Policy Improvement\n",
        "\n",
        "   d) Policy Initialization, Policy Convergence\n",
        "\n",
        "3. In policy iteration, when can we guarantee that policy improvement stops altering the policy?\n",
        "\n",
        "   a) When the state values are maximized\n",
        "\n",
        "   b) When the action values are maximized\n",
        "\n",
        "   c) When the policy achieves a local optimum\n",
        "\n",
        "   d) When two consecutive policies are identical\n",
        "\n",
        "**Value Iteration:**\n",
        "4. In value iteration, the primary difference from policy iteration is:\n",
        "\n",
        "   a) It doesn't require a model of the environment.\n",
        "\n",
        "   b) It doesn't have the policy evaluation step.\n",
        "\n",
        "   c) It uses a different form of the Bellman equation.\n",
        "\n",
        "   d) It doesn't try to improve the policy.\n",
        "\n",
        "5. In value iteration, what is repeatedly updated until convergence?\n",
        "\n",
        "   a) Policy\n",
        "\n",
        "   b) Value Function\n",
        "\n",
        "   c) Reward Function\n",
        "\n",
        "   d) Model of the environment\n",
        "\n",
        "**Exploration-Exploitation Trade-off:**\n",
        "6. What is a common method to address the exploration-exploitation dilemma in RL?\n",
        "\n",
        "   a) Bellman Expectation Equation\n",
        "\n",
        "   b) ε-greedy policy\n",
        "\n",
        "   c) Policy Evaluation\n",
        "\n",
        "   d) Value Iteration\n",
        "\n",
        "7. Which strategy always exploits its knowledge without exploring?\n",
        "\n",
        "   a) ε-greedy with ε=0.1\n",
        "\n",
        "   b) ε-greedy with ε=0\n",
        "\n",
        "   c) Softmax\n",
        "\n",
        "   d) None of them\n",
        "\n",
        "**Monte Carlo Methods:**\n",
        "8. The Monte Carlo method in RL:\n",
        "\n",
        "   a) Uses the Bellman equation for updates.\n",
        "\n",
        "   b) Requires full episodes to estimate values.\n",
        "\n",
        "   c) Can update values in the middle of an episode.\n",
        "\n",
        "   d) Always provides the optimal policy after one episode.\n",
        "\n",
        "9. Which of the following best describes the convergence speed difference between Monte Carlo and Temporal Difference (TD) learning methods?\n",
        "\n",
        "  a) Monte Carlo always converges faster because it uses entire episodes.\n",
        "\n",
        "  b) TD learning tends to converge faster due to its bootstrapping nature.\n",
        "\n",
        "  c) Both converge at the same speed because they rely on the same update mechanism.\n",
        "\n",
        "  d) Convergence speed primarily depends on the learning rate and not the algorithm.\n",
        "\n",
        "\n",
        "**TD Learning:**\n",
        "10. Which of the following is a TD learning method?\n",
        "\n",
        "   a) Monte Carlo\n",
        "\n",
        "   b) Deep Q Networks\n",
        "\n",
        "   c) PILCO\n",
        "\n",
        "   d) Value Iteration\n",
        "\n",
        "11. Which of the following is a model-free TD learning method?\n",
        "\n",
        "   a) SARSA\n",
        "\n",
        "   b) Value Iteration\n",
        "\n",
        "   c) Dynamic Programming\n",
        "\n",
        "   d) Policy Gradients\n",
        "\n",
        "**On-policy vs. Off-policy:**\n",
        "12. SARSA is an ______ method while Q-learning is an ______ method.\n",
        "\n",
        "   a) On-policy, On-policy\n",
        "\n",
        "   b) On-policy, Off-policy\n",
        "\n",
        "   c) Off-policy, On-policy\n",
        "\n",
        "   d) Off-policy, Off-policy\n",
        "\n",
        "13. Which of the following is an on-policy method?\n",
        "\n",
        "   a) Q-learning\n",
        "\n",
        "   b) Expected-SARSA\n",
        "\n",
        "   c) Dyna-Q\n",
        "\n",
        "   d) DQN\n",
        "\n",
        "**Policy Gradient vs. Value-based:**\n",
        "14. Which type of algorithm directly optimizes the policy?\n",
        "\n",
        "   a) Value-based methods\n",
        "\n",
        "   b) Model-based methods\n",
        "\n",
        "   c) Policy gradient methods\n",
        "\n",
        "   d) Dynamic Programming\n",
        "  \n",
        "\n",
        "15. Which method directly optimizes the policy without using a value function?\n",
        "\n",
        "   a) Q-learning\n",
        "\n",
        "   b) Value Iteration\n",
        "\n",
        "   c) Policy Gradients\n",
        "\n",
        "   d) DQN\n",
        "\n",
        "**Model-free vs Model-based:**\n",
        "16. Which type of learning does not require knowledge about the model of the environment?\n",
        "\n",
        "   a) On-policy learning\n",
        "\n",
        "   b) Model-based learning\n",
        "\n",
        "   c) Model-free learning\n",
        "\n",
        "   d) Off-policy learning\n",
        "\n",
        "17. Which of the following methods requires a model of the environment?\n",
        "\n",
        "   a) Every-visit Monte Carlo Control\n",
        "\n",
        "   b) Dynamic Programming\n",
        "\n",
        "   c) Q-learning\n",
        "\n",
        "   d) REINFORCE\n",
        "\n",
        "**Epsilon-greedy Policy:**\n",
        "18. In an ε-greedy strategy, if ε=0.1, the agent will choose the best action with a probability of:\n",
        "\n",
        "   a) 10%\n",
        "\n",
        "   b) 90%\n",
        "\n",
        "   c) 50%\n",
        "\n",
        "   d) 0%\n",
        "\n",
        "19. When ε=1 in an ε-greedy policy, the agent will:\n",
        "\n",
        "   a) Always exploit\n",
        "\n",
        "   b) Always explore\n",
        "\n",
        "   c) Neither explore nor exploit\n",
        "\n",
        "   d) Exploit with 0.1 probability\n",
        "\n",
        "20. Which algorithm uses the Q-value of the actual next action taken, making it susceptible to short-term variations in rewards?\n",
        "\n",
        "  a) Expected-SARSA\n",
        "\n",
        "  b) Q-learning\n",
        "\n",
        "  c) SARSA\n",
        "\n",
        "  d) Double Q-learning\n",
        "\n",
        "**Stochastic vs. Deterministic Environments:**\n",
        "\n",
        "21. In which environment is the next state uncertain given a state and action?\n",
        "\n",
        "   a) Deterministic\n",
        "\n",
        "   b) Stochastic\n",
        "\n",
        "   c) Both\n",
        "\n",
        "   d) Neither\n",
        "\n",
        "**Discrete vs Continuous Actions:**\n",
        "22. Q-learning is typically applied to problems with:\n",
        "\n",
        "   a) Continuous action spaces\n",
        "\n",
        "   b) Discrete action spaces\n",
        "\n",
        "   c) Continuous state spaces\n",
        "\n",
        "   d) Infinite state spaces\n",
        "\n",
        "23. Which algorithm is specifically designed for continuous action spaces?\n",
        "\n",
        "   a) DQN\n",
        "\n",
        "   b) Q-learning\n",
        "\n",
        "   c) REINFORCE\n",
        "\n",
        "   d) Policy iteration\n",
        "\n",
        "**Temporal Difference Learning:**\n",
        "24. Which of the following updates the value function based on an estimate from the current state and reward, without waiting for the final outcome?\n",
        "\n",
        "   a) Monte Carlo Methods\n",
        "\n",
        "   b) Dynamic Programming\n",
        "\n",
        "   c) TD Learning\n",
        "\n",
        "   d) Model-based RL\n",
        "\n",
        "**Convergence of Algorithms:**\n",
        "25. Which of the following guarantees convergence to the optimal policy when used with function approximation?\n",
        "\n",
        "   a) Q-learning\n",
        "\n",
        "   b) SARSA\n",
        "\n",
        "   c) Neither\n",
        "\n",
        "   d) Both\n",
        "\n",
        "**Variance and Bias in RL:**\n",
        "26. Monte Carlo methods typically have:\n",
        "\n",
        "   a) High bias and low variance.\n",
        "\n",
        "   b) Low bias and high variance.\n",
        "\n",
        "   c) Low bias and low variance.\n",
        "\n",
        "   d) High bias and high variance.\n",
        "\n",
        "**High Dimensionality:**\n",
        "27. Which of the following is best suited for high dimensional state spaces?\n",
        "\n",
        "   a) Tabular methods\n",
        "\n",
        "   b) Deep Q-Networks\n",
        "\n",
        "   c) Classical Dynamic Programming\n",
        "\n",
        "   d) Monte Carlo with enumerative states\n",
        "\n",
        "**Sample Complexity:**\n",
        "28. Which method typically requires more samples for convergence in large state spaces?\n",
        "\n",
        "   a) Q-learning\n",
        "\n",
        "   b) Dynamic Programming\n",
        "\n",
        "   c) Policy Gradient methods\n",
        "\n",
        "   d) Model-based methods\n",
        "\n",
        "**Model-free Learning:**\n",
        "29. In which scenario would model-free methods be preferable over model-based?\n",
        "\n",
        "   a) When the environment model is completely known.\n",
        "\n",
        "   b) When we need to plan multiple steps ahead.\n",
        "\n",
        "   c) When the environment is hard to model or changes over time.\n",
        "\n",
        "   d) When sample efficiency is the highest priority.\n",
        "\n",
        "**Complexity of Algorithms:**\n",
        "30. What are potential sources of challenge dubbed as the \"curse of dimensionality\" in RL?\n",
        "\n",
        "   a) Stochasticity in policy to be learned.\n",
        "\n",
        "   b) Exploration and exploitation dilemma.\n",
        "\n",
        "   c) High dimensionality of action space.\n",
        "\n",
        "   d) Bias in sample during training stage.\n",
        "\n",
        "\n",
        "31.  Which of the following is true about SARSA?\n",
        "   a) It is an off-policy algorithm.\n",
        "\n",
        "   b) It learns the Q-value based on the next action that's actually taken.\n",
        "\n",
        "   c) It is used primarily for policy optimization.\n",
        "\n",
        "   d) It doesn't consider the current policy when updating.\n",
        "\n",
        "32.  In First-visit Monte Carlo, how is the value of a state estimated?\n",
        "\n",
        "   a) Using the average of returns from all visits to the state.\n",
        "\n",
        "   b) Using the return from the first visit to the state in an episode.\n",
        "\n",
        "   c) Using the latest return from the state.\n",
        "\n",
        "   d) Using a weighted average of all returns.\n",
        "\n",
        "33.  Which of the following describes Every-visit Monte Carlo?\n",
        "\n",
        "   a) It updates values based on every occurrence of a state in an episode.\n",
        "\n",
        "   b) It only uses the first occurrence of a state to update its value.\n",
        "\n",
        "   c) It doesn't require episodes to terminate.\n",
        "\n",
        "   d) It uses a model of the environment.\n",
        "\n",
        "34. Off-policy Monte Carlo methods require which of the following techniques to correct for the difference in the policy used to generate behavior and the policy being improved?\n",
        "\n",
        "   a) ε-greedy\n",
        "\n",
        "   b) Policy iteration\n",
        "\n",
        "   c) Importance sampling\n",
        "\n",
        "   d) Every-visit Monte Carlo\n",
        "\n",
        "35.  In Generalized Policy Iteration, what two processes happen alternately?\n",
        "\n",
        "   a) Policy degradation and value estimation\n",
        "\n",
        "   b) Policy evaluation and policy improvement\n",
        "\n",
        "   c) Policy stabilization and value degradation\n",
        "\n",
        "   d) Value iteration and policy iteration\n",
        "\n",
        "36. What is the primary purpose of importance sampling in RL?\n",
        "\n",
        "   a) To speed up learning\n",
        "\n",
        "   b) To estimate the expectation under one distribution using samples from\n",
        "   another\n",
        "\n",
        "   c) To reduce variance in Monte Carlo methods\n",
        "\n",
        "   d) To balance exploration and exploitation\n",
        "\n",
        "37. In the Actor-Critic method, what role does the \"critic\" play?\n",
        "\n",
        "   a) Selecting actions\n",
        "\n",
        "   b) Evaluating the current policy\n",
        "\n",
        "   c) Adjusting the learning rate\n",
        "\n",
        "   d) Managing memory replay\n",
        "\n",
        "39. Minimax-Q is a Q-learning adaptation for which kind of environments?\n",
        "\n",
        "   a) Cooperative multi-agent\n",
        "\n",
        "   b) Competitive multi-agent\n",
        "\n",
        "   c) Single agent with a deterministic environment\n",
        "\n",
        "   d) Single agent with a stochastic environment\n",
        "\n",
        "40. What advantage does using a baseline in the REINFORCE algorithm provide?\n",
        "\n",
        "   a) It makes the policy deterministic.\n",
        "\n",
        "   b) It reduces the variance of the gradient estimates.\n",
        "\n",
        "   c) It speeds up the learning process by doubling the learning rate.\n",
        "\n",
        "   d) It allows for continuous action spaces.\n",
        "\n"
      ],
      "metadata": {
        "id": "B3PailmqXFPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Questions"
      ],
      "metadata": {
        "id": "zKIfNqYHjqQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Q-learning:**\n",
        "\n",
        "Given a transition `(s, a, s', r)`, update the Q-value:\n",
        "\n",
        "```python\n",
        "def q_learning_update(Q, s, a, r, s_prime, alpha, gamma):\n",
        "    # Complete the code for Q-value update using Q-learning\n",
        "    Q[s][a] = _________________________________\n",
        "    return Q\n",
        "```\n",
        "\n",
        "**2. SARSA:**\n",
        "\n",
        "Implement the SARSA update:\n",
        "\n",
        "```python\n",
        "def sarsa_update(Q, s, a, r, s_prime, a_prime, alpha, gamma):\n",
        "    # Complete the code for Q-value update using SARSA\n",
        "    Q[s][a] = _________________________________\n",
        "    return Q\n",
        "```\n",
        "\n",
        "**3. Expected SARSA:**\n",
        "\n",
        "Implement the Expected SARSA update:\n",
        "\n",
        "```python\n",
        "def expected_sarsa_update(Q, s, a, r, s_prime, policy, alpha, gamma):\n",
        "    expected_value = _________________________\n",
        "    \n",
        "    # Expected SARSA update\n",
        "    Q[s][a] = _________________________________\n",
        "    return Q\n",
        "```\n",
        "\n",
        "**4. Off-Policy SARSA:**\n",
        "\n",
        "Given a behavior policy, implement off-policy SARSA:\n",
        "\n",
        "```python\n",
        "def off_policy_sarsa_update(Q, s, a, r, s_prime, a_prime target_policy, behavior_policy, alpha, gamma):\n",
        "    importance_sampling_ratio = __________________________\n",
        "\n",
        "    # Off-Policy SARSA update\n",
        "    Q[s][a] = _________________________________\n",
        "    return Q\n",
        "```\n",
        "\n",
        "**5. Epsilon-Greedy Policy:**\n",
        "\n",
        "Implement the epsilon-greedy action selection:\n",
        "\n",
        "```python\n",
        "def epsilon_greedy(Q, state, Q, epsilon):\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        return _________________________________\n",
        "    else:\n",
        "        return _________________________________\n",
        "```\n",
        "\n",
        "**6. Policy Iteration:**\n",
        "\n",
        "Given the value function `V` and transition probabilities `P`, implement the policy iteration:\n",
        "\n",
        "```python\n",
        "def policy_iteration(P, V, s, gamma):\n",
        "    # Policy Evaluation\n",
        "    delta = float('inf')\n",
        "    while delta > 0.01:\n",
        "        delta = 0\n",
        "        for state in P:\n",
        "            v = V[state]\n",
        "            V[state] = _________________________________\n",
        "            delta = max(delta, abs(v - V[state]))\n",
        "\n",
        "    # Policy Improvement\n",
        "    policy_stable = True\n",
        "    for state in P:\n",
        "        old_action = policy[state]\n",
        "        policy[state] = _________________________________\n",
        "        if old_action != policy[state]:\n",
        "            policy_stable = False\n",
        "\n",
        "    return V, policy, policy_stable\n",
        "```\n",
        "\n",
        "\n",
        "**7. Dyna-SARSA**\n",
        "\n",
        "Implement SARSA version of Dyna-Q\n",
        "```python\n",
        "class Dyna-SARSA:\n",
        "    def __init__(self, alpha=0.1, gamma=0.95, epsilon=0.1, planning_n=5):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.planning_n = planning_n\n",
        "\n",
        "        self.Q = defaultdict(float)\n",
        "        self.model = defaultdict(tuple)\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, next_action):\n",
        "        \n",
        "        self.Q[(state, action)] += __________________________\n",
        "\n",
        "        # Update the model\n",
        "        self.model[(state, action)] = (reward, next_state)\n",
        "\n",
        "        # Plan using the model\n",
        "        for _ in range(self.planning_n):\n",
        "            state_sample, action_sample = random.choice(list(self.model.keys()))\n",
        "            reward_sample, next_state_sample = self.model[(state_sample, action_sample)]\n",
        "            \n",
        "            self.Q[(state_sample, action_sample)] += __________________\n",
        "```\n"
      ],
      "metadata": {
        "id": "AOFXjRj1ddFl"
      }
    }
  ]
}