{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLcmore2023/MLcmore2023/blob/main/day3_pm_afternoon/naive-bayes-demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes\n",
        "Naive Bayes is a supervised machine learning algorithm used for classification. It relies on the principle of Bayes' theorem and assumes independence between the features. The algorithm calculates the probability of each class for a given data point and assigns it to the class with the highest probability. It starts by estimating the prior probabilities and likelihoods from the training data. During the prediction phase, it applies these probabilities to classify new instances based on the calculated posterior probabilities.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/naivebayes1.png\">\n"
      ],
      "metadata": {
        "id": "R4rqjn_RK0HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries and initialize random generator"
      ],
      "metadata": {
        "id": "L7pC0nbqK0HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:35:21.403248Z",
          "iopub.execute_input": "2023-07-12T18:35:21.403679Z",
          "iopub.status.idle": "2023-07-12T18:35:21.409869Z",
          "shell.execute_reply.started": "2023-07-12T18:35:21.403648Z",
          "shell.execute_reply": "2023-07-12T18:35:21.408614Z"
        },
        "trusted": true,
        "id": "LG6gHXpyK0HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read data with sklearn\n",
        "we will use the flower iris dataset. This data sets consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length\n",
        "\n"
      ],
      "metadata": {
        "id": "sR9cQYUaK0HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:35:21.415253Z",
          "iopub.execute_input": "2023-07-12T18:35:21.415566Z",
          "iopub.status.idle": "2023-07-12T18:35:21.421106Z",
          "shell.execute_reply.started": "2023-07-12T18:35:21.415540Z",
          "shell.execute_reply": "2023-07-12T18:35:21.420276Z"
        },
        "trusted": true,
        "id": "O-n272GxK0HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split dataset"
      ],
      "metadata": {
        "id": "Nq_w9ADbK0HN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:35:21.422482Z",
          "iopub.execute_input": "2023-07-12T18:35:21.422780Z",
          "iopub.status.idle": "2023-07-12T18:35:21.433308Z",
          "shell.execute_reply.started": "2023-07-12T18:35:21.422754Z",
          "shell.execute_reply": "2023-07-12T18:35:21.432297Z"
        },
        "trusted": true,
        "id": "YMN6zBZpK0HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes algorithm\n",
        "Steps:\n",
        "1. Fit: Calculate the summary statistics and the prior for each class in the (training) dataset\n",
        "2. Predict: Calculate the probability of every class for each sample in the (test) dataset. Therefore, get the probability of data given the classes’ (Gaussian) distribution and combine it with the prior.\n"
      ],
      "metadata": {
        "id": "cwmcITupK0HO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating summary statistics\n",
        "We will calculate the summary statistics for each class (and feature) as well as the prior"
      ],
      "metadata": {
        "id": "6naPyHhLK0HP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples, n_features = X_train.shape # get number of samples (rows) and features (columns)\n",
        "print(n_samples,n_features)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:35:21.435075Z",
          "iopub.execute_input": "2023-07-12T18:35:21.435394Z",
          "iopub.status.idle": "2023-07-12T18:35:21.447219Z",
          "shell.execute_reply.started": "2023-07-12T18:35:21.435368Z",
          "shell.execute_reply": "2023-07-12T18:35:21.446399Z"
        },
        "trusted": true,
        "id": "e4OL7OKgK0HQ",
        "outputId": "aa4f6ad9-2f36-4902-f150-32c553a73271"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "120 4\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(np.unique(y_train)) # get number of uniques classes\n",
        "print(n_classes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:35:21.448255Z",
          "iopub.execute_input": "2023-07-12T18:35:21.448663Z",
          "iopub.status.idle": "2023-07-12T18:35:21.459612Z",
          "shell.execute_reply.started": "2023-07-12T18:35:21.448612Z",
          "shell.execute_reply": "2023-07-12T18:35:21.458386Z"
        },
        "trusted": true,
        "id": "zBTy_uBHK0HS",
        "outputId": "b7705bef-ca5b-4d91-83fe-2ca3221f8c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "3\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$$\n",
        "P(y|x_1, x_2, \\ldots, x_n) = \\frac{P(y) \\cdot P(x_1, x_2, \\ldots, x_n |y)}{P(x_1, x_2, \\ldots, x_n)}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $P(y|x_1, x_2, \\ldots, x_n)$ is the posterior probability of class $y$ given the features $x_1, x_2, \\ldots, x_n$.\n",
        "- $P(y)$ is the prior probability of class $y$.\n",
        "- $P(x_i|y)$ is the probability of feature $x_i$ given class $y$.\n",
        "- $P(x_1, x_2, \\ldots, x_n)$ is the probability of the features $x_1, x_2, \\ldots, x_n$ occurring together.\n",
        "\n",
        "The naive assumption of conditional independence allows us to simplify the equation:\n",
        "\n",
        "$$\n",
        "P(y|x_1, x_2, \\ldots, x_n) = \\frac{P(y) \\cdot P(x_1|y) \\cdot P(x_2|y) \\cdot \\ldots \\cdot P(x_n|y)}{P(x_1, x_2, \\ldots, x_n)}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "k7XxWRbbK0HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create three zero-matrices to store summary stats & prior\n",
        "mean = np.zeros((n_classes, n_features))\n",
        "variance = np.zeros((n_classes, n_features))\n",
        "priors = np.zeros(n_classes)\n",
        "print(\"mean:\\n\",mean)\n",
        "print(\"variance:\\n\",variance)\n",
        "print(\"priors:\\n\",priors)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:35:21.461316Z",
          "iopub.execute_input": "2023-07-12T18:35:21.462160Z",
          "iopub.status.idle": "2023-07-12T18:35:21.471779Z",
          "shell.execute_reply.started": "2023-07-12T18:35:21.462129Z",
          "shell.execute_reply": "2023-07-12T18:35:21.470783Z"
        },
        "trusted": true,
        "id": "ntaiHpLYK0HT",
        "outputId": "f1292fd0-76a2-4712-9155-69368d483963"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "mean:\n [[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\nvariance:\n [[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\npriors:\n [0. 0. 0.]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for class_ind in range(n_classes):\n",
        "    # create a subset of data for the specific class 'class_ind'\n",
        "    X_class = X_train[y_train == class_ind]\n",
        "\n",
        "    # calculate statistics and update zero-matrices, rows=classes, cols=features\n",
        "    mean[class_ind, :] = np.mean(X_class, axis=0)\n",
        "    variance[class_ind, :] = np.var(X_class, axis=0)\n",
        "    priors[class_ind] = X_class.shape[0] / n_samples\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:35:21.473725Z",
          "iopub.execute_input": "2023-07-12T18:35:21.474074Z",
          "iopub.status.idle": "2023-07-12T18:35:21.487251Z",
          "shell.execute_reply.started": "2023-07-12T18:35:21.474040Z",
          "shell.execute_reply": "2023-07-12T18:35:21.486272Z"
        },
        "trusted": true,
        "id": "_0Rzew5jK0HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"mean:\\n\",mean)\n",
        "print(\"variance:\\n\",variance)\n",
        "print(\"priors:\\n\",priors)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:35:21.488780Z",
          "iopub.execute_input": "2023-07-12T18:35:21.489475Z",
          "iopub.status.idle": "2023-07-12T18:35:21.499902Z",
          "shell.execute_reply.started": "2023-07-12T18:35:21.489435Z",
          "shell.execute_reply": "2023-07-12T18:35:21.499059Z"
        },
        "trusted": true,
        "id": "eSm5SyghK0HU",
        "outputId": "d090553a-0e80-408f-c0bb-69b25a177254"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "mean:\n [[4.96153846 3.36666667 1.46666667 0.23333333]\n [5.94594595 2.73243243 4.22972973 1.30540541]\n [6.525      2.95227273 5.53409091 2.02045455]]\nvariance:\n [[0.10749507 0.13042735 0.02529915 0.01094017]\n [0.274916   0.1065157  0.21344047 0.03943024]\n [0.38642045 0.09204029 0.30315599 0.07889979]]\npriors:\n [0.325      0.30833333 0.36666667]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gaussian distribution function\n",
        "In order to make a prediction, we need to get the probability of data belonging to a certain class. Calculating the probability of observing a given real-value is difficult. One way we can do this is to assume that the values are drawn from a distribution, such as a bell curve or Gaussian distribution This allows us to calculate the probability of a value coming from the distribution.\n",
        "\n",
        "For simplicity, we assume the data's underlying distribution is gaussian (normal). If the data's underlying distribution is not gaussian, the real distribution should  be used (such as bernoulli, multinomial, ...)\n",
        "\n",
        "\n",
        "$$\n",
        "f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\, e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
        "$$"
      ],
      "metadata": {
        "id": "AfvJ_IJKK0HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_density(x, mean, var):\n",
        "    # implementation of gaussian density function\n",
        "    const = 1 / np.sqrt(var * 2 * np.pi)\n",
        "    proba = np.exp(-0.5 * ((x - mean) ** 2 / var))\n",
        "    return const * proba"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:35:21.501936Z",
          "iopub.execute_input": "2023-07-12T18:35:21.502894Z",
          "iopub.status.idle": "2023-07-12T18:35:21.510766Z",
          "shell.execute_reply.started": "2023-07-12T18:35:21.502854Z",
          "shell.execute_reply": "2023-07-12T18:35:21.509986Z"
        },
        "trusted": true,
        "id": "Dn8raAnlK0HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating (numerator part of) the posterior probability\n",
        "Recall the posterior equation:\n",
        "$$\n",
        "P(y|x_1, x_2, \\ldots, x_n) = \\frac{P(y) \\cdot P(x_1|y) \\cdot P(x_2|y) \\cdot \\ldots \\cdot P(x_n|y)}{P(x_1, x_2, \\ldots, x_n)}\n",
        "$$\n",
        "\n",
        "In the code below, we are actually just calculating the numerator of the right hand side. The reason for why is left as an exercise.\n",
        "$$\n",
        " {P(y) \\cdot P(x_1|y) \\cdot P(x_2|y) \\cdot \\ldots \\cdot P(x_n|y)}\n",
        " $$\n",
        "\n",
        "note"
      ],
      "metadata": {
        "id": "-3umwGjsK0HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([5.8, 4.0,  1.2, 0.2])\n",
        "for class_ind in range(n_classes):\n",
        "    # get summary stats & prior\n",
        "    class_means = mean[class_ind]\n",
        "    class_variances = variance[class_ind]\n",
        "    class_priors = priors[class_ind]\n",
        "\n",
        "    probability_of_coming_from_distribution = gaussian_density(x, class_means, class_variances)\n",
        "    print(probability_of_coming_from_distribution )\n",
        "\n",
        "    product = np.prod(probability_of_coming_from_distribution)\n",
        "    # calculate new posterior & append to list\n",
        "    posterior = class_priors * product\n",
        "    print(\"(numerator of the) posterior of being class\",class_ind,\"is =\",posterior)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:35:21.511862Z",
          "iopub.execute_input": "2023-07-12T18:35:21.512364Z",
          "iopub.status.idle": "2023-07-12T18:35:21.525040Z",
          "shell.execute_reply.started": "2023-07-12T18:35:21.512337Z",
          "shell.execute_reply": "2023-07-12T18:35:21.523943Z"
        },
        "trusted": true,
        "id": "OqDaF-gSK0HV",
        "outputId": "26e79f53-d3b7-4096-d30b-da50cbb32380"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[0.04624586 0.23736665 0.61517309 3.62530037]\n(numerator of the) posterior of being class 0 is = 0.007956411260335297\n\n\n[7.31957233e-01 6.48131671e-04 3.95908867e-10 3.74734639e-07]\n(numerator of the) posterior of being class 1 is = 2.170143722975343e-20\n\n\n[3.25092838e-01 3.38128400e-03 2.54133428e-14 1.07513501e-09]\n(numerator of the) posterior of being class 2 is = 1.1012483739537062e-26\n\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction function\n",
        "To make predictions, we need to get the probability of an example belonging to a certain class (i.e. coming from the same distribution).\n",
        "\n",
        "To classify a new instance, we select the class with the highest (numerator part of the) posterior probability. This can be written as:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\underset{y}{\\arg\\max} \\left( P(y) \\cdot \\prod_{i=1}^{n} P(x_i|y) \\right)\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jyLdVdctK0HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_list, mean, variance, priors):\n",
        "    n_classes, n_features = mean.shape\n",
        "    y_hat = []\n",
        "\n",
        "    #X_list is a list of iris dimensions, so we will go over each of them\n",
        "    for x in X_list:\n",
        "        # store new posteriors for each class in a single list\n",
        "        posteriors = []\n",
        "\n",
        "        for class_ind in range(n_classes):\n",
        "            # get summary stats & prior\n",
        "            class_means = mean[class_ind]\n",
        "            class_variances = variance[class_ind]\n",
        "            class_priors = priors[class_ind]\n",
        "\n",
        "            probability_of_coming_from_distribution = gaussian_density(x, class_means, class_variances)\n",
        "\n",
        "            product = np.prod(probability_of_coming_from_distribution) # multiply every item of the array\n",
        "            # calculate new posterior & append to list\n",
        "            posterior = class_priors * product\n",
        "            posteriors.append(posterior)\n",
        "        # append the index with the highest class probability\n",
        "        y_hat.append(np.argmax(posteriors))\n",
        "\n",
        "    return np.array(y_hat)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:35:21.526340Z",
          "iopub.execute_input": "2023-07-12T18:35:21.527410Z",
          "iopub.status.idle": "2023-07-12T18:35:21.537018Z",
          "shell.execute_reply.started": "2023-07-12T18:35:21.527378Z",
          "shell.execute_reply": "2023-07-12T18:35:21.536104Z"
        },
        "trusted": true,
        "id": "Wx-yiMjdK0HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate model performance\n",
        "Accuracy, precision, recall, and F1 score are commonly used metrics in machine learning and statistics to evaluate the performance of classification models\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:667/1*3yGLac6F4mTENnj5dBNvNQ.jpeg\">\n",
        "\n",
        "- TP: True Positives\n",
        "- TN: True Negatives\n",
        "- FP: False Positives\n",
        "- FN: False Negatives\n",
        "\n",
        "#### Accuracy\n",
        "\n",
        "Accuracy measures the overall correctness of a classification model by calculating the ratio of correctly predicted instances to the total instances.However, accuracy might not be the best metric when dealing with imbalanced datasets, where one class is much more frequent than the other.\n",
        "\n",
        "Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
        "\n",
        "#### Precision\n",
        "\n",
        "Precision assesses the accuracy of positive predictions by calculating the ratio of true positives to all positive predictions. In other words, it indicates how many of the instances predicted as positive are actually true positives. Precision is particularly useful when the cost of false positives is high.\n",
        "\n",
        "Precision = $\\frac{TP}{TP + FP}$\n",
        "\n",
        "#### Recall\n",
        "Recall, also known as sensitivity or true positive rate, is the metric that focuses on the model's ability to correctly identify all relevant instances from the dataset. It measures the ratio of true positives to the total number of actual positive instances. Recall is especially important when the cost of false negatives is high. It's calculated as:\n",
        "\n",
        "Recall = $\\frac{TP}{TP + FN}$\n",
        "\n",
        "#### F1 score\n",
        "The F1 score is a combination of precision and recall into a single metric, providing a balance between them. It's the harmonic mean of precision and recall and is especially useful when you want to find a balance between identifying relevant instances and minimizing false positives. The F1 score ranges between 0 and 1, where a higher value indicates better performance.\n",
        "F1 Score = $\\frac{2 \\times Precision \\times Recall}{Precision + Recall}$\n",
        "\n"
      ],
      "metadata": {
        "id": "7de2zcobK0HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict(X_test, mean, variance, priors)\n",
        "\n",
        "total_samples = len(y_test)\n",
        "correct_predictions = 0\n",
        "true_positives = 0\n",
        "false_positives = 0\n",
        "false_negatives = 0\n",
        "\n",
        "for i in range(total_samples):\n",
        "    if y_test[i] == predictions[i]:\n",
        "        correct_predictions += 1\n",
        "        if y_test[i] == 1:\n",
        "            true_positives += 1\n",
        "    else:\n",
        "        if y_test[i] == 1:\n",
        "            false_negatives += 1\n",
        "        else:\n",
        "            false_positives += 1\n",
        "\n",
        "accuracy = correct_predictions / total_samples\n",
        "recall = true_positives / (true_positives + false_negatives)\n",
        "precision = true_positives / (true_positives + false_positives)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"F1 Score:\", f1_score)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-12T18:39:50.939672Z",
          "iopub.execute_input": "2023-07-12T18:39:50.940250Z",
          "iopub.status.idle": "2023-07-12T18:39:50.957641Z",
          "shell.execute_reply.started": "2023-07-12T18:39:50.940204Z",
          "shell.execute_reply": "2023-07-12T18:39:50.956119Z"
        },
        "trusted": true,
        "id": "egJo4a1AK0HX",
        "outputId": "33e5d9f3-cd14-4f9e-a38d-e0e8c5fa8f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 0.9666666666666667\nRecall: 0.9230769230769231\nPrecision: 1.0\nF1 Score: 0.9600000000000001\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "1. Under what condition is it suitable to use the gaussian distribution in naive bayes?\n",
        "2. Why did we only calculate the numerator part of the posterior in the prediction function? (hint: What is the goal for us to calculate the posteriors? does the actual values of the posteriors matter?)\n",
        "3. Notice when calculating the posteriors, we get really small numbers. When dealing with probabilities, especially when multiplying them together, the values can become very small. This can cause numerical issues or floating point calculation inaccuracies. Taking the logarithm of the probabilities helps avoid underflow issues by converting the multiplications into additions, which are computationally more stable. We can add the logarithms of the probabilities, which preserves the relative ordering of the probabilities while providing better numerical stability. Change the prediction function so it uses logarithms."
      ],
      "metadata": {
        "id": "3v4vfiY2K0HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exercise code here"
      ],
      "metadata": {
        "id": "hzJIhXtjae2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "- https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
        "- https://github.com/marvinlanhenke/DataScience/tree/main/MachineLearningFromScratch/NaiveBayes\n",
        "- https://towardsdatascience.com/implementing-naive-bayes-from-scratch-df5572e042ac\n",
        "- https://medium.com/@rangavamsi5/na%C3%AFve-bayes-algorithm-implementation-from-scratch-in-python-7b2cc39268b9\n",
        "- https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-ade299cf63cd"
      ],
      "metadata": {
        "id": "grswN2CvK0HY"
      }
    }
  ]
}