{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Naive Bayes\nNaive Bayes is a supervised machine learning algorithm used for classification. It relies on the principle of Bayes' theorem and assumes independence between the features. The algorithm calculates the probability of each class for a given data point and assigns it to the class with the highest probability. It starts by estimating the prior probabilities and likelihoods from the training data. During the prediction phase, it applies these probabilities to classify new instances based on the calculated posterior probabilities.\n\n<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/naivebayes1.png\">\n","metadata":{}},{"cell_type":"markdown","source":"### Import libraries and initialize random generator","metadata":{}},{"cell_type":"code","source":"import numpy as np\nnp.random.seed(0)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:21.403248Z","iopub.execute_input":"2023-07-12T18:35:21.403679Z","iopub.status.idle":"2023-07-12T18:35:21.409869Z","shell.execute_reply.started":"2023-07-12T18:35:21.403648Z","shell.execute_reply":"2023-07-12T18:35:21.408614Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"### Read data with sklearn\nwe will use the flower iris dataset. This data sets consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn import datasets\niris = datasets.load_iris()\nX = iris.data\ny = iris.target","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:21.415253Z","iopub.execute_input":"2023-07-12T18:35:21.415566Z","iopub.status.idle":"2023-07-12T18:35:21.421106Z","shell.execute_reply.started":"2023-07-12T18:35:21.415540Z","shell.execute_reply":"2023-07-12T18:35:21.420276Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### Split dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:21.422482Z","iopub.execute_input":"2023-07-12T18:35:21.422780Z","iopub.status.idle":"2023-07-12T18:35:21.433308Z","shell.execute_reply.started":"2023-07-12T18:35:21.422754Z","shell.execute_reply":"2023-07-12T18:35:21.432297Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes algorithm\nSteps:\n1. Fit: Calculate the summary statistics and the prior for each class in the (training) dataset\n2. Predict: Calculate the probability of every class for each sample in the (test) dataset. Therefore, get the probability of data given the classes’ (Gaussian) distribution and combine it with the prior.\n","metadata":{}},{"cell_type":"markdown","source":"### Calculating summary statistics\nWe will calculate the summary statistics for each class (and feature) as well as the prior","metadata":{}},{"cell_type":"code","source":"n_samples, n_features = X_train.shape # get number of samples (rows) and features (columns)\nprint(n_samples,n_features)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:21.435075Z","iopub.execute_input":"2023-07-12T18:35:21.435394Z","iopub.status.idle":"2023-07-12T18:35:21.447219Z","shell.execute_reply.started":"2023-07-12T18:35:21.435368Z","shell.execute_reply":"2023-07-12T18:35:21.446399Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"120 4\n","output_type":"stream"}]},{"cell_type":"code","source":"n_classes = len(np.unique(y_train)) # get number of uniques classes\nprint(n_classes)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:21.448255Z","iopub.execute_input":"2023-07-12T18:35:21.448663Z","iopub.status.idle":"2023-07-12T18:35:21.459612Z","shell.execute_reply.started":"2023-07-12T18:35:21.448612Z","shell.execute_reply":"2023-07-12T18:35:21.458386Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n$$\nP(y|x_1, x_2, \\ldots, x_n) = \\frac{P(y) \\cdot P(x_1, x_2, \\ldots, x_n |y)}{P(x_1, x_2, \\ldots, x_n)}\n$$\n\nwhere:\n- $P(y|x_1, x_2, \\ldots, x_n)$ is the posterior probability of class $y$ given the features $x_1, x_2, \\ldots, x_n$.\n- $P(y)$ is the prior probability of class $y$.\n- $P(x_i|y)$ is the probability of feature $x_i$ given class $y$.\n- $P(x_1, x_2, \\ldots, x_n)$ is the probability of the features $x_1, x_2, \\ldots, x_n$ occurring together.\n\nThe naive assumption of conditional independence allows us to simplify the equation:\n\n$$\nP(y|x_1, x_2, \\ldots, x_n) = \\frac{P(y) \\cdot P(x_1|y) \\cdot P(x_2|y) \\cdot \\ldots \\cdot P(x_n|y)}{P(x_1, x_2, \\ldots, x_n)}\n$$\n","metadata":{}},{"cell_type":"code","source":"# create three zero-matrices to store summary stats & prior\nmean = np.zeros((n_classes, n_features))\nvariance = np.zeros((n_classes, n_features))\npriors = np.zeros(n_classes)\nprint(\"mean:\\n\",mean)\nprint(\"variance:\\n\",variance)\nprint(\"priors:\\n\",priors)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:21.461316Z","iopub.execute_input":"2023-07-12T18:35:21.462160Z","iopub.status.idle":"2023-07-12T18:35:21.471779Z","shell.execute_reply.started":"2023-07-12T18:35:21.462129Z","shell.execute_reply":"2023-07-12T18:35:21.470783Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"mean:\n [[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\nvariance:\n [[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\npriors:\n [0. 0. 0.]\n","output_type":"stream"}]},{"cell_type":"code","source":"for class_ind in range(n_classes):\n    # create a subset of data for the specific class 'class_ind'\n    X_class = X_train[y_train == class_ind]\n\n    # calculate statistics and update zero-matrices, rows=classes, cols=features\n    mean[class_ind, :] = np.mean(X_class, axis=0)\n    variance[class_ind, :] = np.var(X_class, axis=0)\n    priors[class_ind] = X_class.shape[0] / n_samples\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:21.473725Z","iopub.execute_input":"2023-07-12T18:35:21.474074Z","iopub.status.idle":"2023-07-12T18:35:21.487251Z","shell.execute_reply.started":"2023-07-12T18:35:21.474040Z","shell.execute_reply":"2023-07-12T18:35:21.486272Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"print(\"mean:\\n\",mean)\nprint(\"variance:\\n\",variance)\nprint(\"priors:\\n\",priors)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:21.488780Z","iopub.execute_input":"2023-07-12T18:35:21.489475Z","iopub.status.idle":"2023-07-12T18:35:21.499902Z","shell.execute_reply.started":"2023-07-12T18:35:21.489435Z","shell.execute_reply":"2023-07-12T18:35:21.499059Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"mean:\n [[4.96153846 3.36666667 1.46666667 0.23333333]\n [5.94594595 2.73243243 4.22972973 1.30540541]\n [6.525      2.95227273 5.53409091 2.02045455]]\nvariance:\n [[0.10749507 0.13042735 0.02529915 0.01094017]\n [0.274916   0.1065157  0.21344047 0.03943024]\n [0.38642045 0.09204029 0.30315599 0.07889979]]\npriors:\n [0.325      0.30833333 0.36666667]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Gaussian distribution function\nIn order to make a prediction, we need to get the probability of data belonging to a certain class. Calculating the probability of observing a given real-value is difficult. One way we can do this is to assume that the values are drawn from a distribution, such as a bell curve or Gaussian distribution This allows us to calculate the probability of a value coming from the distribution.\n\nFor simplicity, we assume the data's underlying distribution is gaussian (normal). If the data's underlying distribution is not gaussian, the real distribution should  be used (such as bernoulli, multinomial, ...)\n\n\n$$\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\, e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$","metadata":{}},{"cell_type":"code","source":"def gaussian_density(x, mean, var):\n    # implementation of gaussian density function\n    const = 1 / np.sqrt(var * 2 * np.pi)\n    proba = np.exp(-0.5 * ((x - mean) ** 2 / var))\n    return const * proba","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:21.501936Z","iopub.execute_input":"2023-07-12T18:35:21.502894Z","iopub.status.idle":"2023-07-12T18:35:21.510766Z","shell.execute_reply.started":"2023-07-12T18:35:21.502854Z","shell.execute_reply":"2023-07-12T18:35:21.509986Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"### Calculating (numerator part of) the posterior probability\nRecall the posterior equation:\n$$\nP(y|x_1, x_2, \\ldots, x_n) = \\frac{P(y) \\cdot P(x_1|y) \\cdot P(x_2|y) \\cdot \\ldots \\cdot P(x_n|y)}{P(x_1, x_2, \\ldots, x_n)}\n$$\n\nIn the code below, we are actually just calculating the numerator of the right hand side. The reason for why is left as an exercise.\n$$\n {P(y) \\cdot P(x_1|y) \\cdot P(x_2|y) \\cdot \\ldots \\cdot P(x_n|y)}\n $$\n \nnote","metadata":{}},{"cell_type":"code","source":"x = np.array([5.8, 4.0,  1.2, 0.2])\nfor class_ind in range(n_classes):\n    # get summary stats & prior\n    class_means = mean[class_ind]\n    class_variances = variance[class_ind]\n    class_priors = priors[class_ind]\n\n    probability_of_coming_from_distribution = gaussian_density(x, class_means, class_variances)\n    print(probability_of_coming_from_distribution )\n    \n    product = np.prod(probability_of_coming_from_distribution)\n    # calculate new posterior & append to list\n    posterior = class_priors * product \n    print(\"(numerator of the) posterior of being class\",class_ind,\"is =\",posterior)\n    print(\"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:21.511862Z","iopub.execute_input":"2023-07-12T18:35:21.512364Z","iopub.status.idle":"2023-07-12T18:35:21.525040Z","shell.execute_reply.started":"2023-07-12T18:35:21.512337Z","shell.execute_reply":"2023-07-12T18:35:21.523943Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"[0.04624586 0.23736665 0.61517309 3.62530037]\n(numerator of the) posterior of being class 0 is = 0.007956411260335297\n\n\n[7.31957233e-01 6.48131671e-04 3.95908867e-10 3.74734639e-07]\n(numerator of the) posterior of being class 1 is = 2.170143722975343e-20\n\n\n[3.25092838e-01 3.38128400e-03 2.54133428e-14 1.07513501e-09]\n(numerator of the) posterior of being class 2 is = 1.1012483739537062e-26\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Prediction function\nTo make predictions, we need to get the probability of an example belonging to a certain class (i.e. coming from the same distribution). \n\nTo classify a new instance, we select the class with the highest (numerator part of the) posterior probability. This can be written as:\n\n$$\n\\hat{y} = \\underset{y}{\\arg\\max} \\left( P(y) \\cdot \\prod_{i=1}^{n} P(x_i|y) \\right)\n$$\n\n\n","metadata":{}},{"cell_type":"code","source":"def predict(X_list, mean, variance, priors):\n    n_classes, n_features = mean.shape\n    y_hat = []\n\n    #X_list is a list of iris dimensions, so we will go over each of them\n    for x in X_list:  \n        # store new posteriors for each class in a single list\n        posteriors = []\n\n        for class_ind in range(n_classes):\n            # get summary stats & prior\n            class_means = mean[class_ind]\n            class_variances = variance[class_ind]\n            class_priors = priors[class_ind]\n\n            probability_of_coming_from_distribution = gaussian_density(x, class_means, class_variances)\n\n            product = np.prod(probability_of_coming_from_distribution)\n            # calculate new posterior & append to list\n            posterior = class_priors * product \n            posteriors.append(posterior)\n        # append the index with the highest class probability\n        y_hat.append(np.argmax(posteriors))\n\n    return np.array(y_hat)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:21.526340Z","iopub.execute_input":"2023-07-12T18:35:21.527410Z","iopub.status.idle":"2023-07-12T18:35:21.537018Z","shell.execute_reply.started":"2023-07-12T18:35:21.527378Z","shell.execute_reply":"2023-07-12T18:35:21.536104Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate model performance","metadata":{}},{"cell_type":"code","source":"predictions = predict(X_test, mean, variance, priors)\n\ntotal_samples = len(y_test)\ncorrect_predictions = 0\ntrue_positives = 0\nfalse_positives = 0\nfalse_negatives = 0\n\nfor i in range(total_samples):\n    if y_test[i] == predictions[i]:\n        correct_predictions += 1\n        if y_test[i] == 1:\n            true_positives += 1\n    else:\n        if y_test[i] == 1:\n            false_negatives += 1\n        else:\n            false_positives += 1\n\naccuracy = correct_predictions / total_samples\nrecall = true_positives / (true_positives + false_negatives)\nprecision = true_positives / (true_positives + false_positives)\nf1_score = 2 * (precision * recall) / (precision + recall)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Recall:\", recall)\nprint(\"Precision:\", precision)\nprint(\"F1 Score:\", f1_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:39:50.939672Z","iopub.execute_input":"2023-07-12T18:39:50.940250Z","iopub.status.idle":"2023-07-12T18:39:50.957641Z","shell.execute_reply.started":"2023-07-12T18:39:50.940204Z","shell.execute_reply":"2023-07-12T18:39:50.956119Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Accuracy: 0.9666666666666667\nRecall: 0.9230769230769231\nPrecision: 1.0\nF1 Score: 0.9600000000000001\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Exercise\n1. Under what condition is it suitable to use the gaussian distribution in naive bayes?\n2. Why did we only calculate the numerator part of the posterior in the prediction function? (hint: What is the goal for us to calculate the posteriors? does the actual values of the posteriors matter?)\n3. Notice when calculating the posteriors, we get really small numbers. When dealing with probabilities, especially when multiplying them together, the values can become very small. This can cause numerical issues or floating point calculation inaccuracies. Taking the logarithm of the probabilities helps avoid underflow issues by converting the multiplications into additions, which are computationally more stable. We can add the logarithms of the probabilities, which preserves the relative ordering of the probabilities while providing better numerical stability. Change the prediction function so it uses logarithms.\n4. Change the code into a Bernoulli Naive Bayes. ","metadata":{}},{"cell_type":"markdown","source":"```\ndef predict(X, mean, variance, priors):\n    n_classes, n_features = mean.shape\n    y_hat = []\n\n    for x in X:\n        # store new posteriors for each class in a single list\n        posteriors = []\n\n        for class_ind in range(n_classes):\n            # get summary stats & prior\n            class_mean = mean[class_ind]\n            class_variance = variance[class_ind]\n            class_prior = np.log(priors[class_ind])\n\n            # calculate new posterior & append to list\n            posterior = np.sum(np.log(gaussian_density(x, class_mean, class_variance)))\n            posterior = class_prior + posterior\n            posteriors.append(posterior)\n        # append the index with the highest class probability\n        y_hat.append(np.argmax(posteriors))\n\n    return np.array(y_hat)\n```","metadata":{}},{"cell_type":"markdown","source":"### References\n- https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n- https://github.com/marvinlanhenke/DataScience/tree/main/MachineLearningFromScratch/NaiveBayes \n- https://towardsdatascience.com/implementing-naive-bayes-from-scratch-df5572e042ac\n- https://medium.com/@rangavamsi5/na%C3%AFve-bayes-algorithm-implementation-from-scratch-in-python-7b2cc39268b9","metadata":{}}]}