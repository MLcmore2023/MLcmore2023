{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf87c5a",
   "metadata": {},
   "source": [
    "# D2-M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535de15",
   "metadata": {},
   "source": [
    "# 1. Model Prediction Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fe68df",
   "metadata": {},
   "source": [
    "## 1.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894fde4",
   "metadata": {},
   "source": [
    "There are several methods that can help improve the prediction performance of models. Here are some commonly used techniques:\n",
    "   \n",
    "1. **Data Augmentation**: This refers to techniques that increase the amount of data by adding slightly modified copies of already existing data. For example, in image processing, these techniques could include rotation, scaling, flipping, etc. In text data, it can include methods like back translation or synonym replacement.\n",
    "\n",
    "\n",
    "2. **Data Cleaning**: This involves taking care of missing values (by either filling them in based on existing data, or removing the data points entirely), and handling outliers (which might distort the training of the model).\n",
    "\n",
    "\n",
    "3. **Feature engineering**: This is the process of creating new features from existing data that can help improve model performance. This can involve transformations of existing features, creating interaction features, or any other kind of data manipulation that creates new, useful input for the model.\n",
    "\n",
    "\n",
    "4. **Model Selection**: This involves choosing the right machine learning algorithm for your specific problem. This could be a linear regression model, a decision tree, a neural network, etc. The choice depends on the nature of your data and the problem you're trying to solve.\n",
    "\n",
    "\n",
    "5. **Hyperparameter tuning**: Hyperparameters are parameters that are not learned from the data but are set before the training process. Examples are learning rate, number of layers in a neural network, number of clusters in a K-means clustering, etc. Tuning these can often significantly improve performance. Techniques for hyperparameter tuning include grid search, random search, and more advanced methods like Bayesian optimization.\n",
    "\n",
    "\n",
    "6. **Cross-validation**: This is a resampling procedure used to evaluate the performance of a model on a limited data sample. The dataset is partitioned into 'k' equally sized folds, and the model is trained on 'k-1' folds, and the remaining fold is used for testing. This process is repeated 'k' times so that we obtain a model performance score for each fold. It helps in assessing how the results of a statistical analysis will generalize to an independent data set.\n",
    "\n",
    "\n",
    "7. **Regularization**: This is a technique used to prevent overfitting, which is when a model performs well on the training data but poorly on unseen data. Regularization works by adding a penalty term to the loss function that increases as the complexity of the model increases.\n",
    "\n",
    "\n",
    "8. **Ensemble your model**: This refers to combining different models to improve overall performance. Techniques include Bagging (e.g., Random Forest), Boosting (e.g., Gradient Boosting, XGBoost), and Stacking.\n",
    "\n",
    "\n",
    "Since we have already covered data cleaning, feature engineering in the previous sections, our attention in this section will shift to other topics, including data augmentation, model selection, ensemble model, regularization, cross-validation and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d56593",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab604b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f93c9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "0        0.0        0.0        5.0       13.0        9.0        1.0   \n",
       "1        0.0        0.0        0.0       12.0       13.0        5.0   \n",
       "2        0.0        0.0        0.0        4.0       15.0       12.0   \n",
       "3        0.0        0.0        7.0       15.0       13.0        1.0   \n",
       "4        0.0        0.0        0.0        1.0       11.0        0.0   \n",
       "\n",
       "   pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_7  pixel_7_0  \\\n",
       "0        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "2        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "3        0.0        0.0        0.0        8.0  ...        0.0        0.0   \n",
       "4        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "\n",
       "   pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  pixel_7_6  \\\n",
       "0        0.0        6.0       13.0       10.0        0.0        0.0   \n",
       "1        0.0        0.0       11.0       16.0       10.0        0.0   \n",
       "2        0.0        0.0        3.0       11.0       16.0        9.0   \n",
       "3        0.0        7.0       13.0       13.0        9.0        0.0   \n",
       "4        0.0        0.0        2.0       16.0        4.0        0.0   \n",
       "\n",
       "   pixel_7_7  target  \n",
       "0        0.0     0.0  \n",
       "1        0.0     1.0  \n",
       "2        0.0     2.0  \n",
       "3        0.0     3.0  \n",
       "4        0.0     4.0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Create a dataframe\n",
    "# \"digits.data\" contains the features and \"digits.target\" contains the target\n",
    "df = pd.DataFrame(data= np.c_[digits['data'], digits['target']],\n",
    "                  columns= digits['feature_names'] + ['target'])\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X = df[digits['feature_names']]\n",
    "y = df['target']\n",
    "\n",
    "# Display the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f62bc106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1797 entries, 0 to 1796\n",
      "Data columns (total 65 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   pixel_0_0  1797 non-null   float64\n",
      " 1   pixel_0_1  1797 non-null   float64\n",
      " 2   pixel_0_2  1797 non-null   float64\n",
      " 3   pixel_0_3  1797 non-null   float64\n",
      " 4   pixel_0_4  1797 non-null   float64\n",
      " 5   pixel_0_5  1797 non-null   float64\n",
      " 6   pixel_0_6  1797 non-null   float64\n",
      " 7   pixel_0_7  1797 non-null   float64\n",
      " 8   pixel_1_0  1797 non-null   float64\n",
      " 9   pixel_1_1  1797 non-null   float64\n",
      " 10  pixel_1_2  1797 non-null   float64\n",
      " 11  pixel_1_3  1797 non-null   float64\n",
      " 12  pixel_1_4  1797 non-null   float64\n",
      " 13  pixel_1_5  1797 non-null   float64\n",
      " 14  pixel_1_6  1797 non-null   float64\n",
      " 15  pixel_1_7  1797 non-null   float64\n",
      " 16  pixel_2_0  1797 non-null   float64\n",
      " 17  pixel_2_1  1797 non-null   float64\n",
      " 18  pixel_2_2  1797 non-null   float64\n",
      " 19  pixel_2_3  1797 non-null   float64\n",
      " 20  pixel_2_4  1797 non-null   float64\n",
      " 21  pixel_2_5  1797 non-null   float64\n",
      " 22  pixel_2_6  1797 non-null   float64\n",
      " 23  pixel_2_7  1797 non-null   float64\n",
      " 24  pixel_3_0  1797 non-null   float64\n",
      " 25  pixel_3_1  1797 non-null   float64\n",
      " 26  pixel_3_2  1797 non-null   float64\n",
      " 27  pixel_3_3  1797 non-null   float64\n",
      " 28  pixel_3_4  1797 non-null   float64\n",
      " 29  pixel_3_5  1797 non-null   float64\n",
      " 30  pixel_3_6  1797 non-null   float64\n",
      " 31  pixel_3_7  1797 non-null   float64\n",
      " 32  pixel_4_0  1797 non-null   float64\n",
      " 33  pixel_4_1  1797 non-null   float64\n",
      " 34  pixel_4_2  1797 non-null   float64\n",
      " 35  pixel_4_3  1797 non-null   float64\n",
      " 36  pixel_4_4  1797 non-null   float64\n",
      " 37  pixel_4_5  1797 non-null   float64\n",
      " 38  pixel_4_6  1797 non-null   float64\n",
      " 39  pixel_4_7  1797 non-null   float64\n",
      " 40  pixel_5_0  1797 non-null   float64\n",
      " 41  pixel_5_1  1797 non-null   float64\n",
      " 42  pixel_5_2  1797 non-null   float64\n",
      " 43  pixel_5_3  1797 non-null   float64\n",
      " 44  pixel_5_4  1797 non-null   float64\n",
      " 45  pixel_5_5  1797 non-null   float64\n",
      " 46  pixel_5_6  1797 non-null   float64\n",
      " 47  pixel_5_7  1797 non-null   float64\n",
      " 48  pixel_6_0  1797 non-null   float64\n",
      " 49  pixel_6_1  1797 non-null   float64\n",
      " 50  pixel_6_2  1797 non-null   float64\n",
      " 51  pixel_6_3  1797 non-null   float64\n",
      " 52  pixel_6_4  1797 non-null   float64\n",
      " 53  pixel_6_5  1797 non-null   float64\n",
      " 54  pixel_6_6  1797 non-null   float64\n",
      " 55  pixel_6_7  1797 non-null   float64\n",
      " 56  pixel_7_0  1797 non-null   float64\n",
      " 57  pixel_7_1  1797 non-null   float64\n",
      " 58  pixel_7_2  1797 non-null   float64\n",
      " 59  pixel_7_3  1797 non-null   float64\n",
      " 60  pixel_7_4  1797 non-null   float64\n",
      " 61  pixel_7_5  1797 non-null   float64\n",
      " 62  pixel_7_6  1797 non-null   float64\n",
      " 63  pixel_7_7  1797 non-null   float64\n",
      " 64  target     1797 non-null   float64\n",
      "dtypes: float64(65)\n",
      "memory usage: 912.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d7ae8",
   "metadata": {},
   "source": [
    "## 1.3 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4506ac8",
   "metadata": {},
   "source": [
    "The `augment_data` function is defined to perform data augmentation. It takes the original images and labels as input and generates augmented versions of each image. The augmentation includes adding the original image, its horizontal flip, and a 90-degree rotation. The augmented images and labels are stored in `augmented_images` and `augmented_labels`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d6e2834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digit dataset\n",
    "digits = load_digits()\n",
    "images = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "# Data augmentation (optional)\n",
    "def augment_data(images, labels):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    for image, label in zip(images, labels):\n",
    "        augmented_images.append(image)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        augmented_images.append(np.fliplr(image))\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        augmented_images.append(np.rot90(image, k=1))\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_labels)\n",
    "\n",
    "augmented_images, augmented_labels = augment_data(images, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839801ac",
   "metadata": {},
   "source": [
    "This code combines the original images and their augmented versions into a single dataset, resulting in `all_images` and `all_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ff19b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original and augmented data\n",
    "all_images = np.concatenate([images, augmented_images])\n",
    "all_labels = np.concatenate([labels, augmented_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588cb19",
   "metadata": {},
   "source": [
    "The `plot_images` function is defined to visualize the original images and their augmented counterparts. It uses Matplotlib to create a grid of images, with the number of rows and columns specified by `rows` and `cols`. The function displays `num_samples` samples of original and augmented images side by side for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba807b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the original images and their augmented counterparts\n",
    "def plot_images(images, labels, rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i], cmap='gray')\n",
    "        ax.set_title(f\"Label: {labels[i]}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "num_samples = 10# Number of samples to visualize for each category\n",
    "original_images_sample = images[:num_samples]\n",
    "augmented_images_sample = augmented_images[:num_samples]\n",
    "\n",
    "plot_images(original_images_sample, labels[:num_samples], 1, num_samples)\n",
    "plot_images(augmented_images_sample, labels[:num_samples], 1, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2b44e",
   "metadata": {},
   "source": [
    "## 1.4 Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6bfb1b",
   "metadata": {},
   "source": [
    "The digits dataset from sklearn is a clean dataset, meaning it `doesn't have missing values`, it `doesn't contain categorical features` that need to be encoded, and it `doesn't have obvious outliers`. Therefore, some pre-processing steps like handling missing values, encoding categorical variables, or outlier treatment are not applicable in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72212382",
   "metadata": {},
   "source": [
    "## 1.5 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122af8b",
   "metadata": {},
   "source": [
    "The digits dataset is a set of 8x8 pixel images, and each pixel in the image is a feature. There are a total of 64 features for each image. These features are already in a form that's suitable for machine learning models, so it's typically not necessary to do additional feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867774e5",
   "metadata": {},
   "source": [
    "## 1.6 Hyperparameter Tuning using GridSearchCV with Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58074e",
   "metadata": {},
   "source": [
    "### 1.6.1 Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98e854d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11aafe",
   "metadata": {},
   "source": [
    "## 1.6.2 Hyperparameter Tuning using GridSearchCV with Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7fb903",
   "metadata": {},
   "source": [
    "This section performs model selection and hyperparameter tuning using GridSearchCV. We define `param_grid`, a dictionary with hyperparameters and their possible values. The `GridSearchCV` will perform a grid search over the parameter grid and use 5-fold cross-validation (`cv=5`) to find the best combination of hyperparameters for the Random Forest Classifier. After fitting the grid search, the best model is stored in `grid_search.best_estimator_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3652c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [None, 10, 20],\n",
       "                         'n_estimators': [50, 100, 200]})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train.reshape(len(X_train), -1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8cd393",
   "metadata": {},
   "source": [
    "## 1.6.3 Visualize Hyperparameter Tuning Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8f3c3",
   "metadata": {},
   "source": [
    "This part is new and focuses on the visualization of hyperparameter tuning results. We extract the mean cross-validation scores (`mean_scores`) and the corresponding hyperparameter combinations (`param_combinations`) from the `grid_search.cv_results_ dictionary`. The `for` loop then prints each mean CV score along with its corresponding hyperparameter combination for reference.\n",
    "\n",
    "After printing, we create a scatter plot to visualize the mean CV scores with respect to the hyperparameter combinations. The x-axis represents the values of `n_estimators`, the y-axis represents the values of `max_depth`, and the color of each point corresponds to the mean CV score. This plot helps us understand how different hyperparameter settings affect the model's performance during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b125c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV Score: 0.9765, Hyperparameters: {'max_depth': None, 'n_estimators': 50}\n",
      "Mean CV Score: 0.9797, Hyperparameters: {'max_depth': None, 'n_estimators': 100}\n",
      "Mean CV Score: 0.9821, Hyperparameters: {'max_depth': None, 'n_estimators': 200}\n",
      "Mean CV Score: 0.9741, Hyperparameters: {'max_depth': 10, 'n_estimators': 50}\n",
      "Mean CV Score: 0.9746, Hyperparameters: {'max_depth': 10, 'n_estimators': 100}\n",
      "Mean CV Score: 0.9767, Hyperparameters: {'max_depth': 10, 'n_estimators': 200}\n",
      "Mean CV Score: 0.9774, Hyperparameters: {'max_depth': 20, 'n_estimators': 50}\n",
      "Mean CV Score: 0.9805, Hyperparameters: {'max_depth': 20, 'n_estimators': 100}\n",
      "Mean CV Score: 0.9805, Hyperparameters: {'max_depth': 20, 'n_estimators': 200}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Hyperparameter Tuning Results\n",
    "mean_scores = grid_search.cv_results_['mean_test_score']\n",
    "param_combinations = list(grid_search.cv_results_['params'])\n",
    "\n",
    "for mean_score, params in zip(mean_scores, param_combinations):\n",
    "    print(f\"Mean CV Score: {mean_score:.4f}, Hyperparameters: {params}\")\n",
    "\n",
    "# Plotting the Mean Cross-Validation Scores\n",
    "n_estimators_values = [param['n_estimators'] for param in param_combinations]\n",
    "max_depth_values = [param['max_depth'] for param in param_combinations]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(n_estimators_values, max_depth_values, c=mean_scores, cmap='viridis', s=200, edgecolors='k')\n",
    "plt.colorbar(label='Mean CV Score')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('max_depth')\n",
    "plt.title('Hyperparameter Tuning Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303931c",
   "metadata": {},
   "source": [
    "### 1.6.4 Best Model from Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e57e458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Model from Hyperparameter Tuning\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec023ce",
   "metadata": {},
   "source": [
    "## 1.7 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae5add2",
   "metadata": {},
   "source": [
    "We select the best model found during hyperparameter tuning and store it in `best_model`. Additionally, we create a regularized Random Forest Classifier (`regularized_model`) with a specific `max_depth` hyperparameter to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62e972c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularized_model = RandomForestClassifier(max_depth=10)\n",
    "regularized_model.fit(X_train.reshape(len(X_train), -1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9fa93",
   "metadata": {},
   "source": [
    "## 1.8 Ensemble Techniques (Bagging and Voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c14e60d",
   "metadata": {},
   "source": [
    "In this section, we define two ensemble models: the Bagging Classifier (`bagging_model`) using regularized_model as the base estimator, and the Voting Classifier (`voting_model`) that combines the `regularized_model` and the `bagging_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "102f6b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Techniques (Bagging and Voting)\n",
    "bagging_model = BaggingClassifier(base_estimator=regularized_model, n_estimators=10, random_state=42)\n",
    "voting_model = VotingClassifier([('rf', regularized_model), ('bagging', bagging_model)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04427383",
   "metadata": {},
   "source": [
    "Both the Bagging and Voting models are trained on the entire training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9456e",
   "metadata": {},
   "source": [
    "## 1.9 Train the Bagging and Voting models on the entire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16f645f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(max_depth=10)),\n",
       "                             ('bagging',\n",
       "                              BaggingClassifier(base_estimator=RandomForestClassifier(max_depth=10),\n",
       "                                                random_state=42))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Bagging and Voting models on the entire training data\n",
    "bagging_model.fit(X_train.reshape(len(X_train), -1), y_train)\n",
    "voting_model.fit(X_train.reshape(len(X_train), -1), y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10393b2",
   "metadata": {},
   "source": [
    "## 1.10 Evaluate the Bagging and Voting models on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcfda15",
   "metadata": {},
   "source": [
    "The Bagging and Voting models are evaluated on the test set, and their performance is measured using accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "115d1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bagging = bagging_model.predict(X_test.reshape(len(X_test), -1))\n",
    "y_pred_voting = voting_model.predict(X_test.reshape(len(X_test), -1))\n",
    "\n",
    "test_accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "test_accuracy_voting = accuracy_score(y_test, y_pred_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050344bd",
   "metadata": {},
   "source": [
    "## 1.11 Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f456827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters: {'max_depth': None, 'n_estimators': 200}\n",
      "Test Accuracy (Best Model): 0.980528511821975\n",
      "Test Accuracy (Bagging Model): 0.9756606397774688\n",
      "Test Accuracy (Voting Model): 0.9770514603616134\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "print(\"Test Accuracy (Best Model):\", best_model.score(X_test.reshape(len(X_test), -1), y_test))\n",
    "print(\"Test Accuracy (Bagging Model):\", test_accuracy_bagging)\n",
    "print(\"Test Accuracy (Voting Model):\", test_accuracy_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370b675",
   "metadata": {},
   "source": [
    "## 1.12 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da698ef6",
   "metadata": {},
   "source": [
    "Question: You are working on a classification problem using a Support Vector Machine (SVM) classifier. The dataset you are using is quite large and has a high number of features. You want to optimize the hyperparameters of the SVM model to achieve better performance. Write a code snippet in Python using scikit-learn to perform hyperparameter tuning for the SVM classifier using GridSearchCV. Assume the dataset is already split into X_train, X_test, y_train, and y_test, and the SVM classifier is initialized as SVC().\n",
    "\n",
    "Hint: You can consider tuning hyperparameters like the kernel type, C (regularization parameter), and gamma.\n",
    "\n",
    "Note: For the purpose of this question, you don't need to worry about standardizing or normalizing the features.\n",
    "\n",
    "Please write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f248f485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "Test accuracy with best hyperparameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'kernel': ,\n",
    "    'C': ,\n",
    "    'gamma': \n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object with cross-validation (e.g., 5-fold cross-validation)\n",
    "grid_search =\n",
    "\n",
    "# Perform the hyperparameter tuning on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the GridSearchCV\n",
    "best_params = \n",
    "print(\"Best hyperparameters:\", )\n",
    "\n",
    "# Evaluate the model with the best hyperparameters on the test data\n",
    "best_svm_classifier = \n",
    "y_pred = \n",
    "accuracy = \n",
    "print(\"Test accuracy with best hyperparameters:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb70c9",
   "metadata": {},
   "source": [
    "# 2. Bagging and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f5ed6b",
   "metadata": {},
   "source": [
    "## 2.1 Introduction\n",
    "\n",
    "### 2.1.1 Overview of Ensemble Learning:\n",
    "\n",
    "- Ensemble learning is a powerful technique in machine learning where multiple models are combined to create a more robust and accurate predictive model.\n",
    "- The idea is to leverage the collective intelligence of multiple models to make better predictions than any individual model could achieve alone.\n",
    "- Ensemble learning draws inspiration from the wisdom of crowds, where diverse opinions lead to better decisions than relying on a single perspective.\n",
    "- By aggregating the predictions from multiple models, ensemble learning aims to improve generalization and reduce the risk of overfitting.\n",
    "\n",
    "\n",
    "\n",
    "### 2.1.2 Importance of Ensemble Learning in Data Science:\n",
    "\n",
    "- Data science often deals with complex and noisy datasets where building a single strong model may be challenging.\n",
    "- Ensemble learning helps address the limitations of individual models by combining their strengths and compensating for their weaknesses.\n",
    "- Ensemble methods can significantly boost the accuracy and performance of predictive models, making them invaluable in various real-world applications.\n",
    "- Ensemble learning is widely used in competitions like Kaggle, where top-performing models often utilize sophisticated ensemble techniques.\n",
    "- In practical scenarios, ensemble models are more reliable and trustworthy due to their ability to capture diverse patterns and avoid model bias.\n",
    "\n",
    "### 2.1.3 What to Expect in This Session:\n",
    "\n",
    "- In this session, we will delve into two essential ensemble learning techniques: bagging and boosting.\n",
    "- We will understand the underlying principles of each technique, explore their strengths and weaknesses, and discover when to use them.\n",
    "- We will also demonstrate coding examples using Python and scikit-learn to solidify your understanding and enable you to implement these techniques in your projects.\n",
    "- By the end of this session, you will have a clear understanding of how bagging and boosting contribute to the success of predictive models and the importance of ensemble learning in data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc1f5b",
   "metadata": {},
   "source": [
    "## 2.2 Bagging (Bootstrap Aggregating)\n",
    "\n",
    "### 2.2.1 Definition and Purpose of Bagging:\n",
    "\n",
    "- Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that aims to improve the accuracy and robustness of predictive models.\n",
    "- The primary purpose of bagging is to reduce variance and combat overfitting, which are common challenges in machine learning.\n",
    "- By combining predictions from multiple base learners, bagging reduces the risk of relying too heavily on any one model's idiosyncrasies.\n",
    "\n",
    "\n",
    "### 2.2.2 How Bagging Works:\n",
    "\n",
    "1. Bootstrapping and Creating Subsets:\n",
    "- Bagging starts by creating multiple subsets of the original training data through bootstrapping.\n",
    "- Bootstrapping involves randomly sampling data with replacement, resulting in diverse subsets with the same size as the original dataset.\n",
    "\n",
    "\n",
    "2. Training Multiple Base Learners:\n",
    "- Each subset is used to train an independent base learner (model), such as decision trees, SVMs, or neural networks.\n",
    "- These base learners are trained independently, and their predictions are combined later to form the ensemble.\n",
    "\n",
    "\n",
    "3. Aggregating Predictions:\n",
    "- Once all base learners are trained, the final prediction is made by aggregating their individual predictions.\n",
    "- For regression tasks, predictions are averaged, while for classification tasks, majority voting is used to decide the final prediction.\n",
    "\n",
    "\n",
    "### 2.2.3 Advantages of Bagging:\n",
    "\n",
    "1. Reducing Variance and Overfitting:\n",
    "- Bagging's main advantage is its ability to reduce variance and combat overfitting.\n",
    "- By combining predictions from diverse models, it results in a more balanced and stable ensemble model.\n",
    "\n",
    "\n",
    "2. Robustness to Noisy Data and Outliers:\n",
    "- Bagging is robust to noisy data and outliers, as it diminishes the impact of individual data points through aggregation.\n",
    "- Outliers are less likely to affect the overall prediction due to the averaging or voting process.\n",
    "\n",
    "\n",
    "### 2.2.4 Common Algorithms Using Bagging:\n",
    "\n",
    "1. Random Forest:\n",
    "- Random Forest is one of the most popular bagging algorithms that uses decision trees as base learners.\n",
    "- It builds a large number of decision trees, and each tree is trained on a bootstrapped subset of the data.\n",
    "- The final prediction is made by aggregating the predictions of all individual trees.\n",
    "\n",
    "2. Bagged Decision Trees:\n",
    "- Bagged Decision Trees, or simply Bagging with decision trees, is a straightforward bagging approach.\n",
    "- It applies the same concept as Random Forest but with a smaller number of trees (often not enough to be considered a forest).\n",
    "\n",
    "3. Bagged SVMs (Support Vector Machines):\n",
    "- Bagging can also be applied to other base learners like Support Vector Machines (SVMs).\n",
    "- Bagged SVMs utilize subsets of data for training multiple SVM models, and their predictions are aggregated to make the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd6c29",
   "metadata": {},
   "source": [
    "### 2.2.5 Coding Example with Random Forest (Python and scikit-learn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb4b6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfffceb",
   "metadata": {},
   "source": [
    "#### 2.2.5.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "380a4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the covertype dataset from OpenML\n",
    "covertype = fetch_openml('covertype', version=4)\n",
    "X = covertype.data\n",
    "y = covertype.target.astype('int') - 1  # Shift labels to range 0-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bcc560",
   "metadata": {},
   "source": [
    "This code fetches the **covertype** dataset from OpenML, and then extracts the data and target labels. The labels are shifted from 1-7 to 0-6 for ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cedbb75",
   "metadata": {},
   "source": [
    "#### 2.2.5.2 Data Subset (for speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3919b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of data for demonstration purposes (and for faster execution)\n",
    "X = X[:20000]\n",
    "y = y[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad3e1f6",
   "metadata": {},
   "source": [
    "Given the large size of the **covertype** dataset, we take a subset of 20,000 samples for faster demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f3ed4",
   "metadata": {},
   "source": [
    "#### 2.2.5.3 Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13cc9574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8bfca",
   "metadata": {},
   "source": [
    "This splits the data into training and testing sets. 30% of the data is reserved for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62343e1b",
   "metadata": {},
   "source": [
    "#### 2.2.5.4 Results Storage Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e6af5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store results of each model\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38feaab7",
   "metadata": {},
   "source": [
    "An empty list to store accuracy results for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b071f",
   "metadata": {},
   "source": [
    "#### 2.2.5.5 Simple Decision Tree Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1129e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a simple decision tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "results.append(['Simple Decision Tree', accuracy_score(y_test, tree_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96352280",
   "metadata": {},
   "source": [
    "This section trains a basic decision tree on the training data, makes predictions on the test data, and stores the accuracy in the results list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b9f7d",
   "metadata": {},
   "source": [
    "#### 2.2.5.6 Random Forest Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53539dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a Random Forest with 5 trees\n",
    "rf_small = RandomForestClassifier(n_estimators=5, random_state=42)\n",
    "rf_small.fit(X_train, y_train)\n",
    "rf_small_pred = rf_small.predict(X_test)\n",
    "results.append(['Random Forest (5 trees)', accuracy_score(y_test, rf_small_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eae5d33",
   "metadata": {},
   "source": [
    "Trains a Random Forest model with 5 trees (small for demonstration), makes predictions, and adds the accuracy to the results list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e203d0f",
   "metadata": {},
   "source": [
    "#### 2.2.5.7 Manual Bagging Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e0120f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual implementation of Bagging\n",
    "n_learners = 5  # Number of learners (trees) for bagging\n",
    "predictions = []  # List to store predictions of each learner\n",
    "\n",
    "# Bootstrapping and training for each learner\n",
    "for i in range(n_learners):\n",
    "    boot_X, boot_y = resample(X_train, y_train, replace=True, n_samples=len(X_train))\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(boot_X, boot_y)\n",
    "    predictions.append(tree.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29bcdff",
   "metadata": {},
   "source": [
    "This manually implements bagging by bootstrapping the training data **n_learners** times, training a decision tree on each bootstrap sample, and storing its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce593d5e",
   "metadata": {},
   "source": [
    "#### 2.2.5.8 Mode Calculation for Bagging Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e14284af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute mode for an array\n",
    "def compute_mode(array):\n",
    "    return np.bincount(array).argmax()\n",
    "\n",
    "# Convert predictions list of arrays to a 2D numpy array\n",
    "predictions_array = np.array(predictions)\n",
    "\n",
    "# Calculate mode of predictions for bagging\n",
    "mode_predictions = []\n",
    "for column in predictions_array.T:  # Now we transpose the numpy array\n",
    "    mode_predictions.append(compute_mode(column))\n",
    "mode_predictions = np.array(mode_predictions)\n",
    "results.append(['Manual Bagging (Mode)', accuracy_score(y_test, mode_predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97adc6f3",
   "metadata": {},
   "source": [
    "For each sample in the test set, this computes the mode (most common prediction) across the **n_learners** decision trees and stores the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763178dd",
   "metadata": {},
   "source": [
    "#### 2.2.5.9 Median Calculation for Bagging Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab6ae5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate median of predictions for bagging\n",
    "median_predictions = np.median(predictions, axis=0).astype(int)\n",
    "results.append(['Manual Bagging with Median', accuracy_score(y_test, median_predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c16f89",
   "metadata": {},
   "source": [
    "This section demonstrates another way to aggregate predictions by using the median. It then stores the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a259999",
   "metadata": {},
   "source": [
    "#### 2.2.5.10 Convert Results to DataFrame and Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62a3e2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Manual Bagging (Mode)</td>\n",
       "      <td>0.844833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manual Bagging with Median</td>\n",
       "      <td>0.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest (5 trees)</td>\n",
       "      <td>0.827667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simple Decision Tree</td>\n",
       "      <td>0.812667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy\n",
       "0       Manual Bagging (Mode)  0.844833\n",
       "1  Manual Bagging with Median  0.837000\n",
       "2     Random Forest (5 trees)  0.827667\n",
       "3        Simple Decision Tree  0.812667"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the results list to a DataFrame and display it\n",
    "df_results = pd.DataFrame(results, columns=['Model', 'Accuracy'])\n",
    "\n",
    "# Sort the DataFrame based on the 'Accuracy' column in descending order\n",
    "df_results = df_results.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5194f",
   "metadata": {},
   "source": [
    "This converts the **results** list into a pandas DataFrame and displays it for easy comparison of model accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a646c",
   "metadata": {},
   "source": [
    "### 2.2.6 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bc1e3",
   "metadata": {},
   "source": [
    "- Question: What is the main principle behind the Bagging technique? How does it help in reducing overfitting?\n",
    "- Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e2d9e",
   "metadata": {},
   "source": [
    "## 2.3 Bagging (Bootstrap Aggregating)\n",
    "\n",
    "### 2.3.1 Definition and Purpose of Boosting:\n",
    "\n",
    "- Boosting is an ensemble learning technique that focuses on creating a strong learner by combining multiple weak learners iteratively.\n",
    "- The main purpose of boosting is to improve the accuracy and performance of predictive models by giving more emphasis to misclassified instances during training.\n",
    "- Boosting learns from its mistakes in an adaptive manner, continually refining its predictions to achieve high accuracy on complex datasets.\n",
    "\n",
    "\n",
    "### 2.3.2 How Boosting Works:\n",
    "\n",
    "1. Sequential Learning and Weighted Misclassifications:\n",
    "- Boosting works in a sequential manner, where each base learner is trained based on the performance of the previous ones.\n",
    "- During training, it assigns higher weights to misclassified instances, making them more influential in subsequent iterations.\n",
    "\n",
    "\n",
    "2. Iterative Training of Base Learners:\n",
    "- In each iteration, a new base learner is trained to correct the mistakes made by the ensemble so far.\n",
    "- The base learners are typically weak models (e.g., shallow decision trees) to avoid overfitting and maintain interpretability.\n",
    "\n",
    "\n",
    "3. Emphasizing Difficult Instances:\n",
    "- Boosting focuses on challenging instances that are frequently misclassified by previous base learners.\n",
    "- By repeatedly emphasizing these difficult instances, boosting ensures that the ensemble model pays more attention to them and gradually improves its performance.\n",
    "\n",
    "\n",
    "\n",
    "### 2.3.3 Advantages of Boosting:\n",
    "\n",
    "1. Adaptive Learning and High Accuracy:\n",
    "- Boosting's adaptive learning approach allows it to learn from misclassifications and significantly improve predictive accuracy.\n",
    "- It is particularly effective in handling complex relationships in data, making it suitable for various real-world applications.\n",
    "\n",
    "\n",
    "2. Model Versatility and Feature Importance:\n",
    "- Boosting can be applied with various base learners, such as decision trees, SVMs, and neural networks.\n",
    "- Additionally, many boosting algorithms provide feature importance scores, enabling us to identify the most influential features in the model's decision-making process.\n",
    "\n",
    "\n",
    "### 2.3.4 Common Algorithms Using Boosting:\n",
    "\n",
    "1. AdaBoost (Adaptive Boosting):\n",
    "- AdaBoost is one of the earliest and most popular boosting algorithms.\n",
    "- It assigns higher weights to misclassified instances and combines the predictions of weak learners to create a strong ensemble model.\n",
    "- AdaBoost is suitable for both classification and regression tasks.\n",
    "\n",
    "\n",
    "2. Gradient Boosting Machines (GBM):\n",
    "- GBM builds base learners sequentially, focusing on the gradients of the loss function to optimize the model's performance.\n",
    "- It uses a process called gradient descent to minimize the errors in each iteration.\n",
    "- GBM is widely used for regression and classification tasks and is known for its high accuracy and flexibility.\n",
    "\n",
    "\n",
    "3. XGBoost and LightGBM:\n",
    "- XGBoost and LightGBM are optimized implementations of gradient boosting that are efficient and scalable.\n",
    "- They utilize advanced techniques like parallel processing and tree-pruning to achieve better performance.\n",
    "- These algorithms are popular in data science competitions and real-world applications due to their speed and accuracy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8a61e",
   "metadata": {},
   "source": [
    "### 2.3.5 Coding Example with AdaBoost (Python and scikit-learn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4921a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2679f2f",
   "metadata": {},
   "source": [
    "This section imports all the necessary libraries and modules we'll use throughout the demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde6fd9e",
   "metadata": {},
   "source": [
    "#### 2.3.5.1 Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "deb1a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the covertype dataset from OpenML\n",
    "covertype = fetch_openml('covertype', version=4)\n",
    "X = covertype.data\n",
    "y = covertype.target.astype('int') - 1  # Shift labels to range 0-6\n",
    "\n",
    "# Select a subset of data for demonstration purposes (and for faster execution)\n",
    "X = X[:20000]\n",
    "y = y[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22abef99",
   "metadata": {},
   "source": [
    "#### 2.3.5.2 Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9178fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3e991",
   "metadata": {},
   "source": [
    "This line splits our data into training and test sets. 70% of the data is used for training and 30% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1d620",
   "metadata": {},
   "source": [
    "#### 2.3.5.4 Initialize a list to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4337f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store results of each model\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7c337",
   "metadata": {},
   "source": [
    "We'll store the accuracy results of each model in this list and then convert it to a DataFrame at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b3e43",
   "metadata": {},
   "source": [
    "#### 2.3.5.5 Train and test a simple decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5d11e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a simple decision tree classifier\n",
    "tree = DecisionTreeClassifier(max_depth=1)  # Setting depth to 1, as AdaBoost typically uses 'stumps'\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "results.append(['Simple Decision Tree', accuracy_score(y_test, tree_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc15aceb",
   "metadata": {},
   "source": [
    "We train a simple decision tree \"stump\" (tree of depth 1) on the training data and then predict and evaluate its accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31208d91",
   "metadata": {},
   "source": [
    "#### 2.3.5.6 Train and test a Random Forest with 5 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fdffca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_small = RandomForestClassifier(n_estimators=5, random_state=42)\n",
    "rf_small.fit(X_train, y_train)\n",
    "rf_small_pred = rf_small.predict(X_test)\n",
    "results.append(['Random Forest (5 trees)', accuracy_score(y_test, rf_small_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03c051",
   "metadata": {},
   "source": [
    "Next, we train a random forest ensemble with 5 trees and then predict and evaluate its accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a1d0c",
   "metadata": {},
   "source": [
    "#### 2.3.5.7 Train and test AdaBoost with 5 weak learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "484506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.5.7 Train and test AdaBoost with 5 weak learners\n",
    "# Train and test AdaBoost with 5 weak learners (stumps)\n",
    "adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), \n",
    "                              n_estimators=5, random_state=42)\n",
    "adaboost.fit(X_train, y_train)\n",
    "adaboost_pred = adaboost.predict(X_test)\n",
    "results.append(['AdaBoost (5 learners)', accuracy_score(y_test, adaboost_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916bde36",
   "metadata": {},
   "source": [
    "Here, we train an AdaBoost classifier using decision tree stumps as weak learners. We use 5 such stumps. After training, we predict on the test set and evaluate the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aec6d4",
   "metadata": {},
   "source": [
    "#### 2.3.5.8 Convert the results list to a sorted DataFrame and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b3de225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (5 trees)</td>\n",
       "      <td>0.827667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost (5 learners)</td>\n",
       "      <td>0.488667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simple Decision Tree</td>\n",
       "      <td>0.398500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy\n",
       "0  Random Forest (5 trees)  0.827667\n",
       "1    AdaBoost (5 learners)  0.488667\n",
       "2     Simple Decision Tree  0.398500"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the results list to a DataFrame\n",
    "df_results = pd.DataFrame(results, columns=['Model', 'Accuracy'])\n",
    "\n",
    "# Sort the DataFrame based on the 'Accuracy' column in descending order\n",
    "df_results = df_results.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba779351",
   "metadata": {},
   "source": [
    "### 2.3.6 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fbceff",
   "metadata": {},
   "source": [
    "- Question: Describe how boosting works. How is it different from bagging?\n",
    "- Answer: Boosting trains learners sequentially where each subsequent learner tries to correct the mistakes of the previous one. Unlike bagging, which aims to reduce variance, boosting aims to reduce bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27271d6",
   "metadata": {},
   "source": [
    "## 2.4 Strengths and Weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47ea4c",
   "metadata": {},
   "source": [
    "### 2.4.1 Comparison of Bagging and Boosting:\n",
    "\n",
    "- Bagging and Boosting are both ensemble learning techniques that combine multiple models to improve predictive performance.\n",
    "- Bagging aims to reduce variance and overfitting by aggregating predictions from diverse models.\n",
    "- Boosting focuses on adaptive learning and aims to correct mistakes made by previous base learners in an iterative manner.\n",
    "\n",
    "\n",
    "### 2.4.2 Strengths of Bagging:\n",
    "\n",
    "1. Reduced Variance and Overfitting:\n",
    "- Bagging reduces the risk of overfitting by combining predictions from multiple models with diverse training subsets.\n",
    "\n",
    "2. Robustness to Noisy Data:\n",
    "- Bagging is robust to noisy data and outliers since it aggregates predictions, minimizing the impact of individual data points.\n",
    "\n",
    "3. Efficient Parallelization:\n",
    "- Training base learners in bagging can be easily parallelized, making it efficient and scalable.\n",
    "\n",
    "\n",
    "### 2.4.3 Weaknesses of Bagging:\n",
    "\n",
    "1. Lack of Interpretability:\n",
    "- The ensemble model in bagging may lack interpretability, especially if the base learners are complex models like Random Forest.\n",
    "\n",
    "\n",
    "2. Bias Preservation:\n",
    "- Bagging may not address bias in the base learners; if the base models are biased, the ensemble may inherit the bias.\n",
    "\n",
    "\n",
    "3. Limited Performance Improvement:\n",
    "- Bagging may not improve performance significantly if the base learners are already strong and diverse.\n",
    "\n",
    "\n",
    "\n",
    "### 2.4.4 Strengths of Boosting:\n",
    "1. Adaptive Learning and High Accuracy:\n",
    "- Boosting adaptively learns from mistakes and focuses on challenging instances, leading to high accuracy.\n",
    "2. Model Versatility and Feature Importance:\n",
    "- Boosting can be used with various base learners and often provides feature importance scores for better understanding.\n",
    "\n",
    "\n",
    "\n",
    "### 2.4.5 Weaknesses of Boosting:\n",
    "1. Sensitivity to Noisy Data and Outliers:\n",
    "- Boosting may be sensitive to noisy data and outliers, affecting model performance.\n",
    "2. Slower Training and Limited Parallelization:\n",
    "- Training base learners sequentially makes boosting slower than bagging, and it may be less efficient to parallelize.\n",
    "3. Potential Overfitting:\n",
    "- Boosting may be prone to overfitting, especially if the number of iterations is too high.\n",
    "\n",
    "\n",
    "### 2.4.6 Choosing Between Bagging and Boosting: Considerations:\n",
    "- The choice between bagging and boosting depends on the dataset, the complexity of the problem, and the available computational resources.\n",
    "- Bagging is suitable when the base models are diverse and the goal is to reduce variance and overfitting.\n",
    "- Boosting is preferred when high accuracy is crucial, and the dataset is not significantly affected by noisy data and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252d73f",
   "metadata": {},
   "source": [
    "## 2.5 Strengths and Weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b55f9e",
   "metadata": {},
   "source": [
    "1. High Variance Model:\n",
    "- You've trained a deep decision tree on your dataset and noticed that it performs extremely well on the training data but poorly on the validation data.\n",
    "\n",
    "- Question: Which ensemble technique might help remedy this, bagging or boosting?\n",
    "\n",
    "2. High Bias Model:\n",
    "- Your team trained a shallow decision tree (i.e., a decision stump) on a complex dataset. The model performs poorly on both training and validation sets.\n",
    "\n",
    "- Question: Which ensemble technique might help improve this model's performance, bagging or boosting?\n",
    "\n",
    "3. Large Dataset:\n",
    "- You have a very large dataset and are concerned about the training time. You're considering an ensemble technique to improve your model's performance.\n",
    "\n",
    "- Question: Which technique, bagging or boosting, would typically be faster in training?\n",
    "\n",
    "4. Noisy Data:\n",
    "- Your dataset contains a significant amount of noise, and outliers are causing models to underperform.\n",
    "\n",
    "- Question: Which ensemble method, bagging or boosting, might be more robust to such noise and why?\n",
    "\n",
    "5. Model Diversity:\n",
    "- You're working on an ensemble model, and you have access to various diverse base models. You believe the errors in these models are largely uncorrelated.\n",
    "\n",
    "- Question: Which ensemble technique might benefit more from this diversity, bagging or boosting?\n",
    "\n",
    "6. Information on Error Types:\n",
    "- After evaluating a model, you've noticed that it's making many types of errors, but the frequency of each type is low.\n",
    "\n",
    "- Question: If you had to choose an ensemble technique to correct diverse error types, would you pick bagging or boosting?\n",
    "\n",
    "7. Final Model Interpretability:\n",
    "- You're working on a healthcare project where the interpretability of the model is crucial. Doctors want to understand how the model makes decisions.\n",
    "\n",
    "- Question: Which ensemble technique, bagging or boosting, might be more challenging in terms of interpretability, and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003b8cd",
   "metadata": {},
   "source": [
    "# 3. Supply chain example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdcb5e8",
   "metadata": {},
   "source": [
    "## 3.1 Introduction to the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf600db0",
   "metadata": {},
   "source": [
    "The dataset at hand is a comprehensive sales record dataset provided by a a renowned consumer goods company. In here, we can it \"company A\". This dataset aims to facilitate the prediction of item sales quantities in each unit (EA) using various informative features. The dataset encompasses a wide range of variables that provide insights into the sales dynamics and factors influencing consumer behavior.\n",
    "\n",
    "\n",
    "### Objective:\n",
    "\n",
    "The primary objective of this dataset is to develop a predictive model that accurately estimates the sales quantity of each item (Pos_Qty_EA) based on the provided features. By leveraging the historical sales data, \"company A\" aims to forecast item sales more effectively, optimize inventory management, and make data-driven decisions to maximize sales revenue and profitability.\n",
    "\n",
    "Additionally, this dataset can be utilized to gain insights into the factors that drive or hinder sales, assess the impact of promotional activities, evaluate the performance of different store banners, and analyze the influence of pricing tiers on consumer behavior.\n",
    "\n",
    "Through detailed exploration and analysis of this dataset, Nestle can enhance its understanding of market dynamics, improve sales forecasting accuracy, and make informed business decisions that align with customer demands and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5d6e8",
   "metadata": {},
   "source": [
    "## 3.2 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84788e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54eaa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the files from github\n",
    "!git clone https://github.com/MLcmore2023/MLcmore2023.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./MLcmore2023/'day2_pm_afternoon'/* ./MLcmore2023/'day2_pm_afternoon'/.* ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('campany_A_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371435ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed95fb78",
   "metadata": {},
   "source": [
    "### Features Explanation:\n",
    "\n",
    "\n",
    "1. MATERIAL: The unique identifier for each item in the dataset.\n",
    "2. source: The source of the sales data (e.g., point of sale systems, online sales platform, etc.).\n",
    "3. Banner: The name or identifier of the retail banner (store brand) where the item was sold.\n",
    "4. SRC_BAN_POS: The source of the sales data specific to the retail banner.\n",
    "5. Plan_Banner: The planned retail banner for the item.\n",
    "6. Plan_Region: The planned region for the item's sales.\n",
    "7. CL4Key: The identifier for a higher-level category or classification level 4 of the item.\n",
    "8. CL6Key: The identifier for a lower-level category or classification level 6 of the item.\n",
    "9. pos_date: The date of the sales record.\n",
    "10. **Pos_Qty_EA: The sales quantity of the item in each unit (target variable).**\n",
    "11. Pos_Sales: The sales amount or revenue generated from the item.\n",
    "12. POS_QTY_CS: The sales quantity of the item in case units (CS stands for case).\n",
    "13. UBP: The unit buying price, which represents the cost of purchasing the item.\n",
    "14. UNIT: The unit of measure for the item's sales quantity (EA or CS).\n",
    "15. FACTOR_EACH: The conversion factor between each unit (EA) and case unit (CS).\n",
    "16. PER_SALES_UOM_CASE: The sales quantity per unit of measure (case) for the item.\n",
    "17. Complete_PPG: The identifier for a complete product group, which represents a broader category or grouping of items.\n",
    "18. Total_Sales: The total sales amount or revenue for all items.\n",
    "19. Baseline_Qty: The baseline or expected sales quantity for the item.\n",
    "20. Baseline_Nps: The baseline or expected net promoter score (NPS) associated with the item.\n",
    "21. ForecastDate: The date of the sales forecast for the item.\n",
    "22. Incr_Sales: The incremental or additional sales generated by a promotional activity or event.\n",
    "23. PromoId: The identifier for a specific promotional activity or event.\n",
    "24. InStoreStart: The start date of the promotional activity or event in the store.\n",
    "25. InStoreEnd: The end date of the promotional activity or event in the store.\n",
    "26. PromoDuration: The duration of the promotional activity or event.\n",
    "27. MATERIALDESC: A description or name of the item.\n",
    "28. SUB_CATEGORY: The identifier for a sub-category of the item.\n",
    "29. SUB_CATEGORY_DESC: A description or name of the sub-category.\n",
    "30. CATEGORY: The identifier for the category of the item.\n",
    "31. CATEGORY_DESC: A description or name of the category.\n",
    "32. FGroup_Desc: A description or name of the item's functional group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
