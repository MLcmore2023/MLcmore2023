{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 17.981335,
      "end_time": "2023-07-10T19:55:54.172930",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-07-10T19:55:36.191595",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLcmore2023/MLcmore2023/blob/main/day4_pm_afternoon/convolutional_neural_network_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network\n",
        "CNN is a supervised machine learning algorithm widely used for image classification, object detection, and other computer vision tasks. Inspired by the human visual system, a CNN comprises interconnected layers of specialized neurons that automatically learn and detect features from raw image data. The network's core components include convolutional layers, pooling layers, and fully connected layers. During training, the network adjusts its learnable parameters, such as filters and biases, to minimize the difference between predicted and actual labels. CNNs is good at capturing local patterns and hierarchical representations in images, thanks to the convolutional operations that extract features and the pooling layers that downsample the data."
      ],
      "metadata": {
        "_cell_guid": "ace74b07-803e-4438-abf1-e24439d742fa",
        "_uuid": "2e8d6779-b776-4149-9e86-c518864ca85f",
        "papermill": {
          "duration": 0.014204,
          "end_time": "2023-07-10T19:55:50.924559",
          "exception": false,
          "start_time": "2023-07-10T19:55:50.910355",
          "status": "completed"
        },
        "tags": [],
        "id": "P5fK9gF_zUPc"
      },
      "id": "P5fK9gF_zUPc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries and initialize random generator"
      ],
      "metadata": {
        "_cell_guid": "19c928fa-f6fe-43bf-afd9-7e1ba0c2782d",
        "_uuid": "96012197-48c5-482b-ac01-9530fdbefdd9",
        "papermill": {
          "duration": 0.012732,
          "end_time": "2023-07-10T19:55:50.950851",
          "exception": false,
          "start_time": "2023-07-10T19:55:50.938119",
          "status": "completed"
        },
        "tags": [],
        "id": "l7RGjt5uzUPd"
      },
      "id": "l7RGjt5uzUPd"
    },
    {
      "cell_type": "code",
      "source": [
        "import time # for measuring training time\n",
        "import numpy as np # for linear algebra\n",
        "from keras.datasets import mnist # for loading the dataset\n",
        "\n",
        "np.random.seed(0)\n",
        "np.set_printoptions(threshold=7) # printing format"
      ],
      "metadata": {
        "_cell_guid": "d1ce19e8-0e08-4725-9d8f-9bda6b37a345",
        "_uuid": "c6c91d2b-4cf2-496d-a586-633006132e2c",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": 0.035497,
          "end_time": "2023-07-10T19:55:50.999547",
          "exception": false,
          "start_time": "2023-07-10T19:55:50.964050",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-17T09:00:59.042713Z",
          "iopub.execute_input": "2023-07-17T09:00:59.043106Z",
          "iopub.status.idle": "2023-07-17T09:00:59.051310Z",
          "shell.execute_reply.started": "2023-07-17T09:00:59.043077Z",
          "shell.execute_reply": "2023-07-17T09:00:59.049767Z"
        },
        "trusted": true,
        "id": "DjOHK5RHzUPe"
      },
      "execution_count": 33,
      "outputs": [],
      "id": "DjOHK5RHzUPe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load the MNIST image data using `Keras` library\n",
        "Load the MNIST data as a 2D tuple containing the training data and the test data."
      ],
      "metadata": {
        "_cell_guid": "ceae306c-7c9e-4521-87f0-b622cead3d40",
        "_uuid": "84e4db8e-5c62-4fb1-acd3-2013e14e1c31",
        "papermill": {
          "duration": 0.013248,
          "end_time": "2023-07-10T19:55:51.027471",
          "exception": false,
          "start_time": "2023-07-10T19:55:51.014223",
          "status": "completed"
        },
        "tags": [],
        "id": "gU-SQJGzzUPe"
      },
      "id": "gU-SQJGzzUPe"
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_data = mnist.load_data()"
      ],
      "metadata": {
        "_cell_guid": "27fe035e-61c0-4ba4-bf08-09f5c17fd312",
        "_uuid": "6cab78bd-db67-4837-9690-fd9bb8509948",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": 0.736564,
          "end_time": "2023-07-10T19:55:51.777621",
          "exception": true,
          "start_time": "2023-07-10T19:55:51.041057",
          "status": "failed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-17T09:00:59.053455Z",
          "iopub.execute_input": "2023-07-17T09:00:59.053813Z",
          "iopub.status.idle": "2023-07-17T09:00:59.383461Z",
          "shell.execute_reply.started": "2023-07-17T09:00:59.053784Z",
          "shell.execute_reply": "2023-07-17T09:00:59.382271Z"
        },
        "trusted": true,
        "id": "IoRwZWDqzUPe"
      },
      "execution_count": 34,
      "outputs": [],
      "id": "IoRwZWDqzUPe"
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = mnist_data[0]\n",
        "test_data = mnist_data[1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-17T09:00:59.385796Z",
          "iopub.execute_input": "2023-07-17T09:00:59.386172Z",
          "iopub.status.idle": "2023-07-17T09:00:59.392412Z",
          "shell.execute_reply.started": "2023-07-17T09:00:59.386141Z",
          "shell.execute_reply": "2023-07-17T09:00:59.390992Z"
        },
        "trusted": true,
        "id": "R98phrajzUPf"
      },
      "execution_count": 35,
      "outputs": [],
      "id": "R98phrajzUPf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ``training_data`` is returned as a tuple with two entries.\n",
        "The first entry contains the actual training images."
      ],
      "metadata": {
        "_cell_guid": "ef428efc-0048-4027-a735-3f188cd78f14",
        "_uuid": "1cfbe1c7-5c10-4c5e-84f9-b9b749d4667d",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "tGCRQGx8zUPf"
      },
      "id": "tGCRQGx8zUPf"
    },
    {
      "cell_type": "code",
      "source": [
        "training_inputs, training_results = training_data\n",
        "\"\"\"\n",
        "#same as:\n",
        "training_inputs =training_data[0]\n",
        "training_results =training_data[1]\n",
        "\"\"\""
      ],
      "metadata": {
        "_cell_guid": "1796c907-5b0d-4b5a-9a7e-23353a846fe1",
        "_uuid": "1e59ebd1-7f09-439c-b550-10a689b6fcb7",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-17T09:00:59.394716Z",
          "iopub.execute_input": "2023-07-17T09:00:59.395239Z",
          "iopub.status.idle": "2023-07-17T09:00:59.409070Z",
          "shell.execute_reply.started": "2023-07-17T09:00:59.395201Z",
          "shell.execute_reply": "2023-07-17T09:00:59.407855Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k4CyXTt-zUPf",
        "outputId": "4d8fc421-6479-445a-85c9-a48ab4d198dc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#same as:\\ntraining_inputs =training_data[0]\\ntraining_results =training_data[1]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "id": "k4CyXTt-zUPf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a\n",
        "numpy ndarray with 60,000 entries.  Each entry is, in turn, a\n",
        "numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
        "pixels in a single MNIST image.\n",
        "\n",
        "One example image:"
      ],
      "metadata": {
        "_cell_guid": "f91fe608-219f-4338-a773-f137a947683e",
        "_uuid": "b43bcc21-0b8e-4668-869d-0f0052c015c4",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "x5JkysIBzUPf"
      },
      "id": "x5JkysIBzUPf"
    },
    {
      "cell_type": "code",
      "source": [
        "display(training_inputs)\n",
        "display(training_inputs.shape)"
      ],
      "metadata": {
        "_cell_guid": "fdbf2493-b629-4f16-8317-1e559ce8987d",
        "_uuid": "63446ea7-d5d8-4d9b-86cc-c576d901b79f",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-17T09:00:59.414482Z",
          "iopub.execute_input": "2023-07-17T09:00:59.415449Z",
          "iopub.status.idle": "2023-07-17T09:00:59.428301Z",
          "shell.execute_reply.started": "2023-07-17T09:00:59.415400Z",
          "shell.execute_reply": "2023-07-17T09:00:59.426821Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "1UvKd2ouzUPg",
        "outputId": "1e81013e-5e89-4cb4-8d95-c877f6f636c0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "1UvKd2ouzUPg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second entry in the ``training_data`` tuple is a numpy ndarray\n",
        "containing 60,000 entries.  Those entries are just the digit\n",
        "values (0...9) for the corresponding images contained in the first\n",
        "entry of the tuple."
      ],
      "metadata": {
        "_cell_guid": "09a0c9b2-5aa3-4c5c-b802-d6ae910ec022",
        "_uuid": "bba0118c-1ba4-4c66-9a62-1f7534388283",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "Msl5DJPtzUPg"
      },
      "id": "Msl5DJPtzUPg"
    },
    {
      "cell_type": "code",
      "source": [
        "display(training_results)\n",
        "display(training_results.shape)"
      ],
      "metadata": {
        "_cell_guid": "e73395c8-99d3-4d5a-9844-aa5db304406a",
        "_uuid": "3441b268-b4b9-46e1-b699-a59da409cb84",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-17T09:00:59.430282Z",
          "iopub.execute_input": "2023-07-17T09:00:59.431108Z",
          "iopub.status.idle": "2023-07-17T09:00:59.443594Z",
          "shell.execute_reply.started": "2023-07-17T09:00:59.431067Z",
          "shell.execute_reply": "2023-07-17T09:00:59.442065Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "cZQmkG-MzUPg",
        "outputId": "0897dad9-237a-494d-f6eb-2018201de382"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "cZQmkG-MzUPg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ``test_data`` is the same, except\n",
        "it contains only 10,000 images."
      ],
      "metadata": {
        "_cell_guid": "10cb77f5-44cc-4124-bafa-92669fc55737",
        "_uuid": "1ad2a01e-72f2-4e62-9f76-9690f2e93c42",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "koCER40kzUPg"
      },
      "id": "koCER40kzUPg"
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data[0].shape,test_data[1].shape)"
      ],
      "metadata": {
        "_cell_guid": "45a39186-3cc6-4a15-a38d-e8909c75bcdf",
        "_uuid": "6ba1d8d2-05c1-49e7-8e64-6fbc5937a4b8",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-17T09:00:59.445289Z",
          "iopub.execute_input": "2023-07-17T09:00:59.445693Z",
          "iopub.status.idle": "2023-07-17T09:00:59.453627Z",
          "shell.execute_reply.started": "2023-07-17T09:00:59.445640Z",
          "shell.execute_reply": "2023-07-17T09:00:59.452173Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcWQwNL_zUPg",
        "outputId": "1e70b3d8-6bd5-43f1-d6b9-67a15bf2a171"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 28, 28) (10000,)\n"
          ]
        }
      ],
      "id": "WcWQwNL_zUPg"
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs, test_results = test_data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-17T09:00:59.455037Z",
          "iopub.execute_input": "2023-07-17T09:00:59.455421Z",
          "iopub.status.idle": "2023-07-17T09:00:59.467166Z",
          "shell.execute_reply.started": "2023-07-17T09:00:59.455390Z",
          "shell.execute_reply": "2023-07-17T09:00:59.465799Z"
        },
        "trusted": true,
        "id": "LNreG0RTzUPh"
      },
      "execution_count": 40,
      "outputs": [],
      "id": "LNreG0RTzUPh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the dataset\n",
        "We will only train our model on images of 0 and 1. This is because training on all 10 numbers will take too much time, since CNN is very slow on CPU.\n",
        "\n",
        "We will reshape the and normalize the dataset before passing it into our CNN model. We will convert categorical labels (such as digits 0-9) into a vector numerical format. Later, our model will not directly predict the digits of handwritten images. Rather, our model gives a probability distribution of what digit an image might be. For example, an image of 0 will be [100%, 0%] meaning it have 100% probability of being a 0, and 0% probability of being other digits.\n",
        "By vectorizing the training data, we make later calculations more efficient."
      ],
      "metadata": {
        "_cell_guid": "245919c1-eaf3-4446-877b-560e70fd4ffb",
        "_uuid": "ff797e4a-15c7-4420-89ee-043e0e404f2c",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "c3MUcsL7zUPh"
      },
      "id": "c3MUcsL7zUPh"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "def preprocess_data(x, y, limit):\n",
        "    zero_index = np.where(y == 0)[0][:limit]\n",
        "    one_index = np.where(y == 1)[0][:limit]\n",
        "    all_indices = np.hstack((zero_index, one_index))\n",
        "    all_indices = np.random.permutation(all_indices)\n",
        "    x, y = x[all_indices], y[all_indices]\n",
        "    x = x.reshape(len(x), 1, 28, 28)\n",
        "    x = x.astype(\"float32\") / 255\n",
        "    y = np_utils.to_categorical(y)\n",
        "    y = y.reshape(len(y), 2, 1)\n",
        "    return x, y\n",
        "# 500 ones, 500 zeros\n",
        "x_train, y_train = preprocess_data(training_inputs, training_results, 100) # 100 images from each class = 200\n",
        "x_test, y_test = preprocess_data(test_inputs, test_results, 100) # 100 images from each class = 200\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-17T09:00:59.536062Z",
          "iopub.execute_input": "2023-07-17T09:00:59.536426Z",
          "iopub.status.idle": "2023-07-17T09:00:59.555605Z",
          "shell.execute_reply.started": "2023-07-17T09:00:59.536395Z",
          "shell.execute_reply": "2023-07-17T09:00:59.554539Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyU0mlAizUPi",
        "outputId": "680e06e0-be03-42d9-d71f-7a12d56c9fd7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 1, 28, 28)\n",
            "(200, 2, 1)\n"
          ]
        }
      ],
      "id": "VyU0mlAizUPi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a CNN\n",
        "CNN uses gradient descent just like many other models, and exactly the same as multilayer perceptron models.\n",
        "\n",
        "The rough idea is:\n",
        "1. Randomly initialize the parameters (weights and biases).\n",
        "2. Compute the gradient of the cost function in respect to the weights and biases for EVERY image. (i.e. computing how we should change the weights and biases so that the network is less wrong in EVERY image)\n",
        "3. Now we know how we should change the weights and biases so that the network is less wrong, we use this gradient to update the weights and biases (minus the weights and biases by the gradients times a tiny number called learning rate)\n",
        "4. Repeat for as many times as time and computation resource permits\n",
        "\n",
        "The above is called gradient descent.\n",
        "However, computing the gradient for EVERY image is often too slow. Therefore, we only use SOME subset of the dataset, which the exact amount is called the mini_batch_size. This is called stochastic gradient descent.\n",
        "\n",
        "\n",
        "Gradient descent is like a smooth ball rolling down the hill perfectly towards the steepest direction. Stochastic gradient descent is like a dice stumbling down the hill, sometimes rolling side ways, sometimes rolling up, but in general still going down."
      ],
      "metadata": {
        "id": "TROL7MSvMILb"
      },
      "id": "TROL7MSvMILb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN network structure\n",
        "Just like the multilayer perceptron (MLP) neural network seen before, a CNN a network of layers each with their own weights and biases that takes in inputs and give outputs.\n",
        "\n",
        "\n",
        "$$y = \\text{network}(x,weights, biases)$$\n",
        "\n",
        "\n",
        "However, the main difference is that CNN have many different types of layers instead of like the previous tutorial only having sigmoid neurons. Different types of layers include convolutional, dense, activations, dropouts, etc.\n",
        "\n",
        "```\n",
        "network = [\n",
        "    [ Convolutional(), Convolutional.initialize((1, 28, 28), 3, 32) ],\n",
        "    [ Tanh(), Tanh.initialize() ],\n",
        "    [ Convolutional(), Convolutional.initialize((32, 26, 26), 3, 64)],\n",
        "    [ Tanh(), Tanh.initialize() ],\n",
        "    [ Reshape(), Reshape.initialize( (64, 24, 24), (64 * 24 * 24, 1) ) ],\n",
        "    [ Dense(), Dense.initialize( 64 * 24 * 24, 128) ],\n",
        "    [ Tanh(), Tanh.initialize() ],\n",
        "    [ Dense(), Dense.initialize( 128, 64) ],\n",
        "    [ Tanh(), Tanh.initialize() ],\n",
        "    [ Dense(), Dense.initialize( 64, 10) ],\n",
        "    [ Softmax(), Softmax.initialize() ],\n",
        "]\n",
        "```\n",
        "__sequential network__\n",
        "We will implement a sequential network in this tutorial. This means the each layer in the network passes its output to the next layer until the final output is generated. (Y1 = X2, Y2 = X3 ...)\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn2.JPG\" width=\"50%\">\n",
        "\n",
        "\n",
        "In a sequential network, the information flows in one direction, from the input layer through the hidden layers to the output layer, without any feedback loops or connections between nodes in the same layer. This simplicity and linearity make it easy to build and understand, which is why it is commonly used for various tasks.\n",
        "\n",
        "__Notations__\n",
        "- X: inputs\n",
        "- Y: outputs\n",
        "- W: parameters (weights and biases)\n",
        "- E: error (difference between the model's output and actual labels)"
      ],
      "metadata": {
        "id": "P7H2qyA86-0I"
      },
      "id": "P7H2qyA86-0I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "__forward propagation__\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn1.JPG\" width=\"30%\">\n",
        "\n",
        "No matter which kind of layer, every layer is a function that takes in inputs and gives outputs according to some weights and biases. To make a prediction, we pass the input (hand written image) to the first layer. The first layer's output goes into the second layer, and its output goes into the third layer, and so on until the last layer. This is known as feed forward, or forward propagation. Every layer will have a `.forward()` function that takes in X and gives Y."
      ],
      "metadata": {
        "id": "kBQnCE3y_tum"
      },
      "id": "kBQnCE3y_tum"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(network, input):\n",
        "    for layer, parameters in network:\n",
        "        output = layer.forward(input, parameters)\n",
        "\n",
        "        # the output of this layer becomes of input of the next layer (which is for the next iteration of loop)\n",
        "        input = output\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "ceD7GYTrCzBf"
      },
      "id": "ceD7GYTrCzBf",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__backward propagation__\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn1.JPG\" width=\"30%\">\n",
        "\n",
        "\n",
        "Each layer must also be able to update its weights and biases according to the error derivative, so they can minimize the errors and therefore improve on accuracy. Each layer will take in $\\frac{\\Delta E}{\\Delta Y}$ (we will explain how to get this later) and needs to calculate the gradient $\\frac{\\Delta E}{\\Delta W}$ so it knows how to update its $W$ to minimize the error. It needs to also calculate $\\frac{\\Delta E}{\\Delta X}$ which will be given the layer before this layer. Every layer's $\\frac{\\Delta E}{\\Delta Y}$ is equal to the next layer's $\\frac{\\Delta E}{\\Delta X}$.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn3.jpg\" width=\"70%\">\n",
        "\n",
        "The process of 1) every layer calculating error gradient, 2) updating its parameters, and 3) passing $\\frac{\\Delta E}{\\Delta X}$ to the previous layer is known as back propagation. This is how we train a model. We will explain where the last layer get the error from.\n",
        "\n",
        "Later, we will see that to perform back propagation, every layer needs the input given to this layer. Therefore, we will make another prediction function that keeps the list of inputs as history."
      ],
      "metadata": {
        "id": "FljcfaqyCyJK"
      },
      "id": "FljcfaqyCyJK"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_history(network, input):\n",
        "    input_history = []\n",
        "    for layer, parameters in network:\n",
        "        input_history.append(input)\n",
        "        output = layer.forward(input, parameters)\n",
        "\n",
        "        # the output of this layer becomes of input of the next layer (which is for the next iteration of loop)\n",
        "        input = output\n",
        "\n",
        "    return output, input_history"
      ],
      "metadata": {
        "id": "X0e-ZeMcMQlN"
      },
      "id": "X0e-ZeMcMQlN",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layers\n",
        "As seen above, every layer must have forward propagation and backward propagation. We will call this `forward` and `backward` function, and organize them into classes. All layers will follow this template:"
      ],
      "metadata": {
        "id": "euP57tShF2fS"
      },
      "id": "euP57tShF2fS"
    },
    {
      "cell_type": "code",
      "source": [
        "class Generic_layer:\n",
        "  def forward(input, parameters):\n",
        "    output = None # calculate output\n",
        "    return output\n",
        "\n",
        "  def backward(input, output_gradient, learning_rate, parameters):\n",
        "    # calculate gradients in respect to the parameters\n",
        "    parameter_gradient = None\n",
        "\n",
        "    # update the parameters according to the parameter gradient\n",
        "    parameters = parameters - parameter_gradient * learning_rate\n",
        "\n",
        "    # calculate gradients in respect to the inputs and return it\n",
        "    input_gradient = None\n",
        "    return input_gradient\n"
      ],
      "metadata": {
        "id": "O0VHpo8-GPAC"
      },
      "id": "O0VHpo8-GPAC",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense layer\n",
        "A dense layer, also known as a fully connected layer, is a type of layer where each neuron (or node) in the layer is connected to every neuron in the previous layer. The input data is a 1D array and then uses the weight matrix and biase vector to compute the output (also 1D array). The dense layer plays a crucial role in transforming the input data into higher-level representations, enabling the neural network to learn complex patterns and make predictions across various tasks, such as image recognition, natural language processing, and more.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn4.jpg\" width=\"50%\">\n",
        "\n",
        "This can be written in matrix form. The input to the dense layer is denoted as vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_N]$. Let's assume we have an input vector $\\mathbf{x}$ of size $N$ and output vector $\\mathbf{y}$ of size $M$. The output, denoted as $\\mathbf{y} = [y_1, y_2, \\ldots, y_M]$, is computed as follows:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{W}$ is a $M$x$N$ matrix and $\\mathbf{B}$ is a vector of size $M$\n",
        "\n",
        "The gradient equations for backward propagation is:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn5.jpg\" width=\"20%\">\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R26YqBaWIHHX"
      },
      "id": "R26YqBaWIHHX"
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense():\n",
        "    def initialize(input_size, output_size):\n",
        "        weights = np.random.randn(output_size, input_size)\n",
        "        bias = np.random.randn(output_size, 1)\n",
        "        parameters = [weights, bias]\n",
        "        return parameters\n",
        "\n",
        "    def forward(input, parameters):\n",
        "        # unpack the parameters\n",
        "        weights, bias = parameters\n",
        "\n",
        "        return np.dot(weights, input) + bias\n",
        "\n",
        "    def backward(input, output_gradient, learning_rate, parameters):\n",
        "        # unpack the parameters\n",
        "        weights, bias = parameters\n",
        "\n",
        "        # calculate gradients\n",
        "        weights_gradient = np.dot(output_gradient, input.T)\n",
        "        input_gradient = np.dot(weights.T, output_gradient)\n",
        "\n",
        "        # update the parameters according to the parameter gradient\n",
        "        weights -= learning_rate * weights_gradient\n",
        "        bias -= learning_rate * output_gradient\n",
        "\n",
        "        return input_gradient"
      ],
      "metadata": {
        "id": "wrtL3tVZK6PO"
      },
      "id": "wrtL3tVZK6PO",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation layer\n",
        "The activation layer takes in some input neurons and simply passes them through an activation function. Thus for that layer the output has the same shape as the input. It also have no parameters so the backpropagation function does not update anything.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn7.jpg\" width=\"30%\">\n",
        "\n",
        "The backpropagation equations are as follow:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn6.jpg\" width=\"33%\">\n"
      ],
      "metadata": {
        "id": "aEAc4burNtFJ"
      },
      "id": "aEAc4burNtFJ"
    },
    {
      "cell_type": "code",
      "source": [
        "class Generic_activation_layer():\n",
        "    def initialize():\n",
        "        # no need to do anything because there are no parameters\n",
        "        return None\n",
        "\n",
        "    def forward(input, parameters):\n",
        "        return # activation_func(input)\n",
        "\n",
        "    def backward(input, output_gradient, learning_rate, parameters): #learning rate is not used\n",
        "        return # np.multiply(output_gradient, activation_func_prime(input))\n"
      ],
      "metadata": {
        "id": "b-N_Baz8OTfu"
      },
      "id": "b-N_Baz8OTfu",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tanh activation layer\n",
        "The hyperbolic tangent is an activation function commonly used in neural networks. It maps the input values to the range (-1, 1), making it a good choice for normalization and handling data with negative and positive values. The tanh function is defined as tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x)). As an activation function, tanh is symmetric around the origin, making it suitable for tasks where positive and negative values need to be treated similarly.\n",
        "\n",
        "$$ \\text{tanh}(x) = \\frac{{e^x - e^{-x}}}{{e^x + e^{-x}}} $$\n",
        "\n",
        "$$ \\frac{{d}}{{dx}} \\text{{tanh}}(x) = 1 - \\text{{tanh}}^2(x) $$\n",
        "\n",
        "\n",
        "<img src=\"\n",
        "https://mathworld.wolfram.com/images/interactive/TanhReal.gif\" width=\"30%\">\n"
      ],
      "metadata": {
        "id": "dEQTm_MWPHnU"
      },
      "id": "dEQTm_MWPHnU"
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "def tanh_prime(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "class Tanh():\n",
        "    def initialize():\n",
        "        # no need to do anything because there are no parameters\n",
        "        return None\n",
        "\n",
        "    def forward(input, parameters):\n",
        "        return tanh(input)\n",
        "\n",
        "    def backward(input, output_gradient, learning_rate, parameters): #learning rate is not used\n",
        "        return np.multiply(output_gradient, tanh_prime(input))\n"
      ],
      "metadata": {
        "id": "nUpIYr2qPt53"
      },
      "id": "nUpIYr2qPt53",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rectified Linear Unit (ReLU) activation layer\n",
        "ReLU is defined as ReLU(x) = max(0, x), which means it returns the input value if it is positive and zero otherwise. ReLU introduces non-linearity to the network, enabling it to learn complex patterns and representations. The derivative of ReLU is 1 for positive inputs and 0 for negative inputs, making it computationally efficient to calculate gradients during the backpropagation process.\n",
        "\n",
        "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
        "\n",
        "$$\\frac{d}{dx} \\text{ReLU}(x) =\n",
        "\\begin{cases}\n",
        "      1 & \\text{if } x > 0 \\\\\n",
        "      0 & \\text{if } x \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "<img src=\"https://www.nomidl.com/wp-content/uploads/2022/04/image-10.png\" width=\"30%\">\n",
        "\n"
      ],
      "metadata": {
        "id": "oBpERVBMQyfu"
      },
      "id": "oBpERVBMQyfu"
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "    def initialize():\n",
        "        # no need to do anything because there are no parameters\n",
        "        return None\n",
        "\n",
        "    def forward(input, parameters):\n",
        "        return np.maximum(0, input)\n",
        "\n",
        "    def backward(input, output_gradient, learning_rate, parameters): # learning rate is not used\n",
        "        relu_gradient = input > 0\n",
        "        return np.multiply(output_gradient, relu_gradient)\n"
      ],
      "metadata": {
        "id": "XNX11Y3HQaj2"
      },
      "id": "XNX11Y3HQaj2",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid activation layer\n",
        "The sigmoid function is a popular activation function in early NN models for several reason.\n",
        "1. It is continuous and differentiable, enabling calculation of gradients.\n",
        "2. It is non-linear, which means it can solve non-linearly separable problem\n",
        "3. It's output is between 0 and 1, which stabilize the training process by preventing large, unbounded values from propagating through the network. It also allows for a natural interpretation of the output as a probability.\n",
        "\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "$$\n",
        "\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
        "$$\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png\" width=\"30%\">\n"
      ],
      "metadata": {
        "_cell_guid": "09cb1a0e-d84e-48cb-9b8b-04df89f6c360",
        "_uuid": "8e8dab59-694a-462d-8d10-f819c201a467",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "5ZEirabWzUPj"
      },
      "id": "5ZEirabWzUPj"
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "class Sigmoid():\n",
        "    def initialize():\n",
        "        # no need to do anything because there are no parameters\n",
        "        return None\n",
        "\n",
        "    def forward(input, parameters):\n",
        "        return sigmoid(input)\n",
        "\n",
        "    def backward(input, output_gradient, learning_rate, parameters): #learning rate and parameters is not used\n",
        "        return np.multiply(output_gradient, sigmoid_prime(input))\n"
      ],
      "metadata": {
        "_cell_guid": "dfb29956-d59a-44c5-80c7-d09c8e16fcd2",
        "_uuid": "d67c247f-2fbf-4474-a3d0-375cd1f3f75f",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-07-17T09:07:41.152958Z",
          "iopub.status.idle": "2023-07-17T09:07:41.153513Z",
          "shell.execute_reply.started": "2023-07-17T09:07:41.153284Z",
          "shell.execute_reply": "2023-07-17T09:07:41.153305Z"
        },
        "trusted": true,
        "id": "qnIxSb0lzUPk"
      },
      "execution_count": 49,
      "outputs": [],
      "id": "qnIxSb0lzUPk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax activation layer\n",
        "The softmax activation function is commonly used in the output layer of neural networks for multiclass classification tasks. It converts a vector of real numbers into a probability distribution, where the sum of all the elements in the output vector becomes 1. The softmax function is defined as follows:\n",
        "\n",
        "$$\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}$$\n",
        "$$ \\frac{\\partial}{\\partial x_i} \\text{Softmax}(x_i) = \\frac{e^{x_i} \\left( \\sum_{j=1}^{N} e^{x_j} \\right) - e^{x_i} e^{x_i}}{\\left( \\sum_{j=1}^{N} e^{x_j} \\right)^2} = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}} \\cdot \\left( 1 - \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}} \\right)\n",
        "$$\n",
        "The softmax function ensures that the output values are positive and normalized, allowing them to represent the probabilities of different classes. This makes softmax ideal for multi-class classification problems, as it enables the model to predict the class with the highest probability.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yoenkT69UuCP"
      },
      "id": "yoenkT69UuCP"
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax():\n",
        "    def initialize():\n",
        "        # no need to do anything because there are no parameters\n",
        "        return None\n",
        "\n",
        "    def forward(input, parameters):\n",
        "        tmp = np.exp(input)\n",
        "        output = tmp / np.sum(tmp)\n",
        "        return output\n",
        "\n",
        "    def backward(input, output_gradient, learning_rate, parameters):\n",
        "        output = Softmax.forward(input, parameters)\n",
        "        n = np.size(output)\n",
        "        return np.dot((np.identity(n) - output.T) * output, output_gradient)\n"
      ],
      "metadata": {
        "id": "I-DtJwIzUTS-"
      },
      "id": "I-DtJwIzUTS-",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reshape layer\n",
        "The reshape layer is used to change the dimensions of the input vectors while preserving its total number of elements. It allows for the transformation of the data to fit the desired shape required by subsequent layers in the network.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aVHJyUSwFDeE"
      },
      "id": "aVHJyUSwFDeE"
    },
    {
      "cell_type": "code",
      "source": [
        "class Reshape():\n",
        "    def initialize(input_shape, output_shape):\n",
        "        parameters =  input_shape, output_shape\n",
        "        return parameters\n",
        "\n",
        "    def forward(input, parameters):\n",
        "        input_shape, output_shape = parameters\n",
        "        return np.reshape(input, output_shape)\n",
        "\n",
        "    def backward(input, output_gradient, learning_rate, parameters):\n",
        "        input_shape, output_shape = parameters\n",
        "        return np.reshape(output_gradient, input_shape)\n"
      ],
      "metadata": {
        "id": "DPMrOHOQA05U"
      },
      "id": "DPMrOHOQA05U",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution and cross-correlation\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:640/0*e-SMFTzO8r7skkpc\" width=\"51%\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn9.jpg\" width=\"50%\">"
      ],
      "metadata": {
        "id": "q5oVrhMAajZW"
      },
      "id": "q5oVrhMAajZW"
    },
    {
      "cell_type": "code",
      "source": [
        "def correlate2d(input_image, kernel, mode='valid'):\n",
        "    # Get dimensions of input image and kernel\n",
        "    input_height, input_width = input_image.shape\n",
        "    kernel_height, kernel_width = kernel.shape\n",
        "\n",
        "    if mode == 'valid':\n",
        "        # Calculate output size for 'valid' mode\n",
        "        output_height = input_height - kernel_height + 1\n",
        "        output_width = input_width - kernel_width + 1\n",
        "    elif mode == 'same':\n",
        "        # Calculate output size for 'same' mode\n",
        "        output_height = input_height\n",
        "        output_width = input_width\n",
        "    else:  # mode == 'full'\n",
        "        # Calculate output size for 'full' mode\n",
        "        output_height = input_height + kernel_height - 1\n",
        "        output_width = input_width + kernel_width - 1\n",
        "\n",
        "    # Initialize the output with zeros\n",
        "    output = np.zeros((output_height, output_width))\n",
        "\n",
        "    # Perform 2D correlation\n",
        "    for i in range(output_height):\n",
        "        for j in range(output_width):\n",
        "            # Extract the region of interest (ROI) from the input image\n",
        "            roi = input_image[i:i + kernel_height, j:j + kernel_width]\n",
        "\n",
        "            # Ensure the ROI and kernel have the same shape before element-wise multiplication\n",
        "            if roi.shape == kernel.shape:\n",
        "                output[i, j] = np.sum(roi * kernel)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "hlcYezPcdYeD"
      },
      "id": "hlcYezPcdYeD",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolve involves flipping the kernel both horizontally and vertically before sliding it across the input image to compute the output, while correlate does not perform any flipping and directly slides the kernel across the image. Therefore, we can write the `convolve2d` function using the `correlate2d`"
      ],
      "metadata": {
        "id": "CnOCTVuiUZHS"
      },
      "id": "CnOCTVuiUZHS"
    },
    {
      "cell_type": "code",
      "source": [
        "def convolve2d(input_image, kernel, mode='valid'):\n",
        "    # Flip the kernel horizontally and vertically\n",
        "    flipped_kernel = np.flipud(np.fliplr(kernel))\n",
        "\n",
        "    # Call the correlate2d function with the flipped kernel\n",
        "    conv_result = correlate2d(input_image, flipped_kernel, mode=mode)\n",
        "\n",
        "    return conv_result\n"
      ],
      "metadata": {
        "id": "bG_OdytNL3lo"
      },
      "id": "bG_OdytNL3lo",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Layer\n",
        "The convolutional layer is a fundamental component of CNNs used in image recognition tasks. It applies a set of learnable filters, called kernels, to the input image in order to extract local features. The filters slide over the image, computing element-wise multiplications and summations to produce feature maps that highlight relevant patterns. This process enables the network to learn hierarchical representations, capturing low-level features in early layers and complex patterns in deeper layers, facilitating more efficient and accurate feature extraction for subsequent tasks like classification or object detection.\n",
        "\n",
        "The convolutional layer contains 3 main parameters: the input shape, kernel size, and depth.\n",
        "<img src=\"https://i0.wp.com/developersbreach.com/wp-content/uploads/2020/08/cnn_banner.png?fit=1200%2C564&ssl=1\" width=\"80%\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn11.jpg\" width=\"40%\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn12.jpg\" width=\"30%\">\n",
        "\n"
      ],
      "metadata": {
        "id": "C9Tcbhppd6N2"
      },
      "id": "C9Tcbhppd6N2"
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolutional():\n",
        "    def initialize(input_shape, kernel_size, depth): # e.g. 1x28x28 image, 3x3 kernels\n",
        "        input_depth, input_height, input_width = input_shape\n",
        "        output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
        "        kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
        "        kernels = np.random.randn(*kernels_shape)\n",
        "        biases = np.random.randn(*output_shape)\n",
        "        parameters = depth, input_shape, output_shape, kernels_shape, kernels, biases\n",
        "        return parameters\n",
        "\n",
        "    def forward(input, parameters):\n",
        "        depth, input_shape, output_shape, kernels_shape, kernels, biases = parameters\n",
        "        input_depth, input_height, input_width = input_shape\n",
        "\n",
        "        output = np.copy(biases)\n",
        "        for i in range(depth):\n",
        "            for j in range(input_depth):\n",
        "                output[i] += correlate2d(input[j], kernels[i, j], \"valid\")\n",
        "        return output\n",
        "\n",
        "    def backward(input, output_gradient, learning_rate, parameters):\n",
        "        depth, input_shape, output_shape, kernels_shape, kernels, biases = parameters\n",
        "        input_depth, input_height, input_width = input_shape\n",
        "\n",
        "        kernels_gradient = np.zeros(kernels_shape)\n",
        "        input_gradient = np.zeros(input_shape)\n",
        "\n",
        "        for i in range(depth):\n",
        "            for j in range(input_depth):\n",
        "                kernels_gradient[i, j] = correlate2d(input[j], output_gradient[i], \"valid\")\n",
        "                input_gradient[j] += convolve2d(output_gradient[i], kernels[i, j], \"full\")\n",
        "\n",
        "        kernels -= learning_rate * kernels_gradient\n",
        "        biases -= learning_rate * output_gradient\n",
        "        return input_gradient\n"
      ],
      "metadata": {
        "id": "bZL45oPYcGkB"
      },
      "id": "bZL45oPYcGkB",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical Cross-Entropy Error functions\n",
        "Remember that during backpropagation, every layer gets the output error gradient from the next layer. However, the very last layer does not have another layer after it. The output error gradient of the last layer is essentially the ouput error of the entire network. Therefore, we can calculate the error using the error functions, which compares the output of CNN with the real labels.\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.old/Section%200%20ML%20models/images/cnn8.jpg\" width=\"70%\">\n",
        "\n",
        "Categorical Cross-Entropy is used for multi-class classification problems, where there are more than two classes. It measures the dissimilarity between the predicted probability distribution and the true probability distribution of the target classes.\n",
        "\n",
        "$$L_{\\text{CCE}} = -\\sum_{i=1}^{K} y_i \\cdot \\log(p_i)\n",
        "$$\n",
        "\n",
        "$$\\frac{{\\partial L_{\\text{CCE}}}}{{\\partial p_i}} = -\\frac{{y_i}}{{p_i}}\n",
        "$$"
      ],
      "metadata": {
        "id": "z3CZZ_QqW8d_"
      },
      "id": "z3CZZ_QqW8d_"
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def binary_cross_entropy_prime(y_true, y_pred):\n",
        "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
      ],
      "metadata": {
        "id": "GrLtvOW7XAAV"
      },
      "id": "GrLtvOW7XAAV",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = [\n",
        "    [ Convolutional, Convolutional.initialize((1, 28, 28), 3, 5) ],\n",
        "    [ Tanh, Tanh.initialize() ],\n",
        "    [ Reshape, Reshape.initialize( (5, 26, 26), (5*26*26, 1) ) ],\n",
        "    [ Dense, Dense.initialize( 5* 26 * 26, 100) ],\n",
        "    [ Tanh, Tanh.initialize() ],\n",
        "    [ Dense, Dense.initialize( 100, 2) ],\n",
        "    [ Softmax, Softmax.initialize() ],\n",
        "]\n",
        "\n",
        "def train(network, loss, loss_prime, x_train, y_train, epochs, learning_rate):\n",
        "    for e in range(epochs):\n",
        "        time1 = time.time()\n",
        "\n",
        "        error = 0\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            # forward\n",
        "            output, input_history = predict_with_history(network, x)\n",
        "\n",
        "            # error\n",
        "            error += loss(y, output)\n",
        "\n",
        "            # backward\n",
        "            grad = loss_prime(y, output)\n",
        "            for layer, parameters in reversed(network):\n",
        "                grad = layer.backward(input_history[-1], grad, learning_rate, parameters)\n",
        "                del input_history[-1] # delete last item of input_history, so in the next loop the 2nd last item is used.\n",
        "\n",
        "        time2 = time.time()\n",
        "        print(f\"{e + 1}/{epochs}, error={error}, time = {time2-time1}\")\n",
        "# train\n",
        "train(\n",
        "    network,\n",
        "    binary_cross_entropy,\n",
        "    binary_cross_entropy_prime,\n",
        "    x_train[:100],\n",
        "    y_train[:100],\n",
        "    epochs=5,\n",
        "    learning_rate=0.01\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTW_RTo1eldb",
        "outputId": "dfe608fc-1bd6-443d-f8a5-ee833cba2107"
      },
      "id": "nTW_RTo1eldb",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/5, error=201.84634894241643, time = 15.334441661834717\n",
            "2/5, error=54.28732029204529, time = 11.595065593719482\n",
            "3/5, error=39.368823194789535, time = 10.426090955734253\n",
            "4/5, error=23.178749988653525, time = 13.723630905151367\n",
            "5/5, error=13.572673149687727, time = 12.342512369155884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test accuracy\n",
        "count_of_corrects = 0\n",
        "N = len(x_test)\n",
        "for x, y in zip(x_test, y_test):\n",
        "    output = predict(network, x)\n",
        "    if np.argmax(output)==np.argmax(y):\n",
        "        count_of_corrects+=1\n",
        "print(count_of_corrects/N)\n"
      ],
      "metadata": {
        "id": "ojyFdgbde39w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4972473c-348a-4b04-d3d6-949d2e63cec2"
      },
      "id": "ojyFdgbde39w",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises\n",
        "1. Explain why the 3rd layer `[ Reshape, Reshape.initialize( (5, 26, 26), (5*26*26, 1) ) ]` have shape (5,26,26). Can it be changed to (50,36,36)? Why?\n",
        "2.Change the last hidden layer of the network from 100 neurons to 50 neurons, and observe the result"
      ],
      "metadata": {
        "_cell_guid": "407b489b-e7d4-47c6-8d0d-5bc62e49061e",
        "_uuid": "f1748679-ce0f-4927-aaf6-89898cceed49",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "wD98FOnmzUPo"
      },
      "id": "wD98FOnmzUPo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "- https://medium.com/@bdhuma/6-basic-things-to-know-about-convolution-daef5e1bc411\n",
        "- https://towardsdatascience.com/building-a-convolutional-neural-network-from-scratch-using-numpy-a22808a00a40\n",
        "- https://github.com/TheIndependentCode/Neural-Network\n",
        "- https://github.com/andreoniriccardo/CNN-from-scratch\n",
        "- https://www.youtube.com/watch?v=Lakz2MoHy6o\n",
        "- http://neuralnetworksanddeeplearning.com/\n",
        "- https://www.youtube.com/watch?v=pauPCy_s0Ok\n"
      ],
      "metadata": {
        "_cell_guid": "5967597a-639e-4097-bdbd-ee6f1b2512b0",
        "_uuid": "e3772d6d-7fde-42f1-a7ba-833fd2ef17fd",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "usiOVtBmzUPo"
      },
      "id": "usiOVtBmzUPo"
    }
  ]
}