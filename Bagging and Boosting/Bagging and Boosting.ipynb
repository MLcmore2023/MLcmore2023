{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78773546",
   "metadata": {},
   "source": [
    "# Bagging and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc1bc8b",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd017a3e",
   "metadata": {},
   "source": [
    "### 1.1 Overview of Ensemble Learning:\n",
    "\n",
    "- Ensemble learning is a powerful technique in machine learning where multiple models are combined to create a more robust and accurate predictive model.\n",
    "- The idea is to leverage the collective intelligence of multiple models to make better predictions than any individual model could achieve alone.\n",
    "- Ensemble learning draws inspiration from the wisdom of crowds, where diverse opinions lead to better decisions than relying on a single perspective.\n",
    "- By aggregating the predictions from multiple models, ensemble learning aims to improve generalization and reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d91e2c4",
   "metadata": {},
   "source": [
    "### 1.2 Importance of Ensemble Learning in Data Science:\n",
    "\n",
    "- Data science often deals with complex and noisy datasets where building a single strong model may be challenging.\n",
    "- Ensemble learning helps address the limitations of individual models by combining their strengths and compensating for their weaknesses.\n",
    "- Ensemble methods can significantly boost the accuracy and performance of predictive models, making them invaluable in various real-world applications.\n",
    "- Ensemble learning is widely used in competitions like Kaggle, where top-performing models often utilize sophisticated ensemble techniques.\n",
    "- In practical scenarios, ensemble models are more reliable and trustworthy due to their ability to capture diverse patterns and avoid model bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23907de",
   "metadata": {},
   "source": [
    "### 1.3 What to Expect in This Session:\n",
    "\n",
    "- In this session, we will delve into two essential ensemble learning techniques: bagging and boosting.\n",
    "- We will understand the underlying principles of each technique, explore their strengths and weaknesses, and discover when to use them.\n",
    "- We will also demonstrate coding examples using Python and scikit-learn to solidify your understanding and enable you to implement these techniques in your projects.\n",
    "- By the end of this session, you will have a clear understanding of how bagging and boosting contribute to the success of predictive models and the importance of ensemble learning in data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ffcb0",
   "metadata": {},
   "source": [
    "# 2. Bagging (Bootstrap Aggregating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219b1ff",
   "metadata": {},
   "source": [
    "### 2.1 Definition and Purpose of Bagging:\n",
    "\n",
    "- Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that aims to improve the accuracy and robustness of predictive models.\n",
    "- The primary purpose of bagging is to reduce variance and combat overfitting, which are common challenges in machine learning.\n",
    "- By combining predictions from multiple base learners, bagging reduces the risk of relying too heavily on any one model's idiosyncrasies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d7ea7",
   "metadata": {},
   "source": [
    "### 2.2 How Bagging Works:\n",
    "\n",
    "1. Bootstrapping and Creating Subsets:\n",
    "- Bagging starts by creating multiple subsets of the original training data through bootstrapping.\n",
    "- Bootstrapping involves randomly sampling data with replacement, resulting in diverse subsets with the same size as the original dataset.\n",
    "\n",
    "\n",
    "2. Training Multiple Base Learners:\n",
    "- Each subset is used to train an independent base learner (model), such as decision trees, SVMs, or neural networks.\n",
    "- These base learners are trained independently, and their predictions are combined later to form the ensemble.\n",
    "\n",
    "\n",
    "3. Aggregating Predictions:\n",
    "- Once all base learners are trained, the final prediction is made by aggregating their individual predictions.\n",
    "- For regression tasks, predictions are averaged, while for classification tasks, majority voting is used to decide the final prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fed609",
   "metadata": {},
   "source": [
    "### 2.3 Advantages of Bagging:\n",
    "\n",
    "1. Reducing Variance and Overfitting:\n",
    "- Bagging's main advantage is its ability to reduce variance and combat overfitting.\n",
    "- By combining predictions from diverse models, it results in a more balanced and stable ensemble model.\n",
    "\n",
    "\n",
    "2. Robustness to Noisy Data and Outliers:\n",
    "- Bagging is robust to noisy data and outliers, as it diminishes the impact of individual data points through aggregation.\n",
    "- Outliers are less likely to affect the overall prediction due to the averaging or voting process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d54f77",
   "metadata": {},
   "source": [
    "### 2.4 Common Algorithms Using Bagging:\n",
    "\n",
    "1. Random Forest:\n",
    "- Random Forest is one of the most popular bagging algorithms that uses decision trees as base learners.\n",
    "- It builds a large number of decision trees, and each tree is trained on a bootstrapped subset of the data.\n",
    "- The final prediction is made by aggregating the predictions of all individual trees.\n",
    "\n",
    "2. Bagged Decision Trees:\n",
    "- Bagged Decision Trees, or simply Bagging with decision trees, is a straightforward bagging approach.\n",
    "- It applies the same concept as Random Forest but with a smaller number of trees (often not enough to be considered a forest).\n",
    "\n",
    "3. Bagged SVMs (Support Vector Machines):\n",
    "- Bagging can also be applied to other base learners like Support Vector Machines (SVMs).\n",
    "- Bagged SVMs utilize subsets of data for training multiple SVM models, and their predictions are aggregated to make the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7fc12d",
   "metadata": {},
   "source": [
    "### 2.5 Coding Example with Random Forest (Python and scikit-learn):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f885e4",
   "metadata": {},
   "source": [
    "#### 2.5.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cf1265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e987d",
   "metadata": {},
   "source": [
    "These are the necessary libraries for the following tasks:\n",
    "\n",
    "- **pandas:** For data handling (e.g., creating DataFrames).\n",
    "- **fetch_openml:** Fetches datasets from the OpenML repository.\n",
    "- **DecisionTreeClassifier:** The decision tree classifier we'll use.\n",
    "- **RandomForestClassifier:** The Random Forest ensemble method.\n",
    "- **train_test_split:** Splits the data into training and testing sets.\n",
    "- **accuracy_score:** Computes the accuracy of predictions.\n",
    "- **resample:** Helps in bootstrapping samples.\n",
    "- **numpy:** For numerical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de39c9",
   "metadata": {},
   "source": [
    "#### 2.5.2 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5f3f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the covertype dataset from OpenML\n",
    "covertype = fetch_openml('covertype', version=4)\n",
    "X = covertype.data\n",
    "y = covertype.target.astype('int') - 1  # Shift labels to range 0-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62305863",
   "metadata": {},
   "source": [
    "This code fetches the **covertype** dataset from OpenML, and then extracts the data and target labels. The labels are shifted from 1-7 to 0-6 for ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1f6a1",
   "metadata": {},
   "source": [
    "#### 2.5.3 Data Subset (for speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77813af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of data for demonstration purposes (and for faster execution)\n",
    "X = X[:20000]\n",
    "y = y[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0edf82",
   "metadata": {},
   "source": [
    "Given the large size of the **covertype** dataset, we take a subset of 20,000 samples for faster demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a614f",
   "metadata": {},
   "source": [
    "#### 2.5.4 Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8818c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ccacc6",
   "metadata": {},
   "source": [
    "This splits the data into training and testing sets. 30% of the data is reserved for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8459d3",
   "metadata": {},
   "source": [
    "#### 2.5.5 Results Storage Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f453cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store results of each model\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0fe70f",
   "metadata": {},
   "source": [
    "An empty list to store accuracy results for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdcd93f",
   "metadata": {},
   "source": [
    "#### 2.5.6 Simple Decision Tree Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8f8ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a simple decision tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "results.append(['Simple Decision Tree', accuracy_score(y_test, tree_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fd93d",
   "metadata": {},
   "source": [
    "This section trains a basic decision tree on the training data, makes predictions on the test data, and stores the accuracy in the results list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559ad25",
   "metadata": {},
   "source": [
    "#### 2.5.7 Random Forest Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "409e5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a Random Forest with 5 trees\n",
    "rf_small = RandomForestClassifier(n_estimators=5, random_state=42)\n",
    "rf_small.fit(X_train, y_train)\n",
    "rf_small_pred = rf_small.predict(X_test)\n",
    "results.append(['Random Forest (5 trees)', accuracy_score(y_test, rf_small_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ca0d9",
   "metadata": {},
   "source": [
    "Trains a Random Forest model with 5 trees (small for demonstration), makes predictions, and adds the accuracy to the results list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3c53a",
   "metadata": {},
   "source": [
    "#### 2.5.8 Manual Bagging Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18426bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual implementation of Bagging\n",
    "n_learners = 5  # Number of learners (trees) for bagging\n",
    "predictions = []  # List to store predictions of each learner\n",
    "\n",
    "# Bootstrapping and training for each learner\n",
    "for i in range(n_learners):\n",
    "    boot_X, boot_y = resample(X_train, y_train, replace=True, n_samples=len(X_train))\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(boot_X, boot_y)\n",
    "    predictions.append(tree.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef8078",
   "metadata": {},
   "source": [
    "This manually implements bagging by bootstrapping the training data **n_learners** times, training a decision tree on each bootstrap sample, and storing its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e1a047",
   "metadata": {},
   "source": [
    "#### 2.5.9 Mode Calculation for Bagging Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6325c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute mode for an array\n",
    "def compute_mode(array):\n",
    "    return np.bincount(array).argmax()\n",
    "\n",
    "# Convert predictions list of arrays to a 2D numpy array\n",
    "predictions_array = np.array(predictions)\n",
    "\n",
    "# Calculate mode of predictions for bagging\n",
    "mode_predictions = []\n",
    "for column in predictions_array.T:  # Now we transpose the numpy array\n",
    "    mode_predictions.append(compute_mode(column))\n",
    "mode_predictions = np.array(mode_predictions)\n",
    "results.append(['Manual Bagging (Mode)', accuracy_score(y_test, mode_predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c2157",
   "metadata": {},
   "source": [
    "For each sample in the test set, this computes the mode (most common prediction) across the **n_learners** decision trees and stores the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bcbbe",
   "metadata": {},
   "source": [
    "#### 2.5.10 Median Calculation for Bagging Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d48c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate median of predictions for bagging\n",
    "median_predictions = np.median(predictions, axis=0).astype(int)\n",
    "results.append(['Manual Bagging with Median', accuracy_score(y_test, median_predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dcb68f",
   "metadata": {},
   "source": [
    "This section demonstrates another way to aggregate predictions by using the median. It then stores the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e90b86",
   "metadata": {},
   "source": [
    "#### 2.5.11 Convert Results to DataFrame and Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59420825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Manual Bagging (Mode)</td>\n",
       "      <td>0.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manual Bagging with Median</td>\n",
       "      <td>0.835667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest (5 trees)</td>\n",
       "      <td>0.827667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simple Decision Tree</td>\n",
       "      <td>0.815167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy\n",
       "0       Manual Bagging (Mode)  0.841000\n",
       "1  Manual Bagging with Median  0.835667\n",
       "2     Random Forest (5 trees)  0.827667\n",
       "3        Simple Decision Tree  0.815167"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the results list to a DataFrame and display it\n",
    "df_results = pd.DataFrame(results, columns=['Model', 'Accuracy'])\n",
    "\n",
    "# Sort the DataFrame based on the 'Accuracy' column in descending order\n",
    "df_results = df_results.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c64b71",
   "metadata": {},
   "source": [
    "This converts the **results** list into a pandas DataFrame and displays it for easy comparison of model accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983580b0",
   "metadata": {},
   "source": [
    "### 2.6 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9e011",
   "metadata": {},
   "source": [
    "1. Basic Understanding:\n",
    "\n",
    "- Question: What is the main principle behind the Bagging technique? How does it help in reducing overfitting?\n",
    "- Answer: Bagging, or Bootstrap Aggregating, involves training multiple models independently on different bootstrapped subsets of the data and then aggregating their predictions. By averaging out the errors, bagging helps in reducing the variance and thus overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dead7c3",
   "metadata": {},
   "source": [
    "2. Practical Implementation:\n",
    "\n",
    "- Exercise: Implement bagging from scratch using Decision Trees as the base learner on the Iris dataset. Compare its performance with a single Decision Tree.\n",
    "- Hint: Remember to bootstrap samples for each tree and average the predictions for regression or take a mode for classification.\n",
    "3. Deep Dive:\n",
    "\n",
    "- Question: How does out-of-bag (OOB) error estimation work in Bagging?\n",
    "- Answer: Each bootstrapped subset on average contains about 63.2% of the unique samples of the dataset. The remaining 36.8% that are not sampled are called out-of-bag samples. Each base learner (e.g., tree) in bagging can be validated using its OOB samples, and this gives an unbiased error estimate. The OOB error is the average error for each training sample calculated using predictions from the trees that did not have this sample in their bootstrap sample.\n",
    "4. Advanced Topic:\n",
    "\n",
    "- Exercise: Study Random Forests. How do they extend the basic idea of bagging? Implement a Random Forest using scikit-learn on the Wine dataset and tweak its hyperparameters.\n",
    "- Hint: Look into parameters like max_features, max_depth, and min_samples_split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6591ec",
   "metadata": {},
   "source": [
    "# 3. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df031e2",
   "metadata": {},
   "source": [
    "### 3.1 Definition and Purpose of Boosting:\n",
    "\n",
    "- Boosting is an ensemble learning technique that focuses on creating a strong learner by combining multiple weak learners iteratively.\n",
    "- The main purpose of boosting is to improve the accuracy and performance of predictive models by giving more emphasis to misclassified instances during training.\n",
    "- Boosting learns from its mistakes in an adaptive manner, continually refining its predictions to achieve high accuracy on complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4976a3db",
   "metadata": {},
   "source": [
    "### 3.2 How Boosting Works:\n",
    "\n",
    "1. Sequential Learning and Weighted Misclassifications:\n",
    "- Boosting works in a sequential manner, where each base learner is trained based on the performance of the previous ones.\n",
    "- During training, it assigns higher weights to misclassified instances, making them more influential in subsequent iterations.\n",
    "\n",
    "\n",
    "2. Iterative Training of Base Learners:\n",
    "- In each iteration, a new base learner is trained to correct the mistakes made by the ensemble so far.\n",
    "- The base learners are typically weak models (e.g., shallow decision trees) to avoid overfitting and maintain interpretability.\n",
    "\n",
    "\n",
    "3. Emphasizing Difficult Instances:\n",
    "- Boosting focuses on challenging instances that are frequently misclassified by previous base learners.\n",
    "- By repeatedly emphasizing these difficult instances, boosting ensures that the ensemble model pays more attention to them and gradually improves its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c8ca5c",
   "metadata": {},
   "source": [
    "### 3.3 Advantages of Boosting:\n",
    "\n",
    "1. Adaptive Learning and High Accuracy:\n",
    "- Boosting's adaptive learning approach allows it to learn from misclassifications and significantly improve predictive accuracy.\n",
    "- It is particularly effective in handling complex relationships in data, making it suitable for various real-world applications.\n",
    "\n",
    "\n",
    "2. Model Versatility and Feature Importance:\n",
    "- Boosting can be applied with various base learners, such as decision trees, SVMs, and neural networks.\n",
    "- Additionally, many boosting algorithms provide feature importance scores, enabling us to identify the most influential features in the model's decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf17fc",
   "metadata": {},
   "source": [
    "### 3.4 Common Algorithms Using Boosting:\n",
    "\n",
    "1. AdaBoost (Adaptive Boosting):\n",
    "- AdaBoost is one of the earliest and most popular boosting algorithms.\n",
    "- It assigns higher weights to misclassified instances and combines the predictions of weak learners to create a strong ensemble model.\n",
    "- AdaBoost is suitable for both classification and regression tasks.\n",
    "\n",
    "\n",
    "2. Gradient Boosting Machines (GBM):\n",
    "- GBM builds base learners sequentially, focusing on the gradients of the loss function to optimize the model's performance.\n",
    "- It uses a process called gradient descent to minimize the errors in each iteration.\n",
    "- GBM is widely used for regression and classification tasks and is known for its high accuracy and flexibility.\n",
    "\n",
    "\n",
    "3. XGBoost and LightGBM:\n",
    "- XGBoost and LightGBM are optimized implementations of gradient boosting that are efficient and scalable.\n",
    "- They utilize advanced techniques like parallel processing and tree-pruning to achieve better performance.\n",
    "- These algorithms are popular in data science competitions and real-world applications due to their speed and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5dbad0",
   "metadata": {},
   "source": [
    "### 3.5 Coding Example with AdaBoost (Python and scikit-learn):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6a181",
   "metadata": {},
   "source": [
    "#### 3.5.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0eaa1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ede917",
   "metadata": {},
   "source": [
    "This section imports all the necessary libraries and modules we'll use throughout the demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b84a28",
   "metadata": {},
   "source": [
    "#### 3.5.2 Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16ddbaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the covertype dataset from OpenML\n",
    "covertype = fetch_openml('covertype', version=4)\n",
    "X = covertype.data\n",
    "y = covertype.target.astype('int') - 1  # Shift labels to range 0-6\n",
    "\n",
    "# Select a subset of data for demonstration purposes (and for faster execution)\n",
    "X = X[:20000]\n",
    "y = y[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c45a1",
   "metadata": {},
   "source": [
    "#### 3.5.3 Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c95db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e4f66",
   "metadata": {},
   "source": [
    "This line splits our data into training and test sets. 70% of the data is used for training and 30% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac5e88",
   "metadata": {},
   "source": [
    "#### 3.5.4 Initialize a list to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01fb6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store results of each model\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a20be",
   "metadata": {},
   "source": [
    "We'll store the accuracy results of each model in this list and then convert it to a DataFrame at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6285a",
   "metadata": {},
   "source": [
    "#### 3.5.5 Train and test a simple decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7963046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a simple decision tree classifier\n",
    "tree = DecisionTreeClassifier(max_depth=1)  # Setting depth to 1, as AdaBoost typically uses 'stumps'\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "results.append(['Simple Decision Tree', accuracy_score(y_test, tree_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99950d",
   "metadata": {},
   "source": [
    "We train a simple decision tree \"stump\" (tree of depth 1) on the training data and then predict and evaluate its accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373f8cf",
   "metadata": {},
   "source": [
    "#### 3.5.6 Train and test a Random Forest with 5 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "612d92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_small = RandomForestClassifier(n_estimators=5, random_state=42)\n",
    "rf_small.fit(X_train, y_train)\n",
    "rf_small_pred = rf_small.predict(X_test)\n",
    "results.append(['Random Forest (5 trees)', accuracy_score(y_test, rf_small_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b788146b",
   "metadata": {},
   "source": [
    "Next, we train a random forest ensemble with 5 trees and then predict and evaluate its accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d18c0",
   "metadata": {},
   "source": [
    "#### 3.5.7 Train and test AdaBoost with 5 weak learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01cdec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test AdaBoost with 5 weak learners (stumps)\n",
    "adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), \n",
    "                              n_estimators=5, random_state=42)\n",
    "adaboost.fit(X_train, y_train)\n",
    "adaboost_pred = adaboost.predict(X_test)\n",
    "results.append(['AdaBoost (5 learners)', accuracy_score(y_test, adaboost_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8607597",
   "metadata": {},
   "source": [
    "Here, we train an AdaBoost classifier using decision tree stumps as weak learners. We use 5 such stumps. After training, we predict on the test set and evaluate the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ca487",
   "metadata": {},
   "source": [
    "#### 3.5.8 Convert the results list to a sorted DataFrame and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee79f888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (5 trees)</td>\n",
       "      <td>0.827667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost (5 learners)</td>\n",
       "      <td>0.488667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simple Decision Tree</td>\n",
       "      <td>0.398500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy\n",
       "0  Random Forest (5 trees)  0.827667\n",
       "1    AdaBoost (5 learners)  0.488667\n",
       "2     Simple Decision Tree  0.398500"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the results list to a DataFrame\n",
    "df_results = pd.DataFrame(results, columns=['Model', 'Accuracy'])\n",
    "\n",
    "# Sort the DataFrame based on the 'Accuracy' column in descending order\n",
    "df_results = df_results.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155315f",
   "metadata": {},
   "source": [
    "### 3.6 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f2ac0",
   "metadata": {},
   "source": [
    "1. Basic Understanding:\n",
    "\n",
    "- Question: Describe how boosting works. How is it different from bagging?\n",
    "- Answer: Boosting trains learners sequentially where each subsequent learner tries to correct the mistakes of the previous one. Unlike bagging, which aims to reduce variance, boosting aims to reduce bias and variance.\n",
    "2. Practical Implementation:\n",
    "\n",
    "- Exercise: Implement AdaBoost using scikit-learn on the digits dataset. Compare its performance with a single Decision Tree.\n",
    "- Hint: Use AdaBoostClassifier with DecisionTreeClassifier as the base estimator.\n",
    "3. Deep Dive:\n",
    "\n",
    "- Question: What is the role of learning rate in boosting algorithms like AdaBoost and Gradient Boosting?\n",
    "- Answer: The learning rate, often termed as alpha, determines the contribution of each tree to the final prediction. A smaller learning rate would require more boosting rounds to achieve the same reduction in error, but it often results in a better generalized model.\n",
    "4. Advanced Topic:\n",
    "\n",
    "- Exercise: Explore XGBoost, a popular gradient boosting algorithm. Install the library and implement it on the Boston Housing dataset. Compare its performance with basic Gradient Boosting from scikit-learn.\n",
    "- Hint: Remember to convert the dataset to DMatrix for XGBoost. Look into parameters like eta, max_depth, and subsample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1efdc",
   "metadata": {},
   "source": [
    "# 4. Strengths and Weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4209d790",
   "metadata": {},
   "source": [
    "### 4.1 Comparison of Bagging and Boosting:\n",
    "\n",
    "- Bagging and Boosting are both ensemble learning techniques that combine multiple models to improve predictive performance.\n",
    "- Bagging aims to reduce variance and overfitting by aggregating predictions from diverse models.\n",
    "- Boosting focuses on adaptive learning and aims to correct mistakes made by previous base learners in an iterative manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e5e2f",
   "metadata": {},
   "source": [
    "### 4.2 Strengths of Bagging:\n",
    "\n",
    "1. Reduced Variance and Overfitting:\n",
    "- Bagging reduces the risk of overfitting by combining predictions from multiple models with diverse training subsets.\n",
    "\n",
    "2. Robustness to Noisy Data:\n",
    "- Bagging is robust to noisy data and outliers since it aggregates predictions, minimizing the impact of individual data points.\n",
    "\n",
    "3. Efficient Parallelization:\n",
    "- Training base learners in bagging can be easily parallelized, making it efficient and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc096ec",
   "metadata": {},
   "source": [
    "### 4.3 Weaknesses of Bagging:\n",
    "\n",
    "1. Lack of Interpretability:\n",
    "- The ensemble model in bagging may lack interpretability, especially if the base learners are complex models like Random Forest.\n",
    "\n",
    "\n",
    "2. Bias Preservation:\n",
    "- Bagging may not address bias in the base learners; if the base models are biased, the ensemble may inherit the bias.\n",
    "\n",
    "\n",
    "3. Limited Performance Improvement:\n",
    "- Bagging may not improve performance significantly if the base learners are already strong and diverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc7f55",
   "metadata": {},
   "source": [
    "### 4.4 Strengths of Boosting:\n",
    "1. Adaptive Learning and High Accuracy:\n",
    "- Boosting adaptively learns from mistakes and focuses on challenging instances, leading to high accuracy.\n",
    "2. Model Versatility and Feature Importance:\n",
    "- Boosting can be used with various base learners and often provides feature importance scores for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b6b9e",
   "metadata": {},
   "source": [
    "### 4.5 Weaknesses of Boosting:\n",
    "1. Sensitivity to Noisy Data and Outliers:\n",
    "- Boosting may be sensitive to noisy data and outliers, affecting model performance.\n",
    "2. Slower Training and Limited Parallelization:\n",
    "- Training base learners sequentially makes boosting slower than bagging, and it may be less efficient to parallelize.\n",
    "3. Potential Overfitting:\n",
    "- Boosting may be prone to overfitting, especially if the number of iterations is too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60c163",
   "metadata": {},
   "source": [
    "### 4.6 Choosing Between Bagging and Boosting: Considerations:\n",
    "- The choice between bagging and boosting depends on the dataset, the complexity of the problem, and the available computational resources.\n",
    "- Bagging is suitable when the base models are diverse and the goal is to reduce variance and overfitting.\n",
    "- Boosting is preferred when high accuracy is crucial, and the dataset is not significantly affected by noisy data and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a72008c",
   "metadata": {},
   "source": [
    "# 5. Strengths and Weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d2dc2",
   "metadata": {},
   "source": [
    "1. High Variance Model:\n",
    "- You've trained a deep decision tree on your dataset and noticed that it performs extremely well on the training data but poorly on the validation data.\n",
    "\n",
    "- Question: Which ensemble technique might help remedy this, bagging or boosting?\n",
    "- Answer: Bagging. The described scenario suggests overfitting, which is a result of high variance. Bagging is more appropriate for reducing variance.\n",
    "2. High Bias Model:\n",
    "- Your team trained a shallow decision tree (i.e., a decision stump) on a complex dataset. The model performs poorly on both training and validation sets.\n",
    "\n",
    "- Question: Which ensemble technique might help improve this model's performance, bagging or boosting?\n",
    "- Answer: Boosting. The model is underfitting the data, which is a sign of high bias. Boosting aims to reduce bias by sequentially correcting the errors of the previous models.\n",
    "3. Large Dataset:\n",
    "- You have a very large dataset and are concerned about the training time. You're considering an ensemble technique to improve your model's performance.\n",
    "\n",
    "- Question: Which technique, bagging or boosting, would typically be faster in training?\n",
    "- Answer: Bagging. Boosting trains models sequentially, where each model tries to correct the errors of the previous one, which can be time-consuming. Bagging trains its models in parallel, making it often faster, especially with large datasets.\n",
    "4. Noisy Data:\n",
    "- Your dataset contains a significant amount of noise, and outliers are causing models to underperform.\n",
    "\n",
    "- Question: Which ensemble method, bagging or boosting, might be more robust to such noise and why?\n",
    "- Answer: Bagging. Boosting might overemphasize the outliers by giving them higher weights, leading to overfitting. Bagging, by averaging out predictions, is generally more robust to noisy data.\n",
    "5. Model Diversity:\n",
    "- You're working on an ensemble model, and you have access to various diverse base models. You believe the errors in these models are largely uncorrelated.\n",
    "\n",
    "- Question: Which ensemble technique might benefit more from this diversity, bagging or boosting?\n",
    "- Answer: Bagging. Bagging benefits greatly from the independence of errors among base models. If each model makes different errors, bagging can average them out, leading to a strong combined prediction.\n",
    "6. Information on Error Types:\n",
    "- After evaluating a model, you've noticed that it's making many types of errors, but the frequency of each type is low.\n",
    "\n",
    "- Question: If you had to choose an ensemble technique to correct diverse error types, would you pick bagging or boosting?\n",
    "- Answer: Boosting. Boosting is designed to sequentially correct the errors of previous models, making it suitable for addressing diverse types of errors.\n",
    "7. Final Model Interpretability:\n",
    "- You're working on a healthcare project where the interpretability of the model is crucial. Doctors want to understand how the model makes decisions.\n",
    "\n",
    "- Question: Which ensemble technique, bagging or boosting, might be more challenging in terms of interpretability, and why?\n",
    "- Answer: Both techniques can be challenging for interpretability as they combine multiple models. However, boosting, especially with many iterations, can be more challenging because it focuses on correcting errors sequentially, leading to a more complex combined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcb505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
