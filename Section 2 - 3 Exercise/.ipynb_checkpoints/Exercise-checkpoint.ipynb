{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment - Exploratory Data Analysis\n",
    "\n",
    "**Learning Objectives**\n",
    "After completing this assignment, you should be comfortable:\n",
    "\n",
    "- Using `matplotlib` and `seaborn` for data visualization.\n",
    "- Removing outliers.\n",
    "- Investigating missing data.\n",
    "\n",
    "You are free to add new cells to use as a scratch pad, but make sure to clean you code up and present your answer in the cell indicated with `# Write your code here`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction and Preparetion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Overview\n",
    "You've just been hired by the City of Toronto. Congratulation! Toronto has been collecting data on its bike share program since 2017 and the 2019 data has just become available. The city has implemented some new initiatives to try to increase ridership numbers such as **Free Ride Wednesdays** in the month of September and the addition of new bike lanes. You manager has asked you to:\n",
    "1. Merge the bike share data with local weather data from the TORONTO CITY CENTRE weather station.\n",
    "2. Investigate the effect of temperature on ridership numbers.   \n",
    "3. Explore different consumer behaviours between Annual Members and Casual Memebers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 3rd party libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check to see what weather files are available in the directory. Weather file names have the following structure `en_climate_hourly_ON_6158355_01-2017_P1H.csv`. All weather file names contain the number `6158359`, which is the `TORONTO CITY CENTRE` weather station ID. Create a variable called `weather_filenames` and assign a list containing all weather file names to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_filenames = [filename for filename in os.listdir() if '6158359' in filename]\n",
    "\n",
    "# Print file names\n",
    "print(weather_filenames[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weather_filenames` contains 12 files containing monthly weather data for 2019. Create a variable `weather_data` and assign a DataFrame to it that contains the data from all 12 `.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "weather_data = pd.concat(map(pd.read_csv, weather_filenames))\n",
    "\n",
    "# View DataFrame\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column called `'Date/Time'` contains hourly datetime stamps in the format `YYYY-MM-DD HH:MM`. Use `pd.DatetimeIndex()` to set the `'Date/Time'` column as the index of `weather_data`. Now the index of `weather_data` is composed of `Timestamps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.set_index(pd.DatetimeIndex(weather_data['Date/Time']), inplace=True)\n",
    "del weather_data[\"Date/Time\"]\n",
    "\n",
    "# View DataFrame\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of `weather_data` (`weather_data.index`) should be a series of Timestamps (e.g. `Timestamp('2019-01-01 00:00:00')`).\n",
    "\n",
    "Then localize them to Toronto's time zone (Eastern Standard Time - `EST`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "weather_data.index = weather_data.index.tz_localize('EST')\n",
    "\n",
    "# View DataFrame\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Next, plot temperature as a function of the datetime index. Your plot should look something like this.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/temp_2019.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Bike Share Data\n",
    "The assignment folder contains data about bike share trips in the city of Toronto for 2019 where there is one `.csv` file for each month. File names have the structure `bike_share_YYYY-MM.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bike share file names\n",
    "trips_filenames = [filename for filename in os.listdir() if 'bike_share' in filename]\n",
    "\n",
    "# Print file names\n",
    "print(trips_filenames[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable `trips_data` and assign a DataFrame to it that contains the bike share data from all 12 `.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_data = pd.concat(map(pd.read_csv, trips_filenames))\n",
    "\n",
    "# Let's remove double spaces from the column names\n",
    "trips_data.columns = [' '.join(col.split()) for col in trips_data.columns]              \n",
    "\n",
    "# View DataFrame\n",
    "trips_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert columns `'Start Time'` and `'End Time'` to datetimes. Then, localize `'Start Time'` and `'End Time'` to Eastern Standard Time (EST). This might take a minute or two.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "trips_data['Start Time'] = pd.DatetimeIndex(trips_data['Start Time']).tz_localize('EST')\n",
    "trips_data['End Time'] = pd.DatetimeIndex(trips_data['End Time']).tz_localize('EST')\n",
    "\n",
    "# View DataFrame\n",
    "trips_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "To check that these datetime conversion were done correctly, generate a plot of daily ride counts. Your plot should look something like this. Hint: Check out `.resample()` and consider making a new variable.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/trips_2019.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_graph = trips_data.set_index('Start Time').resample('D').count()\n",
    "\n",
    "# write your code in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clean Bike Share Data\n",
    "## Question 3a - Missing Data\n",
    "Large datasets are rarely completely full (no missing values) and its always a good ideas to evaluatate if there is missing data and for what fields. \n",
    "\n",
    "First, check for missing values in `weather_data`. Create a DataFrame named `weather_data_missing` where the index in the column names of `weather_data` and there is one column named `'count'` which contains the number of missing values for a particular column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "weather_data_missing = \n",
    "\n",
    "# View DataFrame\n",
    "weather_data_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check for missing values in `trips_data`. Create a DataFrame named `trips_data_missing` where the index in the column names of `trips_data` and there is one column named `'count'` which contains the number of missing values for a particular column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "trips_data_missing = \n",
    "\n",
    "# View DataFrame\n",
    "trips_data_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some columns have missing values. However, having missing data does not necessarily mean that something is wrong with an entry. For example, the `Weather` column contains the following unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['Weather'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that only non-normal/clear weather events are listed. So, when `weather_data['Weather'] == NaN`, the conditions are clear. Therefore, we would never want to remove rows where `weather_data['Weather'] == NaN`.\n",
    "\n",
    "We can see that the first 8 columns of `weather_data_missing` have no missing data, so we can leave `weather_data` and address the missingness on a case-by-case basis depending on which columns we're analyzing.\n",
    "\n",
    "For `trips_data`, we can see that `'End Station Id'` and `'End Station Name'` have 454 missing values, which is only 0.01% of the dataset. This might suggest corruption and given the small number of missing values, we can safetly drop these rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3b - Missing Data\n",
    "Drop any rows of `trips_data` with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "trips_data = \n",
    "\n",
    "# View DataFrame\n",
    "trips_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4a - Outliers\n",
    "Outliers in your datasets can be both good and bad. One the one hand, they may contain important information while on the other hand, they skew your visualizations and may bias your models. \n",
    "\n",
    "As a simple first pass, let's look at the summary statistics for `trips_data` using `.describe()` (remember, it only works for numeric data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4a\n",
    "\n",
    "Right away we notice something a bit funny with `'Trip Duration'`. The min and max values seem implausible. A trip cannot last `0 seconds` (you'd have to be biking at the speed of light!) and its unlikely that a trip lasted for `1.240378e+07 seconds`. `1.240378e+07 seconds` is roughly 4.78 months, which would be quite the ride and cost tens of thousands of dollars. We can see that the average `'Trip Duration'` is roughly 17 minutes.\n",
    "\n",
    "We've been told by Bike Share Toronto that trips lasting less than 1 minute can be considered false trips. Remove all trips from `trips_data` with a duration less than 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "trips_data = \n",
    "\n",
    "# View DataFrame\n",
    "trips_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4b - Outliers\n",
    "Next, remove any `'Trip Duration'` values less than `Q1 - 1.5 * IQR` and greater than `Q3 + 1.5 * IQR`. \n",
    "\n",
    "- Q1: The first quartile (`.quantile(0.25)`)\n",
    "- Q3: The third quartile (`.quantile(0.75)`)\n",
    "- IQR: The first quartil (`Q3 - Q1`)\n",
    "<br>\n",
    "<img src=\"images/probability_density.png\" alt=\"drawing\" width=\"450\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "# View DataFrame\n",
    "trips_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4c - Outliers\n",
    "Plot a histogram + density plot using `sns.distplot()` of the `'Trip Duration'`. Ensure that `'Trip Duration'` is displayed in minutes. Your plot should look something like this.\n",
    "<br>\n",
    "<img src=\"images/trip_durations.png\" alt=\"drawing\" width=\"450\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 - Duplicates\n",
    "Remove any entries from `trips_data` which have the same `'Trip Id'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "trips_data = \n",
    "\n",
    "\n",
    "# View DataFrame\n",
    "trips_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge Datasets\n",
    "To facilitate an analysis of the effect of weather on ridership, we must merge two DataFrames (`weather_data` and `trips_data`).\n",
    "\n",
    "\n",
    "Use the `.merge()` function to combine `weather_data` and `trips_data` using datetime information and set the output to a new variable called `data_merged`. In `trips_data` there are two time stamps corresponding to the start and end of the ride. Use the `'Start Time'` of the rides to merge. \n",
    "\n",
    "`trips_data` datetimes contain information down to the minute, while `weather_data` is reported every hour. Thus, we must merge based on a common year, month, day, hour. Hint: create a new column in `trips_data` called `'merge_time'` and set it equal to `trips_data['Start Time']` rounded to the nearest hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column called merge_time, which stores the converted value from the Start_time column from trip_data\n",
    "trips_data['merge_time'] = trips_data['Start Time'].dt.round(freq='H')\n",
    "\n",
    "data_merged = pd.merge(weather_data, trips_data, left_on = 'Date/Time',right_on = 'merge_time')\n",
    "\n",
    "# View DataFrame\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analysis of 'User Type'\n",
    "\n",
    "First, we'll explore the daily number for Annual Members and Casual Members. Casual Members pay on a per ride basis while Annual Members pay a monthly subcription fee. The DataFrame `data_merged` has a temporal resolution of a minute. Therefore, in order to look at daily numbers, we'll need to convert `data_merged` so that every row corresponds to a day. Create a new DataFrame called `data_days` with three columns:\n",
    "- ride: The total number of rides for a particular day.\n",
    "- annual_members: Number of rides by Annual Members.\n",
    "- casual_members: Number of rides by Casual Members.\n",
    "- workday: Was this day a workday (True) or a weekend day (False).\n",
    "\n",
    "Your DataFrame should looks something like this.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/data_days.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "As a quick sanity check you can check that the number of rows in `data_merged` is equal to the sum of `data_days['rides']`.\n",
    "\n",
    "Hint: You can use the `.groupby()` method and the `agg()` method to compute this transformation in a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isworkday(number):\n",
    "    if number < 5:\n",
    "        return True \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "data_days = data_merged.groupby(pd.Grouper(key = \"Start Time\", freq = \"D\")).agg(\n",
    "    ride = ('Start Time','count'),\n",
    "    annul_members = ('User Type', lambda x: (x == 'Annual Member').sum()),\n",
    "    casual_members = ('User Type', lambda x: (x == 'Casual Member').sum()),\n",
    ")\n",
    "data_days['workday'] = data_days.index.weekday\n",
    "data_days['workday'] = data_days['workday'].apply(isworkday)\n",
    "\n",
    "# View DataFrame\n",
    "data_days.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5a\n",
    "Use `sns.distplot()` to create a plot showing the distributions of daily ride counts from `data_days` for Casual Members and Annual Members. Your plot should look something like this. \n",
    "\n",
    "<br>\n",
    "<img src=\"images/ride_count_histogram.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5b\n",
    "Use `sns.scatterplot()` to create a scatter plot showing the relationship between daily ride counts from `data_days` for Casual Members and Annual Members. Your plot should look something like this. \n",
    "\n",
    "<br>\n",
    "<img src=\"images/ride_count_scatter.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the hourly ride counts for `Annual Members` and `Casual Members`. First thing we have to do i create a new DataFrame called `data_hours`. `data_hours` should have its index set to hours (0 to 23) using the `'Start Time'` column and three columns `'rides', 'annual_members', 'casual_members'`. These should be average hourly values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hours = data_merged.groupby(pd.Grouper(key = \"Start Time\", freq = \"H\")).agg(\n",
    "    ride = ('Start Time','count'),\n",
    "    annul_members = ('User Type', lambda x: (x == 'Annual Member').sum()),\n",
    "    casual_members = ('User Type', lambda x: (x == 'Casual Member').sum())\n",
    ")\n",
    "\n",
    "data_hours['hour']=data_hours.index.hour\n",
    "data_hours=data_hours.groupby('hour').mean()\n",
    "\n",
    "# View DataFrame\n",
    "data_hours.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5c\n",
    "Use `data_hours` to create a plot showing the average number of hourly rides for `Annual Members` and `Casual Memebers`. Your plot should look something like this.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/hourly_rides.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Analysis of 'Weather'\n",
    "In this section, we'll be looking at the influence of weather conditions, such as temperature and precipitation, on ridership activity.\n",
    "\n",
    "First, let's take a look at the missingness for `data_merged`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.isnull().sum(axis=0).to_frame('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `'Weather'` column has 824,894 missing values. Now let's take a look at the unique labels in the `'Weather'` column and how many entries contain each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most common `'Weather'` labels are `'Rain'`, `'Fog'`, and `'Rain,Fog'`. There is no label for **clear** condition, which suggests that the 824,894 NaN values correspond to **clear** conditions.\n",
    "\n",
    "\n",
    "The first thing we have to do is transform `data_merged` to contain aggregated values for each hour. Remember, `data_merged`'s grannularity is at the ride level. Each row, corresponds to one ride with a temporal resolution of one minute. Therefore, there can be multiple entries for the same minute.\n",
    "\n",
    "Create a new variable called `hourly_rides_and_weather` and assign a DataFrame to it containing the following information:\n",
    "- Index: DatetimeIndex with a resolution of 1 hour (2019-01-01 10:00:00, 2019-01-01 11:00:00, 2019-01-01 12:00:00, 2019-01-01 13:00:00, etc.). Use `'Start Time'` to generate this index.\n",
    "- Column 1 `'rides'`: How many rides were recorded during a particular hour.\n",
    "- Column 2 `'annual_members'`: How many `'Annual Member'` rides were recorded during a particular hour.\n",
    "- Column 3 `'casual_members'`: How many `'Casual Member'` rides were recorded during a particular hour.\n",
    "- Column 4 `'workday'`: Does this hour correspond to a workday or a weekend day (True, False). \n",
    "- Column 5 `'temp'`: Reported temperature from the `'Temp (°C)'` column. \n",
    "- Column 6 `'weather'`: Reported weather conditions from the `'Weather'` column. \n",
    "\n",
    "<br>\n",
    "<img src=\"images/hourly_rides_and_weather_1.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isworkday(number):\n",
    "    if number < 5:\n",
    "        return True \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "hourly_rides_and_weather = data_merged.groupby(pd.Grouper(key = \"Start Time\", freq = \"H\")).agg(\n",
    "    ride = ('Start Time','count'),\n",
    "    annul_members = ('User Type', lambda x: (x == 'Annual Member').sum()),\n",
    "    casual_members = ('User Type', lambda x: (x == 'Casual Member').sum()),\n",
    "    temp = ('Temp (°C)','max')\n",
    ")\n",
    "# iloc\n",
    "hourly_rides_and_weather['weather'] = weather_data['Weather']\n",
    "# use iloc\n",
    "hourly_rides_and_weather['workday'] = hourly_rides_and_weather.index.weekday\n",
    "hourly_rides_and_weather['workday'] = hourly_rides_and_weather['workday'].apply(isworkday)\n",
    "\n",
    "# change the order of the columns\n",
    "columnsTitles = ['ride', 'annul_members', 'casual_members', 'workday', 'temp', 'weather']\n",
    "hourly_rides_and_weather = hourly_rides_and_weather.reindex(columns=columnsTitles)\n",
    "\n",
    "hourly_rides_and_weather_temp = hourly_rides_and_weather\n",
    "# View DataFrame\n",
    "hourly_rides_and_weather.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, let's transform `hourly_rides_and_weather` from hourly to daily sampling. As we saw for **Question 7g**, there are strong trends within each day, which could complicate our initial analysis. Therefore, by aggrigating by day-of-the-week, we'll remove some of this trend.\n",
    "\n",
    "Modify `hourly_rides_and_weather` to include the following information:\n",
    "- Index: DatetimeIndex with a resolution of 1 day (2019-01-01 00:00:00, 2019-01-02 00:00:00, 2019-01-03 00:00:00, 2019-01-04 00:00:00, etc.). Use `'Start Time'` to generate this index.\n",
    "- Column 1 `'rides'`: How many rides were recorded during a particular day.\n",
    "- Column 2 `'annual_members'`: How many `'Annual Member'` rides were recorded during a particular day.\n",
    "- Column 3 `'casual_members'`: How many `'Casual Member'` rides were recorded during a particular day.\n",
    "- Column 4 `'workday'`: Does this a workday or a weekend day (True, False). \n",
    "- Column 5 `'temp'`: The maximum temperature recorded for a particular day. \n",
    "- Column 6 `'weather'`: This column should contain one of two values (`'clear'` or `'Precipitation'`). `'Clear'` should be assigned to days where 50% or more of the hours of that day had no precipitation events (Rain, Fog, Snow, Rain, Fog, etc.). Remember, `hourly_rides_and_weather['weather']` contains an `NaN` value when there was no precipitation event. When more than 50% of the hours of a day had a precipitation event, assign `'Precipitation'`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "hourly_rides_and_weather = data_merged.groupby(pd.Grouper(key = \"Start Time\", freq = \"D\")).agg(\n",
    "    ride = ('Start Time','count'),\n",
    "    annul_members = ('User Type', lambda x: (x == 'Annual Member').sum()),\n",
    "    casual_members = ('User Type', lambda x: (x == 'Casual Member').sum()),\n",
    "    temp = ('Temp (°C)','max'),\n",
    "#     weather = ('Weather', lambda x: 'Clear' if x.isna().all.sum() > 12 else 'Precipitation')\n",
    ")\n",
    "\n",
    "weather_list = hourly_rides_and_weather_temp.weather.values.tolist()\n",
    "\n",
    "# split the list into 24 sublists and combine them into a giant chunk\n",
    "def split(list_a, chunk_size):\n",
    "  for i in range(0, len(list_a), chunk_size):\n",
    "    yield list_a[i:i + chunk_size]\n",
    "\n",
    "# the organized weather list\n",
    "weather_list = list(split(weather_list, 24))\n",
    "\n",
    "# count the number of nan in a day\n",
    "def count_clear(day_weather_list):\n",
    "    count = 0\n",
    "    clear_day = []\n",
    "    for i in weather_list:\n",
    "        for j in range(len(i)):\n",
    "            if not isinstance(i[j], str):\n",
    "                count = count + 1\n",
    "        clear_day.append(count)\n",
    "        count = 0\n",
    "    return clear_day\n",
    "\n",
    "# the number of nan in a day list\n",
    "clear_day = count_clear(weather_list)\n",
    "\n",
    "# check if the weather is clear or precipitation\n",
    "def check_weather(weather_List):\n",
    "    result = []\n",
    "    for i in weather_List:\n",
    "        if i >= 12:\n",
    "            result.append('Clear')\n",
    "        else:\n",
    "            result.append('Precipitation')\n",
    "    return result\n",
    "\n",
    "weather_result = check_weather(clear_day)\n",
    "\n",
    "hourly_rides_and_weather['weather'] = weather_result\n",
    "\n",
    "# View DataFrame\n",
    "hourly_rides_and_weather.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6a\n",
    "Let's investigate the relationship between weather conditions and ridership numbers. Create a violin plot using `sns.violinplot()` that looks something like the figure below.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/weather_daily_rides_1.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6b\n",
    "Let's investigate the relationship between the maximum daily temperature and ridership numbers. Create a scatter plot using `sns.scatterplot()` that looks something like the figure below.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/temp_daily_rides.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
