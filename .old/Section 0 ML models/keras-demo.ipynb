{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CNN using `Keras` library\n\nThis tutorial will demonstrate image classification using the `Keras` library. This library allows for GPU acceleration. By utilizing GPUs, Keras significantly speeds up the training process, enabling the parallel processing of matrix operations involved in CNNs, resulting in faster and improved performance when training on large datasets like MNIST.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### Import libraries and initialize random generator","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport keras\nnp.random.seed(0)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:37:37.065783Z","iopub.execute_input":"2023-07-19T19:37:37.066197Z","iopub.status.idle":"2023-07-19T19:37:45.669881Z","shell.execute_reply.started":"2023-07-19T19:37:37.066168Z","shell.execute_reply":"2023-07-19T19:37:45.668904Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"A GPU (Graphics Processing Unit) is crucial for training CNN on large dataset due to its ability to perform parallel computations efficiently. CNNs involve lots of matrix operations, such as convolutions and pooling, which can be computationally intensive. GPUs excel at executing these operations simultaneously across multiple cores, dramatically accelerating the training process compared to a CPU. The parallel nature of GPUs allows for faster gradient calculations during backpropagation, leading to quicker weight updates and convergence to an optimal solution. Thus, the GPU's parallel processing capabilities significantly reduce the training time, making it an indispensable tool for effectively training CNNs on large datasets like MNIST.\n","metadata":{}},{"cell_type":"code","source":"# Check GPU availability\nimport tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:37:56.546011Z","iopub.execute_input":"2023-07-19T19:37:56.547343Z","iopub.status.idle":"2023-07-19T19:37:56.763294Z","shell.execute_reply.started":"2023-07-19T19:37:56.547299Z","shell.execute_reply":"2023-07-19T19:37:56.761899Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Num GPUs Available:  1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Read data using Keras","metadata":{}},{"cell_type":"code","source":"# Load the MNIST dataset\nfrom keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:38:07.090181Z","iopub.execute_input":"2023-07-19T19:38:07.090543Z","iopub.status.idle":"2023-07-19T19:38:07.568668Z","shell.execute_reply.started":"2023-07-19T19:38:07.090511Z","shell.execute_reply":"2023-07-19T19:38:07.567034Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preprocess the data\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n\nfrom keras.utils import np_utils\ny_train = np_utils.to_categorical(y_train, 10)\ny_test = np_utils.to_categorical(y_test, 10)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:38:38.350190Z","iopub.execute_input":"2023-07-19T19:38:38.350543Z","iopub.status.idle":"2023-07-19T19:38:38.477676Z","shell.execute_reply.started":"2023-07-19T19:38:38.350514Z","shell.execute_reply":"2023-07-19T19:38:38.476690Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Create the CNN model\n","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='sigmoid'))\nmodel.add(Dense(10, activation='softmax'))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:39:57.904483Z","iopub.execute_input":"2023-07-19T19:39:57.904861Z","iopub.status.idle":"2023-07-19T19:39:57.952331Z","shell.execute_reply.started":"2023-07-19T19:39:57.904831Z","shell.execute_reply":"2023-07-19T19:39:57.951432Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:39:59.365572Z","iopub.execute_input":"2023-07-19T19:39:59.365964Z","iopub.status.idle":"2023-07-19T19:39:59.922160Z","shell.execute_reply.started":"2023-07-19T19:39:59.365937Z","shell.execute_reply":"2023-07-19T19:39:59.920976Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model using GPU acceleration\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/device:GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_filey4zci_5h.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (200, 10, 10) and (200, 10) are incompatible\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (200, 10, 10) and (200, 10) are incompatible\n","output_type":"error"}]},{"cell_type":"code","source":"# Evaluate the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    score = model.evaluate(X_test, y_test, verbose=0)\n    print('Test loss:', score[0])\n    print('Test accuracy:', score[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experimentation with models with different layers","metadata":{}},{"cell_type":"code","source":"# Create the CNN model. No dropout\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n\n# Evaluate the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    score = model.evaluate(X_test, y_test, verbose=0)\n    print('Test loss:', score[0])\n    print('Test accuracy:', score[1])","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:00:33.813590Z","iopub.execute_input":"2023-07-17T09:00:33.814043Z","iopub.status.idle":"2023-07-17T09:00:56.106450Z","shell.execute_reply.started":"2023-07-17T09:00:33.814007Z","shell.execute_reply":"2023-07-17T09:00:56.105491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the CNN model. No dropout. no pooling\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n\n# Evaluate the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    score = model.evaluate(X_test, y_test, verbose=0)\n    print('Test loss:', score[0])\n    print('Test accuracy:', score[1])","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:00:56.108759Z","iopub.execute_input":"2023-07-17T09:00:56.109591Z","iopub.status.idle":"2023-07-17T09:01:11.602126Z","shell.execute_reply.started":"2023-07-17T09:00:56.109554Z","shell.execute_reply":"2023-07-17T09:01:11.600980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the CNN model. No dropout. no pooling\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='sigmoid'))\nmodel.add(Dense(10, activation='softmax'))\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n\n# Evaluate the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    score = model.evaluate(X_test, y_test, verbose=0)\n    print('Test loss:', score[0])\n    print('Test accuracy:', score[1])","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:03:28.476096Z","iopub.execute_input":"2023-07-17T09:03:28.476499Z","iopub.status.idle":"2023-07-17T09:03:50.807863Z","shell.execute_reply.started":"2023-07-17T09:03:28.476466Z","shell.execute_reply":"2023-07-17T09:03:50.806872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the CNN model. No dropout. no pooling\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='sigmoid'))\nmodel.add(Dense(10, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n\n# Evaluate the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    score = model.evaluate(X_test, y_test, verbose=0)\n    print('Test loss:', score[0])\n    print('Test accuracy:', score[1])","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:04:22.234721Z","iopub.execute_input":"2023-07-17T09:04:22.235126Z","iopub.status.idle":"2023-07-17T09:04:38.170761Z","shell.execute_reply.started":"2023-07-17T09:04:22.235092Z","shell.execute_reply":"2023-07-17T09:04:38.169710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1)))\nmodel.add(Activation(\"tanh\"))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation(\"tanh\"))\nmodel.add(Dense(10))\nmodel.add(Activation(\"softmax\"))\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n\n# Evaluate the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    score = model.evaluate(X_test, y_test, verbose=0)\n    print('Test loss:', score[0])\n    print('Test accuracy:', score[1])","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:16:28.116437Z","iopub.execute_input":"2023-07-17T09:16:28.117057Z","iopub.status.idle":"2023-07-17T09:16:50.492675Z","shell.execute_reply.started":"2023-07-17T09:16:28.117020Z","shell.execute_reply":"2023-07-17T09:16:50.491582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the CNN model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n\n# Evaluate the model using GPU acceleration\nwith tf.device('/device:GPU:0'):\n    score = model.evaluate(X_test, y_test, verbose=0)\n    print('Test loss:', score[0])\n    print('Test accuracy:', score[1])","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:01:34.540277Z","iopub.execute_input":"2023-07-17T09:01:34.542165Z","iopub.status.idle":"2023-07-17T09:02:18.267670Z","shell.execute_reply.started":"2023-07-17T09:01:34.542129Z","shell.execute_reply":"2023-07-17T09:02:18.266587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN CPU","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import np_utils\nimport tensorflow as tf\n\n# Load the MNIST dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Preprocess the data\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n\ny_train = np_utils.to_categorical(y_train, 10)\ny_test = np_utils.to_categorical(y_test, 10)\n\n# Build the CNN model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nbatch_size = 128\nepochs = 10\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n\n# Evaluate the model\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T23:16:23.688227Z","iopub.execute_input":"2023-07-18T23:16:23.688509Z","iopub.status.idle":"2023-07-18T23:17:05.622115Z","shell.execute_reply.started":"2023-07-18T23:16:23.688482Z","shell.execute_reply":"2023-07-18T23:17:05.621119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-07-18T23:24:19.539102Z","iopub.execute_input":"2023-07-18T23:24:19.539476Z","iopub.status.idle":"2023-07-18T23:24:19.934195Z","shell.execute_reply.started":"2023-07-18T23:24:19.539446Z","shell.execute_reply":"2023-07-18T23:24:19.927924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom keras.datasets import mnist\n\n# Load MNIST dataset using Keras\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Preprocess the data\nX_train = X_train.reshape(-1, 28, 28, 1) / 255.0\nX_test = X_test.reshape(-1, 28, 28, 1) / 255.0\ny_train = y_train.astype(np.int32)\ny_test = y_test.astype(np.int32)\n\n# Define activation function and its derivative\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    return np.where(x > 0, 1, 0)\n\n# Implement Convolutional Neural Network (CNN) from scratch\nclass CNN:\n    def __init__(self):\n        self.conv_filters = 8\n        self.kernel_size = 3\n        self.pool_size = 2\n        self.fc_units = 128\n        self.learning_rate = 0.001\n\n        # Initialize weights and biases for the convolutional and fully connected layers\n        self.conv_weights = np.random.randn(self.conv_filters, self.kernel_size, self.kernel_size)\n        self.conv_biases = np.zeros(self.conv_filters)\n        self.fc_weights = np.random.randn(12 * 12 * self.conv_filters, self.fc_units)\n        self.fc_biases = np.zeros(self.fc_units)\n        self.output_weights = np.random.randn(self.fc_units, 10)\n        self.output_biases = np.zeros(10)\n\n    def forward(self, x):\n        # Convolution layer\n        conv_out = np.zeros((x.shape[0], 28 - self.kernel_size + 1, 28 - self.kernel_size + 1, self.conv_filters))\n        for i in range(self.conv_filters):\n            for j in range(28 - self.kernel_size + 1):\n                for k in range(28 - self.kernel_size + 1):\n                    conv_out[:, j, k, i] = np.sum(x[:, j:j+self.kernel_size, k:k+self.kernel_size] * self.conv_weights[i, :, :],\n                                                  axis=(1, 2)) + self.conv_biases[i]\n\n        conv_out = relu(conv_out)\n\n        # Max pooling layer\n        pool_out = np.zeros((x.shape[0], 14, 14, self.conv_filters))\n        for i in range(self.conv_filters):\n            for j in range(14):\n                for k in range(14):\n                    pool_out[:, j, k, i] = np.max(conv_out[:, j*2:j*2+self.pool_size, k*2:k*2+self.pool_size, i], axis=(1, 2))\n\n        pool_out = pool_out.reshape(pool_out.shape[0], -1)\n\n        # Fully connected layer\n        fc_out = relu(np.dot(pool_out, self.fc_weights) + self.fc_biases)\n\n        # Output layer\n        output = np.dot(fc_out, self.output_weights) + self.output_biases\n\n        # Apply softmax activation for the final output\n        exp_scores = np.exp(output - np.max(output, axis=1, keepdims=True))\n        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n        return probabilities\n\n    def train(self, X, y, epochs=10, batch_size=32):\n        num_samples = X.shape[0]\n\n        for epoch in range(epochs):\n            total_loss = 0\n            for i in range(0, num_samples, batch_size):\n                X_batch = X[i:i+batch_size]\n                y_batch = y[i:i+batch_size]\n\n                # Forward pass\n                output = self.forward(X_batch)\n\n                # Compute loss using cross-entropy\n                loss = -np.log(output[np.arange(batch_size), y_batch]).mean()\n                total_loss += loss\n\n                # Backpropagation\n                one_hot_y = np.zeros_like(output)\n                one_hot_y[np.arange(batch_size), y_batch] = 1\n                delta3 = output - one_hot_y\n\n                delta2 = np.dot(delta3, self.output_weights.T) * relu_derivative(np.dot(X_batch, self.fc_weights) + self.fc_biases)\n                delta2 = delta2.reshape(delta2.shape[0], 12, 12, self.conv_filters)\n\n                dconv_weights = np.zeros_like(self.conv_weights)\n                dconv_biases = np.zeros_like(self.conv_biases)\n                for j in range(self.conv_filters):\n                    for k in range(12):\n                        for l in range(12):\n                            dconv_weights[j, :, :] += np.sum(delta2[:, k, l, j][:, np.newaxis, np.newaxis] *\n                                                             X_batch[:, k:k+self.kernel_size, l:l+self.kernel_size],\n                                                             axis=0)\n                dconv_biases = np.sum(delta2, axis=(0, 1, 2))\n\n                dfc_weights = np.dot(pool_out.T, delta2.reshape(delta2.shape[0], -1)) / batch_size\n                dfc_biases = np.sum(delta2, axis=0).sum(axis=1) / batch_size\n\n                dout_weights = np.dot(fc_out.T, delta3) / batch_size\n                dout_biases = np.sum(delta3, axis=0) / batch_size\n\n                # Update weights and biases using gradients\n                self.conv_weights -= self.learning_rate * dconv_weights\n                self.conv_biases -= self.learning_rate * dconv_biases\n                self.fc_weights -= self.learning_rate * dfc_weights\n                self.fc_biases -= self.learning_rate * dfc_biases\n                self.output_weights -= self.learning_rate * dout_weights\n                self.output_biases -= self.learning_rate * dout_biases\n\n            avg_loss = total_loss / (num_samples // batch_size)\n            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n    def predict(self, X):\n        return np.argmax(self.forward(X), axis=1)\n\n# Create the CNN model and train it\nmodel = CNN()\nmodel.train(X_train, y_train, epochs=10, batch_size=32)\n\n# Evaluate the model on the test set\npredictions = model.predict(X_test)\naccuracy = np.mean(predictions == y_test)\nprint(\"Test accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T23:26:49.060913Z","iopub.execute_input":"2023-07-18T23:26:49.061316Z","iopub.status.idle":"2023-07-18T23:26:49.710600Z","shell.execute_reply.started":"2023-07-18T23:26:49.061284Z","shell.execute_reply":"2023-07-18T23:26:49.709271Z"},"trusted":true},"execution_count":null,"outputs":[]}]}